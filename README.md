# `arxivscraper` *(Coursework)*

Repository created as part of the "How can we capture data from the web?" assignment for the course *M2.851 - Typology and Data Life Cycle* in the Master's Degree in Data Science (UOC) program. In it, we where asked to identify and extract relevant data for an analytical project using web scraping techniques and tools. As a use case, we selected the preprint plataform **arXiv** to implement a web scraper aimed at extracting metadata and relevant information from selected scientific articles.

In this repository we present the source-code we developed for this endevour, collected into the [`arxivscraper`]() library; alongside an example database (publised on [Zenodo](https://zenodo.org/)), a [project report]() and a [project video]().

## `arxivscraper` library
The centerpiece of this repo, the `arxivscraper` python module allows the user to gather the principal data of all preprints of a given topic published between two given dates. This data, published as a `.csv`, includes:
```
//data-point extract
{
  'index': '2510.23607',
  'title': 'Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations',
  'tags': ['cs.CV'],
  'authors: ['Yujia Zhang', 'Xiaoyang Wu', 'Yixing Lao', 'Chengyao Wang', 'Zhuotao Tian', 'Naiyan Wang', 'Hengshuang Zhao'],
  'abstract': 'Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.'
}
```
The collected articles are gathered from the advance search provided by the **arXiv** platform itself. The dates employed are taken to be the "Announcement date" for consistency and by default cross-listed papers are excluded (meaning that only the primary tag is consulted) unless the `--cross_list` flag is added to the incantation (see example use below). Only primary classifications are accepted for the search as this is what the advance search option allows (one can search for "astro-ph" but not "astro-ph.GA" for example). Three parameters are needed to be input by the user using the following flags: `--start_date` (`YYYY-mm-dd` format), `--end_date` (`YYYY-mm-dd` format) and `--category` (**arXiv**-compatible classification acronym). Two optional flags can be added: `--output` (file path for the resulting `.csv`) and `--cross_list`.

#### CLI example use
```bash
$ python3 arxivscraper/arxivscraper.py \
  --start_date "2025-02-01" \
  --end_date "2025-05-01" \
  --category "nlin" \
  --output "Data/arxiv_data.csv" \
  --cross_list

```

Import use is also allowed, using the `arxivscraper` as a module, with `from arxivscraper.arxivscraper import main` which accepts dict-type object or args.parse as arguments with the same formats as the flags previously presented.

#### Installation instructions
```bash
git clone https://github.com/<user>/arxivscraper.git
cd arxivscraper
pip install -r requirements.txt

```
---
### Output example
After running `arxivscraper`, the output is saved as a `.csv` file containing metadata for the collected preprints, here we pressent a snippet of said data
```csv
index,title,tags,authors,abstract
2510.23607,Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations,['cs.CV'],"['Yujia Zhang', 'Xiaoyang Wu', 'Yixing Lao', 'Chengyao Wang', 'Zhuotao Tian', 'Naiyan Wang', 'Hengshuang Zhao']","Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency."
2510.23606,Variational Masked Diffusion Models,"['cs.LG', 'cs.AI', 'cs.CL']","['Yichi Zhang', 'Alex Schwing', 'Zhizhen Zhao']","Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully learns dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where learning of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: https://riccizz.github.io/VMD."
2510.23605,"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling","['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'cs.RO']","['Shuhong Zheng', 'Ashkan Mirzaei', 'Igor Gilitschenski']","Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/."
2510.23604,"Solution to a Quantum Impurity Model for Moiré Systems: Fermi Liquid, Pairing, and Pseudogap",['cond-mat.str-el'],"['Yi-Jie Wang', 'Geng-Dong Zhou', 'Hyunsung Jung', 'Seongyeon Youn', 'Seung-Sup B. Lee', 'Zhi-Da Song']","Recent theoretical and experimental studies have revealed the co-existence of heavy and light electrons in magic-angle multilayer graphene, which form a periodic lattice of Anderson impurities hybridizing with Dirac semi-metals. This work demonstrates that nontrivial features -- pairing potential, pseudogap, and continuous quantum phase transitions -- already appear at the single-impurity level, if valley-anisotropic anti-Hund's interactions ($J_S$, $J_D$) are included, favoring either a singlet ($J_S>J_D$) or a valley doublet ($J_D>J_S$) impurity configuration. We derive a complete phase diagram and analytically solve the impurity problem at several fixed points using bosonization and refermionization techniques. When $J_D>J_S,J_D>0$, the valley doublet only couples via pair-hopping processes to the conduction electrons, in sharp contrast to the conventional Kondo scenario. Upon increasing $J_D$, there is a quantum phase transition of the BKT universality class, from a Fermi liquid to an anisotropic doublet phase, the latter exhibiting power-law susceptibilities with non-universal exponents. On the other hand, when $J_S>J_D,J_S>0$, increasing $J_S$ induces a second-order phase transition from Fermi liquid to a local singlet phase, which involves a non-Fermi liquid as an intermediate fixed point. Near the transition towards the anisotropic doublet (local singlet) phase, the renormalized interaction of the Fermi liquid becomes attractive, favoring doublet (singlet) pairing. Based on analytic solutions, we construct ansatz for the impurity spectral function and correlation self-energy, which account for the pseudogap accompanying side peaks, found in recent spectroscopic measurements and a DMFT study. In particular, we obtain a non-analytic V-shaped spectral function with non-universal exponents in the anisotropic doublet phase. All results are further verified by NRG calculations."
2510.23603,PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity,['cs.CV'],"['Yuqian Yuan', 'Wenqiao Zhang', 'Xin Li', 'Shihao Wang', 'Kehan Li', 'Wentong Li', 'Jun Xiao', 'Lei Zhang', 'Beng Chin Ooi']","Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency."
2510.23602,Genuine $C_n$-equivariant $\mathrm{TMF}$,"['math.AT', 'hep-th']","['Ying-Hsuan Lin', 'Akira Tominaga', 'Mayuko Yamashita']","We determine the $\mathrm{TMF}$-module structures of the genuine $C_2$-equivariant $\mathrm{TMF}$ with $\mathrm{RO}(C_2)$-gradings and of the $C_3$-equivariant $\mathrm{TMF}$. Moreover, we propose a general strategy for studying $C_n$-equivariant $\mathrm{TMF}$ via $U(1)$-equivariant $\mathrm{TMF}$ and a duality phenomenon in equivariant $\mathrm{TMF}$."
2510.23601,Alita-G: Self-Evolving Generative Agent for Agent Generation,['cs.AI'],"['Jiahao Qiu', 'Xuan Qi', 'Hongru Wang', 'Xinzhe Juan', 'Yimin Wang', 'Zelin Zhao', 'Jiayi Geng', 'Jiacheng Guo', 'Peihang Li', 'Jingzhe Shi', 'Shilong Liu', 'Mengdi Wang']","Large language models (LLMs) have been shown to perform better when scaffolded into agents with memory, tools, and feedback. Beyond this, self-evolving agents have emerged, but current work largely limits adaptation to prompt rewriting or failure retries. Therefore, we present ALITA-G, a self-evolution framework that transforms a general-purpose agent into a domain expert by systematically generating, abstracting, and curating Model Context Protocol (MCP) tools. In this framework, a generalist agent executes a curated suite of target-domain tasks and synthesizes candidate MCPs from successful trajectories. These are then abstracted to parameterized primitives and consolidated into an MCP Box. At inference time, ALITA-G performs retrieval-augmented MCP selection with the help of each tool's descriptions and use cases, before executing an agent equipped with the MCP Executor. Across several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains strong gains while reducing computation costs. On GAIA validation, it achieves 83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result while reducing mean tokens per example by approximately 15% relative to a strong baseline agent. ALITA-G thus provides a principled pathway from generalist capability to reusable, domain-specific competence, improving both accuracy and efficiency on complex reasoning tasks."
2510.23600,Coupling-induced universal dynamics in bilayer two-dimensional Bose gases,"['cond-mat.quant-gas', 'physics.atom-ph']","['En Chang', 'Vijay Pal Singh', 'Abel Beregi', 'Erik Rydow', 'Ludwig Mathey', 'Christopher J. Foot', 'Shinichi Sunami']","The emergence of order in many-body systems and the associated self-similar dynamics governed by dynamical scaling laws is a hallmark of universality far from equilibrium. Measuring and classifying such nontrivial behavior for novel symmetry classes remains challenging. Here, we realize a well-controlled interlayer coupling quench in a tunable bilayer two-dimensional Bose gas, driving the system to an ordered phase. We observe robust self-similar dynamics and a universal critical exponent consistent with diffusion-like coarsening, driven by vortex and antivortex annihilation induced by the interlayer coupling. Our results extend the understanding of universal dynamics in many-body systems and provide a robust foundation for quantitative tests of nonequilibrium effective field theories."
```
> ℹ️ **Note:** Only the first 5 entries are shown here for brevity. The full dataset is available [on Zenodo](https://zenodo.org/).

---
### ⚠️ Usage Notes

Some **arXiv** categories are exceptionally prolific. To avoid overwhelming the server and to ensure smooth operation, the script introduces a polite 15-second delay for every 200 results retrieved.  

We recommend:
- Focusing the search on short time periods (even a single day).
- Performing a preliminary search on the **arXiv** website to estimate the number of results your query might return.



[![CanoJones](https://img.shields.io/badge/author-CanoJones-blue?logo=github&logoColor=white)](https://github.com/Cano-Jones)
[![clopezvice](https://img.shields.io/badge/author-clopezvice-orange?logo=github&logoColor=white)](https://github.com/clopezvice)



[![CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC%20BY--NC--ND%204.0-lightgrey?logo=creativecommons&logoColor=white)](https://creativecommons.org/licenses/by-nc-nd/4.0/)


