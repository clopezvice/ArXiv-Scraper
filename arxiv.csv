index,title,tags,authors,abstract
2510.23607,Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations,['cs.CV'],"['Yujia Zhang', 'Xiaoyang Wu', 'Yixing Lao', 'Chengyao Wang', 'Zhuotao Tian', 'Naiyan Wang', 'Hengshuang Zhao']","Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency."
2510.23606,Variational Masked Diffusion Models,"['cs.LG', 'cs.AI', 'cs.CL']","['Yichi Zhang', 'Alex Schwing', 'Zhizhen Zhao']","Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully learns dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where learning of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: https://riccizz.github.io/VMD."
2510.23605,"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling","['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'cs.RO']","['Shuhong Zheng', 'Ashkan Mirzaei', 'Igor Gilitschenski']","Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/."
2510.23604,"Solution to a Quantum Impurity Model for Moiré Systems: Fermi Liquid, Pairing, and Pseudogap",['cond-mat.str-el'],"['Yi-Jie Wang', 'Geng-Dong Zhou', 'Hyunsung Jung', 'Seongyeon Youn', 'Seung-Sup B. Lee', 'Zhi-Da Song']","Recent theoretical and experimental studies have revealed the co-existence of heavy and light electrons in magic-angle multilayer graphene, which form a periodic lattice of Anderson impurities hybridizing with Dirac semi-metals. This work demonstrates that nontrivial features -- pairing potential, pseudogap, and continuous quantum phase transitions -- already appear at the single-impurity level, if valley-anisotropic anti-Hund's interactions ($J_S$, $J_D$) are included, favoring either a singlet ($J_S>J_D$) or a valley doublet ($J_D>J_S$) impurity configuration. We derive a complete phase diagram and analytically solve the impurity problem at several fixed points using bosonization and refermionization techniques. When $J_D>J_S,J_D>0$, the valley doublet only couples via pair-hopping processes to the conduction electrons, in sharp contrast to the conventional Kondo scenario. Upon increasing $J_D$, there is a quantum phase transition of the BKT universality class, from a Fermi liquid to an anisotropic doublet phase, the latter exhibiting power-law susceptibilities with non-universal exponents. On the other hand, when $J_S>J_D,J_S>0$, increasing $J_S$ induces a second-order phase transition from Fermi liquid to a local singlet phase, which involves a non-Fermi liquid as an intermediate fixed point. Near the transition towards the anisotropic doublet (local singlet) phase, the renormalized interaction of the Fermi liquid becomes attractive, favoring doublet (singlet) pairing. Based on analytic solutions, we construct ansatz for the impurity spectral function and correlation self-energy, which account for the pseudogap accompanying side peaks, found in recent spectroscopic measurements and a DMFT study. In particular, we obtain a non-analytic V-shaped spectral function with non-universal exponents in the anisotropic doublet phase. All results are further verified by NRG calculations."
2510.23603,PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity,['cs.CV'],"['Yuqian Yuan', 'Wenqiao Zhang', 'Xin Li', 'Shihao Wang', 'Kehan Li', 'Wentong Li', 'Jun Xiao', 'Lei Zhang', 'Beng Chin Ooi']","Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency."
2510.23602,Genuine $C_n$-equivariant $\mathrm{TMF}$,"['math.AT', 'hep-th']","['Ying-Hsuan Lin', 'Akira Tominaga', 'Mayuko Yamashita']","We determine the $\mathrm{TMF}$-module structures of the genuine $C_2$-equivariant $\mathrm{TMF}$ with $\mathrm{RO}(C_2)$-gradings and of the $C_3$-equivariant $\mathrm{TMF}$. Moreover, we propose a general strategy for studying $C_n$-equivariant $\mathrm{TMF}$ via $U(1)$-equivariant $\mathrm{TMF}$ and a duality phenomenon in equivariant $\mathrm{TMF}$."
2510.23601,Alita-G: Self-Evolving Generative Agent for Agent Generation,['cs.AI'],"['Jiahao Qiu', 'Xuan Qi', 'Hongru Wang', 'Xinzhe Juan', 'Yimin Wang', 'Zelin Zhao', 'Jiayi Geng', 'Jiacheng Guo', 'Peihang Li', 'Jingzhe Shi', 'Shilong Liu', 'Mengdi Wang']","Large language models (LLMs) have been shown to perform better when scaffolded into agents with memory, tools, and feedback. Beyond this, self-evolving agents have emerged, but current work largely limits adaptation to prompt rewriting or failure retries. Therefore, we present ALITA-G, a self-evolution framework that transforms a general-purpose agent into a domain expert by systematically generating, abstracting, and curating Model Context Protocol (MCP) tools. In this framework, a generalist agent executes a curated suite of target-domain tasks and synthesizes candidate MCPs from successful trajectories. These are then abstracted to parameterized primitives and consolidated into an MCP Box. At inference time, ALITA-G performs retrieval-augmented MCP selection with the help of each tool's descriptions and use cases, before executing an agent equipped with the MCP Executor. Across several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains strong gains while reducing computation costs. On GAIA validation, it achieves 83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result while reducing mean tokens per example by approximately 15% relative to a strong baseline agent. ALITA-G thus provides a principled pathway from generalist capability to reusable, domain-specific competence, improving both accuracy and efficiency on complex reasoning tasks."
2510.23600,Coupling-induced universal dynamics in bilayer two-dimensional Bose gases,"['cond-mat.quant-gas', 'physics.atom-ph']","['En Chang', 'Vijay Pal Singh', 'Abel Beregi', 'Erik Rydow', 'Ludwig Mathey', 'Christopher J. Foot', 'Shinichi Sunami']","The emergence of order in many-body systems and the associated self-similar dynamics governed by dynamical scaling laws is a hallmark of universality far from equilibrium. Measuring and classifying such nontrivial behavior for novel symmetry classes remains challenging. Here, we realize a well-controlled interlayer coupling quench in a tunable bilayer two-dimensional Bose gas, driving the system to an ordered phase. We observe robust self-similar dynamics and a universal critical exponent consistent with diffusion-like coarsening, driven by vortex and antivortex annihilation induced by the interlayer coupling. Our results extend the understanding of universal dynamics in many-body systems and provide a robust foundation for quantitative tests of nonequilibrium effective field theories."
2510.23599,The Benjamin-Ono equation with quasi-periodic data,['math.AP'],['Hagen Papenburg'],"We construct local solutions to the Benjamin-Ono equation for quasi-periodic initial data. The solution is unique among limits of smooth solutions and depends continuously on the data. Our result applies to a richer class of quasi-periodic functions than previous theorems. Central to the argument is an a-priori estimate, the proof of which utilizes Strichartz estimates for quasi-periodic functions obtained recently via decoupling, and a quasi-periodic extension of Tao's gauge transform. As a byproduct of our method, we also establish new local wellposedness results in certain anisotropic Sobolev spaces."
2510.23598,HST-COS Transit Spectroscopy of KELT-20b: First Detection of Excess Far-ultraviolet Absorption From an Ultra-hot Jupiter,['astro-ph.EP'],"['Patrick R. Behr', 'Kevin France', 'Luca Fossati', 'Tommi Koskinen', 'Patricio E. Cubillos', 'Arika Egan', 'P. Wilson Cauley']","KELT-20 b is an ultra-hot Jupiter with an equilibrium temperature of $2260$ K orbiting a bright (V =7.6), fast-rotating ($v\sin{i}$=117 km s$^{-1}$) A2 V star. The atmosphere of KELT-20 b has been studied extensively via transmission spectroscopy at optical wavelengths, showing strong hydrogen absorption as well as metals including Na I, Ca II, Fe I, Fe II, Mg I, Si I and Cr II. The atmospheric and ionization conditions of this planet may differ from Jupiter-mass exoplanets due to the relatively weak extreme-ultraviolet radiation from its host star, as the stellar dynamo that generates chromospheric and coronal activity is thought to shut down at spectral types earlier than A4. We present the first spectroscopic observations of KELT-20 b in the far-ultraviolet using the Hubble Space Telescope Cosmic Origins Spectrograph, searching for previously undetected low-ionization and neutral atoms in the upper atmosphere. We find that the FUV transit depth increases with decreasing wavelengths, from $1.88\pm0.04$\% at 1600--1760 Å to $2.28\pm0.04$\% at 1410--1570 Å, yielding planetary radii of $0.1139\pm0.06$ $R_*$ and $0.1222\pm0.07$ $R_*$, respectively. We report tentative detections of Fe II and N I at $2.4σ$ each, and non-detections of C I, S I, Al II, and Si II. We find no evidence for molecular absorption from CO or H$_2$ and no sign of hydrodynamic escape."
2510.23597,The Compressed 3D Lyman-Alpha Forest Bispectrum,"['astro-ph.CO', 'astro-ph.GA', 'hep-th']","['Roger de Belsunce', 'James M. Sullivan', 'Patrick McDonald']","Cosmological studies of the Lyman-Alpha (Lya) forest typically constrain parameters using two-point statistics. However, higher-order statistics, such as the three-point function (or its Fourier counterpart, the bispectrum) offer additional information and help break the degeneracy between the mean flux and power spectrum amplitude, albeit at a significant computational cost. To address this, we extend an existing highly informative compression of the bispectrum, the skew spectra, to the Lya forest. We derive the tree-level bispectrum of Lya forest fluctuations in the framework of effective field theory (EFT) directly in redshift space and validate our methodology on synthetic Lya forest data. We measure the anisotropic cross-spectra between the transmitted flux fraction and all quadratic operators arising in the bispectrum, yielding a set of 26 skew spectra. Using idealized 3D Gaussian smoothing (R=10 Mpc/h), we find good agreement (1-2 sigma level based on the statistical errors of the mocks) with the theoretical tree-level bispectrum prediction for monopole and quadrupole up to k <= 0.17 h/Mpc. To enable the cosmological analysis of Lya forest data from the currently observing Dark Energy Spectroscopic Instrument (DESI), where we cannot do 3D smoothing, we use a line-of-sight smoothing and introduce a new statistic, the shifted skew spectra. These probe non-squeezed bispectrum triangles and avoid locally applying quadratic operators to the field by displacing one copy of the field in the radial direction. Using a fixed displacement of 40 Mpc/h (and line-of-sight smoothing of 10 Mpc/h) yields a similar agreement with the theory prediction. For the special case of correlating the squared (and displaced) field with the original one, we analytically forward model the window function making this approach readily applicable to DESI data."
2510.23596,Think Twice: Branch-and-Rethink Reasoning Reward Model,['cs.CL'],"['Yizhu Jiao', 'Jiaqi Zeng', 'Julien Veron Vialard', 'Oleksii Kuchaiev', 'Jiawei Han', 'Olivier Delalleau']","Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-oncescoringintofocused, second-lookreasoning, BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet consequential errors while remaining practical and scalable. Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains. The code and the model will be released soon."
2510.23595,Multi-Agent Evolve: LLM Self-Improve through Co-evolution,['cs.AI'],"['Yixing Chen', 'Yiding Wang', 'Siqi Zhu', 'Haofei Yu', 'Tao Feng', 'Muhan Zhan', 'Mostofa Patwary', 'Jiaxuan You']","Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision."
2510.23594,PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection,['cs.CV'],"['Yusu Qian', 'Cheng Wan', 'Chao Jia', 'Yinfei Yang', 'Qingyu Zhao', 'Zhe Gan']","We introduce \textbf{PRISM-Bench}, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs."
2510.23593,On Terwilliger $\mathbb{F}$-algebras of factorial association schemes,"['math.CO', 'math.RT']","['Jiu-Yang He', 'Yu Jiang']","The Terwilliger algebras of association schemes over an arbitrary field $\mathbb{F}$ were called the Terwilliger $\mathbb{F}$-algebras of association schemes in [9]. In this paper, we study the Terwilliger $\mathbb{F}$-algebras of factorial association schemes. We determine the centers, the semisimplicity, the Jacobson radicals and their nilpotent indices, the Wedderburn-Artin decompositions of the Terwilliger $\mathbb{F}$-algebras of factorial association schemes. Moreover, we determine all Terwilliger $\mathbb{F}$-algebras of factorial association schemes that are the symmetric $\mathbb{F}$-algebras or the Frobenius $\mathbb{F}$-algebras."
2510.23592,Coherent all-optical tuning of large-area phase-gradient metasurface,['physics.optics'],"['Zhiping He', 'Xu Fang', 'Juejun Hu']","Tunable active metasurfaces have become a major research focus in recent years. Among tuning mechanisms, all-optical coherent control stands out because it requires no material or geometric change, enabling ultrafast, low-energy, interference-based modulation of amplitude, phase, and polarization in ultrathin devices. However, when applied to phase-gradient metasurfaces, coherent control has been limited to small apertures effectively confined to a single Fresnel zone, leading to large divergence and degraded beam quality. Here we propose and numerically validate a scalable method that enables large-area coherent control. The key idea is to use coherent illumination to tune the phase gradient within each Fresnel zone while a direct search algorithm optimizes zone-by-zone parameters to meet system-level targets. Using this principle, we demonstrate continuous tuning of a large-area metasurface for continuous beam-steering without per-meta-atom phase actuation. The same framework applies broadly to continuously tunable phase-gradient optics, including varifocal metalenses, parfocal zoom metalenses, tunable axicons, and related dynamic focusing elements."
2510.23591,Gaussian tomography for cold-atom simulators,"['quant-ph', 'cond-mat.quant-gas']","['Matthew Kiser', 'Max McGinley', 'Daniel Malz']","A limitation of analog quantum simulators based on cold atoms in optical lattices is that readout is typically limited to observables diagonal in the charge basis, i.e., densities and density correlation functions. To overcome this limitation, we propose experiment-friendly schemes to measure charge-off-diagonal correlations (such as currents). Our protocols use non-interacting dynamics for random times followed by standard quantum gas microscope measurements to effectively measure in random bases. The main requirement of our scheme is the ability to turn off interactions, which can be done in many atomic species using Feshbach resonances. Importantly, our scheme requires no local control and otherwise also exhibits modest requirements in terms of total evolution time and number of repetitions. We numerically demonstrate efficient estimation of bilinear correlation functions, requiring less than $4000$ samples to measure local currents to 5% error (system-size independent) and $\sim 10^4$ samples to simultaneously measure all non-local correlations in 70-site systems. Due to its simplicity, our protocol is implementable in existing platforms and thus paves the way to precision measurements beyond particle number measurements."
2510.23590,Lightweight Robust Direct Preference Optimization,['cs.LG'],"['Cheol Woo Kim', 'Shresth Verma', 'Mauricio Tec', 'Milind Tambe']","Direct Preference Optimization (DPO) has become a popular method for fine-tuning large language models (LLMs) due to its stability and simplicity. However, it is also known to be sensitive to noise in the data and prone to overfitting. Recent works have proposed using distributionally robust optimization (DRO) to address potential noise and distributional shift in the data. However, these methods often suffer from excessive conservatism and high computational cost. We propose DPO-PRO (DPO with Preference Robustness), a robust fine-tuning algorithm based on DPO which accounts for uncertainty in the preference distribution through a lightweight DRO formulation. Unlike prior DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences, avoiding unnecessary conservatism and incurring negligible computational overhead. We further show that DPO-PRO is equivalent to a regularized DPO objective that penalizes model overconfidence under weak preference signals. We evaluate DPO-PRO on standard alignment benchmarks and a real-world public health task. Experimental results show that our method consistently improves robustness to noisy preference signals compared to existing DPO variants."
2510.23589,InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video Cameras,['cs.CV'],"['Erich Liang', 'Roma Bhattacharjee', 'Sreemanti Dey', 'Rafael Moschopoulos', 'Caitlin Wang', 'Michel Liao', 'Grace Tan', 'Andrew Wang', 'Karhan Kayan', 'Stamatis Alexandropoulos', 'Jia Deng']","Accurately tracking camera intrinsics is crucial for achieving 3D understanding from 2D video. However, most 3D algorithms assume that camera intrinsics stay constant throughout a video, which is often not true for many real-world in-the-wild videos. A major obstacle in this field is a lack of dynamic camera intrinsics benchmarks--existing benchmarks typically offer limited diversity in scene content and intrinsics variation, and none provide per-frame intrinsic changes for consecutive video frames. In this paper, we present Intrinsics in Flux (InFlux), a real-world benchmark that provides per-frame ground truth intrinsics annotations for videos with dynamic intrinsics. Compared to prior benchmarks, InFlux captures a wider range of intrinsic variations and scene diversity, featuring 143K+ annotated frames from 386 high-resolution indoor and outdoor videos with dynamic camera intrinsics. To ensure accurate per-frame intrinsics, we build a comprehensive lookup table of calibration experiments and extend the Kalibr toolbox to improve its accuracy and robustness. Using our benchmark, we evaluate existing baseline methods for predicting camera intrinsics and find that most struggle to achieve accurate predictions on videos with dynamic intrinsics. For the dataset, code, videos, and submission, please visit https://influx.cs.princeton.edu/."
2510.23588,FARMER: Flow AutoRegressive Transformer over Pixels,['cs.CV'],"['Guangting Zheng', 'Qinyu Zhao', 'Tao Yang', 'Fei Xiao', 'Zhijie Lin', 'Jie Wu', 'Jiajun Deng', 'Yanyong Zhang', 'Rui Zhu']","Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training."
2510.23587,A Survey of Data Agents: Emerging Paradigm or Overstated Hype?,"['cs.DB', 'cs.AI']","['Yizhang Zhu', 'Liangwei Wang', 'Chenyu Yang', 'Xiaotian Lin', 'Boyan Li', 'Wei Zhou', 'Xinyu Liu', 'Zhangyang Peng', 'Tianqi Luo', 'Yu Li', 'Chengliang Chai', 'Chong Chen', 'Shimin Di', 'Ju Fan', 'Ji Sun', 'Nan Tang', 'Fugee Tsung', 'Jiannan Wang', 'Chenglin Wu', 'Yanwei Xu', 'Shaolei Zhang', 'Yong Zhang', 'Xuanhe Zhou', 'Guoliang Li', 'Yuyu Luo']","The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term ""data agent"" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents."
2510.23586,From Zonal to Nodal Capacity Expansion Planning: Spatial Aggregation Impacts on a Realistic Test-Case,"['math.OC', 'eess.SY']","['Elizabeth Glista', 'Bernard Knueven', 'Jean-Paul Watson']","Solving power system capacity expansion planning (CEP) problems at realistic spatial resolutions is computationally challenging. Thus, a common practice is to solve CEP over zonal models with low spatial resolution rather than over full-scale nodal power networks. Due to improvements in solving large-scale stochastic mixed integer programs, these computational limitations are becoming less relevant, and the assumption that zonal models are realistic and useful approximations of nodal CEP is worth revisiting. This work is the first to conduct a systematic computational study on the assumption that spatial aggregation can reasonably be used for ISO- and interconnect-scale CEP. By considering a realistic, large-scale test network based on the state of California with over 8,000 buses and 10,000 transmission lines, we demonstrate that well-designed small spatial aggregations can yield good approximations but that coarser zonal models result in large distortions of investment decisions."
2510.23585,Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models,"['cs.CL', 'cs.AI']","['Luis Ramos', 'Hiram Calvo', 'Olga Kolesnikova']","The identification of hope speech has become a promised NLP task, considering the need to detect motivational expressions of agency and goal-directed behaviour on social media platforms. This proposal evaluates traditional machine learning models and fine-tuned transformers for a previously split hope speech dataset as train, development and test set. On development test, a linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM with RBF kernel reached 0.77, and Naïve Bayes hit 0.75. Transformer models delivered better results, the best model achieved weighted precision of 0.82, weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80 accuracy. These results suggest that while optimally configured traditional machine learning models remain agile, transformer architectures detect some subtle semantics of hope to achieve higher precision and recall in hope speech detection, suggesting that larges transformers and LLMs could perform better in small datasets."
2510.23584,Newtonian Gravity Can Produce Quantum Entanglement,['hep-th'],"['Feng-Li Lin', 'Sayid Mondal']","In this note, we combine the key ingredient for preparing quantum bodies from superposed mass distributions with the post-Newtonian approach to obtain an effective field theory of quantum bodies, thereby addressing the question of whether classical gravity can produce quantum entanglement. In this framework, we find that the effective theory is a two-qubit interaction with a coupling given by the Newtonian nonlocal quadrupole-quadrupole interaction. This non-local coupling produces the entanglement between quantum bodies."
2510.23583,Many-body chaos and pole-skipping in holographic charged rotating fluids,"['hep-th', 'cond-mat.str-el', 'gr-qc']","['Hong-Da Lyu', 'Jun-Kun Zhao', 'Li Li']","Recent developments identify pole-skipping as a `smoking-gun' signature of the hydrodynamic nature of chaos, offering an alternative way to probe quantum chaos in addition to the out-of-time-ordered correlator (OTOC). We study the quantum chaos and pole-skipping phenomenon in the strongly coupled charged rotating fluids, holographically dual to rotating black holes with nontrivial gauge field. We find that the near-horizon equation governing energy-density fluctuations differs from the source-less shock wave equation determining the OTOC, which depends on the $U(1)$ gauge choice. This discrepancy is eliminated under an appropriate boundary condition on the $U(1)$ gauge potential at the event horizon, as required by the vanishing of Wilson loop at the Euclidean horizon. We further investigate the dependence of the butterfly velocity on the charge and rotation parameters in a specific black hole configuration--the Cvetič-Lü-Pope solution."
2510.23582,Cosmic magnification on multi-catalogue Herschel submillimetre galaxies,['astro-ph.CO'],"['R. Fernandez-Fernandez', 'M. M. Cueli', 'J. González-Nuevo', 'L. Bonavera', 'D. Crespo', 'E. Goitia', 'J. M. Casas', 'J. A. Cano', 'M. Migliaccio']","{Submillimetre galaxies (SMGs) are excellent background sources for magnification-bias studies, but the limited sky coverage in the submillimetre (sub-mm) band constrains their statistical power. Beyond H-ATLAS, Herschel produced additional sub-mm catalogues, though not optimised for spatial statistical lensing analyses.} {Our goal is to refine cosmological constraints from SMG magnification bias by exploiting the full sub-mm sky surveyed by Herschel.} {We expanded the SMG sample by incorporating other Herschel catalogues overlapping SDSS spectroscopic lenses. Random catalogues were generated via kernel density estimation to compute cross-correlations, and Markov Chain Monte Carlo methods were applied to infer astrophysical and cosmological parameters for each catalogue and for the combined dataset.} {We report the first detection of magnification bias in SMGs beyond H-ATLAS, reinforcing the robustness of this observable. Individual Herschel catalogues yield reasonable central values for $Ω_m$ and $σ_8$, although with large uncertainties. The combined analysis, dominated by the more powerful H-ATLAS sample, gives results consistent with $Λ$CDM: $Ω_m = 0.30^{+0.05}_{-0.07}$, $σ_8 = 0.80 (+/- 0.07)$, and $h < 0.80$, in better agreement with \textit{Planck} 2018 than previous non-tomographic studies.} {SMGs are promising tracers for magnification bias, but the narrow sub-mm coverage remains a major limitation. Wider surveys optimised for lensing would enable cross-correlations on larger scales, yielding tighter cosmological constraints.}"
2510.23581,Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation,"['cs.CV', 'cs.LG']","['Junyoung Seo', 'Rodrigo Mira', 'Alexandros Haliassos', 'Stella Bounareli', 'Honglie Chen', 'Linh Tran', 'Seungryong Kim', 'Zoe Landgraf', 'Jie Shen']","Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io."
2510.23580,Sheaves on Quivers via a Grothendieck Topology on the Path Category,"['math.CT', 'math.RT']","['Eric M. Schmid Jr.', 'Fernando Tohmé', 'William Chin']","We construct Grothendieck topologies on the path category of a finite graph, examining both coarse and discrete cases that offer different perspectives on quiver representations. The coarse topology declares each vertex covered by all incoming morphisms, giving the minimal non-trivial Grothendieck topology where sheaves correspond to dual representations via dualization. The discrete topology is the finest possible, forcing sheaves to be locally constant with isomorphic restriction maps. We verify these satisfy Grothendieck's axioms, characterize their sheaf categories, and establish functorial relationships between them. Sheaves on the coarse site arise naturally from quiver representations through dualization, while discrete sheaves correspond to representations of the groupoid completion. This work suggests intermediate topologies could capture subtler representation-theoretic phenomena."
2510.23579,High-Efficiency Thermoelectric Transport in Aharonov-Bohm-Casher Rings,['cond-mat.mes-hall'],"['Diego García', 'Sergio Arias', 'Rosa López']","Quantum heat engines are nanoscale devices that convert heat into work by exploiting quantum effects, such as coherence and interference. Previous studies of these devices did not consider spin-dependent effects, which can influence the thermoelectric performance of the engine. In this work, we study the thermoelectric behavior of a quantum heat engine based on an Aharonov-Bohm ring - a mesoscopic ring where electrons exhibit interference depending on the magnetic flux it encloses - incorporating Rashba spin-orbit interaction (SOI), which couples the electron's motion and spin. We find that Rashba SOI enhances the figure of merit $ZT$, measure of the engine's conversion efficiency. Our results suggest that controlling spin-dependent interference could lead to improvements in the fabrication of efficient thermoelectric devices."
2510.23578,Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study,['cs.AI'],"['Joachim Baumann', 'Aleksandra Urman', 'Ulrich Leicht-Deobald', 'Zachary J. Roman', 'Anikó Hannák', 'Markus Christen']","The rapid adoption of generative artificial intelligence (GenAI) technologies has led many organizations to integrate AI into their products and services, often without considering user preferences. Yet, public attitudes toward AI use, especially in impactful decision-making scenarios, are underexplored. Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488) representative of the Swiss population, we examine shifts in public attitudes toward AI before and after the launch of ChatGPT. We find that the GenAI boom is significantly associated with reduced public acceptance of AI (see Figure 1) and increased demand for human oversight in various decision-making contexts. The proportion of respondents finding AI ""not acceptable at all"" increased from 23% to 30%, while support for human-only decision-making rose from 18% to 26%. These shifts have amplified existing social inequalities in terms of widened educational, linguistic, and gender gaps post-boom. Our findings challenge industry assumptions about public readiness for AI deployment and highlight the critical importance of aligning technological development with evolving public preferences."
2510.23577,TAMI: Taming Heterogeneity in Temporal Interactions for Temporal Graph Link Prediction,"['cs.LG', 'cs.AI']","['Zhongyi Yu', 'Jianqiu Wu', 'Zhenghao Wu', 'Shuhan Zhong', 'Weifeng Su', 'Chul-Ho Lee', 'Weipeng Zhuo']","Temporal graph link prediction aims to predict future interactions between nodes in a graph based on their historical interactions, which are encoded in node embeddings. We observe that heterogeneity naturally appears in temporal interactions, e.g., a few node pairs can make most interaction events, and interaction events happen at varying intervals. This leads to the problems of ineffective temporal information encoding and forgetting of past interactions for a pair of nodes that interact intermittently for their link prediction. Existing methods, however, do not consider such heterogeneity in their learning process, and thus their learned temporal node embeddings are less effective, especially when predicting the links for infrequently interacting node pairs. To cope with the heterogeneity, we propose a novel framework called TAMI, which contains two effective components, namely log time encoding function (LTE) and link history aggregation (LHA). LTE better encodes the temporal information through transforming interaction intervals into more balanced ones, and LHA prevents the historical interactions for each target node pair from being forgotten. State-of-the-art temporal graph neural networks can be seamlessly and readily integrated into TAMI to improve their effectiveness. Experiment results on 13 classic datasets and three newest temporal graph benchmark (TGB) datasets show that TAMI consistently improves the link prediction performance of the underlying models in both transductive and inductive settings. Our code is available at https://github.com/Alleinx/TAMI_temporal_graph."
2510.23576,UrbanVLA: A Vision-Language-Action Model for Urban Micromobility,"['cs.RO', 'cs.AI', 'cs.CV']","['Anqi Li', 'Zhiyong Wang', 'Jiazhao Zhang', 'Minghan Li', 'Yunpeng Qi', 'Zhibo Chen', 'Zhizheng Zhang', 'He Wang']","Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties."
2510.23575,Bessel duality of Gabor systems: A von Neumann algebraic perspective,"['math.FA', 'math.OA']","['Ulrik Enstad', 'Franz Luef']","Bessel duality of regular Gabor systems states that a Gabor system over a lattice is a Bessel sequence if and only if the corresponding Gabor system over the adjoint lattice is a Bessel sequence. We show that this fundamental result of time-frequency analysis can be deduced from a theorem in the theory of bimodules over von Neumann algebras, namely that under certain conditions, their left and right bounded vectors coincide."
2510.23574,More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models,['cs.CV'],"['Hongkai Lin', 'Dingkang Liang', 'Mingyang Du', 'Xin Zhou', 'Xiang Bai']","Generative depth estimation methods leverage the rich visual priors stored in pre-trained text-to-image diffusion models, demonstrating astonishing zero-shot capability. However, parameter updates during training lead to catastrophic degra- dation in the image generation capability of the pre-trained model. We introduce MERGE, a unified model for image generation and depth estimation, starting from a fixed pre-trained text-to-image model. MERGE demonstrates that the pre-trained text-to-image model can do more than image generation, but also expand to depth estimation effortlessly. Specifically, MERGE introduces a play- and-plug framework that enables seamless switching between image generation and depth estimation modes through simple and pluggable converters. Meanwhile, we propose a Group Reuse Mechanism to encourage parameter reuse and im- prove the utilization of the additional learnable parameters. MERGE unleashes the powerful depth estimation capability of the pre-trained text-to-image model while preserving its original image generation ability. Compared to other unified models for image generation and depth estimation, MERGE achieves state-of- the-art performance across multiple depth estimation benchmarks. The code will be made available at https://github.com/H-EmbodVis/MERGE"
2510.23573,An Erdős--Szekeres type result for words with repeats,['math.CO'],"['Kyle Celano', 'Abigail Ollson', 'Niraj Velankar', 'Jun Yan']","In this short note, we prove an Erdős--Szekeres type result for words with repeats. Specifically, we show that every word with $kn^6+1$ repeats contains one of the following patterns: $0^{k+2}$, $0011\cdots nn$, $nn\cdots1100$, $012 \cdots n012 \cdots n$, $012 \cdots nn\cdots 210$, $n\cdots 210012\cdots n$, $n\cdots 210n\cdots 210$. Moreover, when $k=1$, we show that this is best possible by constructing a word with $n^6$ repeats that does not contain any of these patterns."
2510.23572,FPGA-Based Adaptive Control for Phase Stabilization in Fiber-Optic Interferometers Using Correlated Photons,['quant-ph'],"['P. M. Berto', 'F. CampodÓnico', 'A. A. Matoso', 'S. Vergara', 'P. A. Coelho', 'G. Lima', 'S. PÁdua', 'J. CariÑe']","Time-bin encoded photon pairs enable robust, decoherence-resistant transmission through optical fibers for long-distance quantum communication, where phase noise poses a critical limitation to stable operation. Here, we implement an adaptive Perturbation-and-Observe algorithm on a fully digital FPGA platform operating with real-time feedback at 1 Hz. The control signal is derived from the coincidence counts of correlated photon pairs. This adaptive approach reduces the rise time by 70\% and the coincidence noise by 30\%, resulting in visibility improvements sustained for more than 600 s.These results provide an efficient solution for long-term phase stabilization in quantum and photonic systems."
2510.23571,RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']","['Yash Jangir', 'Yidi Zhang', 'Kashu Yamazaki', 'Chenyu Zhang', 'Kuan-Hsun Tu', 'Tsung-Wei Ke', 'Lei Ke', 'Yonatan Bisk', 'Katerina Fragkiadaki']","The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining ""success"" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape."
2510.23570,The Euler characteristic of Milnor fibers over 2-generic symmetric determinantal varieties,['math.AG'],"['Thaís M. Dalbelo', 'Daniel Duarte', 'Danilo da Nóbrega Santos']","In this work we present a formula to compute the Euler characteristic of the Milnor fiber of non-degenerate functions $f: X \to \mathbb{C}$ with isolated critical set, where $X$ is a $2$-generic symmetric determinantal variety. The formula is obtained in two steps. Firstly, we explicitly describe the toric structure of those varieties. Secondly, we compute volumes of Newton polyhedra arising from the toric structure. The result then follows from Matsui-Takeuchi's formula for the Milnor fibers over toric varieties. As an application, we compute the local Euler obstruction of $X$ at the origin. This, in turn, allow us to provide a family of odd-dimensional normal toric varieties with isolated singular point not satisfying Matsui-Takeuchi's conjecture of the characterization of smoothness through the local Euler obstruction."
2510.23569,EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT,['cs.CV'],"['Baoqi Pei', 'Yifei Huang', 'Jilan Xu', 'Yuping He', 'Guo Chen', 'Fei Wu', 'Yu Qiao', 'Jiangmiao Pang']","Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models MLLMs, which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense hand-object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning RFT to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks. Full code and data are released at https://github.com/InternRobotics/EgoThinker."
2510.23568,COMAP Pathfinder -- Season 2 results IV. A stack on eBOSS/DESI quasars,"['astro-ph.CO', 'astro-ph.GA']","['D. A. Dunne', 'K. A. Cleary', 'J. G. S. Lunde', 'D. T. Chung', 'P. C. Breysse', 'N. O. Stutzer', 'J. R. Bond', 'H. K. Eriksen', 'J. O. Gundersen', 'G. A. Hoerning', 'J. Kim', 'E. M. Mansfield', 'S. R. Mason', 'N. Murray', 'T. J. Rennie', 'D. Tolgay', 'S. Valentine', 'I. K. Wehus', 'COMAP Collaboration']","We present a stack of data from the second season of the CO Mapping Array Project (COMAP) Pathfinder on the positions of quasars from eBOSS and DESI. COMAP is a Line Intensity Mapping (LIM) experiment targeting dense molecular gas via CO(1--0) emission at $z\sim3$. COMAP's Season 2 represents a $3\times$ increase in map-level sensitivity over the previous Early Science data release. We do not detect any CO emission in the stack, instead finding an upper limit of $10.0\times 10^{10}\ \mathrm{K\ km\ s^{-1}\ pc^2}$ at 95\% confidence within an $\sim 18\ \mathrm{cMpc}$ box. We compare this upper limit to models of the CO emission stacked on quasars and find a tentative ($\sim 3 σ$) tension between the limit and the brightest stack models after accounting for a suite of additional sources of experimental attenuation and uncertainty, including quasar velocity uncertainty, pipeline signal loss, cosmic variance, and interloper emission in the LIM data. The COMAP-eBOSS/DESI stack is primarily a measurement of the CO luminosity in the quasars' wider environment and is therefore potentially subject to environmental effects such as feedback. With our current simple models of the galaxy-halo connection, we are thus unable to confidently rule out any models of cosmic CO with the stack alone. Conversely, the stack's sensitivity to these large-scale environmental effects has the potential to make it a powerful tool for galaxy formation science, once we are able to constrain the average CO luminosity via the auto power spectrum (a key goal of COMAP)."
2510.23567,Lax-Kirchhoff moduli spaces and Hamiltonian 2D TQFT,"['math.DG', 'math.SG']","['Mohamed Moussadek Maiza', 'Maxence Mayrand']","We introduce the Lax-Kirchhoff moduli space associated with a finite quiver $Γ$ and a compact connected Lie group $G$. On each oriented edge we consider the Lax equation $\dot{A}_1 + [A_0, A_1] = 0$ and impose a Kirchhoff-type matching condition for the fields $A_1$ at interior vertices. Modulo gauge transformations trivial on the boundary, this yields a moduli space $\mathcal{M}(Γ)$. We prove that $\mathcal{M}(Γ)$ is a finite-dimensional smooth symplectic manifold carrying a Hamiltonian action of $G^{\partialΓ}$ whose moment map records the boundary values of $A_1$. Analytically, we construct slices for the infinite-dimensional gauge action and realize $\mathcal{M}(Γ)$ by Marsden-Weinstein reduction. For the quiver consisting of a single edge, we recover the classical identification $\mathcal{M} \cong T^*G$. In general, we identify $\mathcal{M}(Γ)$ with a symplectic reduction of $T^*G^E$ by $G^{Γ_{\mathrm{int}}}$, where $E$ is the set of edges and $Γ_{\mathrm{int}}$ is the set of interior vertices. We further show that $\mathcal{M}(Γ)$ is invariant under quiver homotopies, implying that it depends only on the surface with boundary obtained by thickening $Γ$. We then assemble these spaces into a two-dimensional topological quantum field theory valued in a category of Hamiltonian spaces."
2510.23566,Dark Energy Survey Year 6 Results: Redshift Calibration of the Weak Lensing Source Galaxies,['astro-ph.CO'],"['B. Yin', 'A. Amon', 'A. Campos', 'M. A. Troxel', ""W. d'Assignies"", 'G. M. Bernstein', 'G. Camacho-Ciurana', 'S. Mau', 'M. R. Becker', 'G. Giannini', 'A. Alarcón', 'D. Gruen', 'J. McCullough', 'M. Yamamoto', 'D. Anbajagane', 'S. Dodelson', 'C. Sánchez', 'J. Myles', 'J. Prat', 'C. Chang', 'M. Crocce', 'K. Bechtol', 'A. Ferté', 'M. Gatti', 'N. MacCrann']","Determining the distribution of redshifts for galaxies in wide-field photometric surveys is essential for robust cosmological studies of weak gravitational lensing. We present the methodology, calibrated redshift distributions, and uncertainties of the final Dark Energy Survey Year 6 (Y6) weak lensing galaxy data, divided into four redshift bins centered at $\langle z \rangle = [0.414, 0.538, 0.846, 1.157]$. We combine independent information from two methods on the full shape of redshift distributions: optical and near-infrared photometry within an improved Self-Organizing Map $p(z)$ (SOMPZ) framework, and cross-correlations with spectroscopic galaxy clustering measurements (WZ), which we demonstrate to be consistent both in terms of the redshift calibration itself and in terms of resulting cosmological constraints within 0.1$σ$. We describe the process used to produce an ensemble of redshift distributions that account for several known sources of uncertainty. Among these, imperfection in the calibration sample due to the lack of faint, representative spectra is the dominant factor. The final uncertainty on mean redshift in each bin is $σ_{\langle z\rangle} = [0.012, 0.008,0.009, 0.024]$. We ensure the robustness of the redshift distributions by leveraging new image simulations and a cross-check with galaxy shape information via the shear ratio (SR) method."
2510.23565,Dark Energy Survey Year 6 Results: Clustering-redshifts and importance sampling of Self-Organised-Maps $n(z)$ realizations for $3\times2$pt samples,['astro-ph.CO'],"[""W. d'Assignies"", 'G. M. Bernstein', 'B. Yin', 'G. Giannini', 'A. Alarcon', 'M. Manera', 'C. To', 'M. Yamamoto', 'N. Weaverdyck', 'R. Cawthon', 'M. Gatti', 'A. Amon', 'D. Anbajagane', 'S. Avila', 'M. R. Becker', 'K. Bechtol', 'C. Chang', 'M. Crocce', 'J. De Vicente', 'S. Dodelson', 'J. Fang', 'A. Ferté', 'D. Gruen', 'E. Legnani', 'A. Porredon']","This work is part of a series establishing the redshift framework for the $3\times2$pt analysis of the Dark Energy Survey Year 6 (DES Y6). For DES Y6, photometric redshift distributions are estimated using self-organizing maps (SOMs), calibrated with spectroscopic and many-band photometric data. To overcome limitations from color-redshift degeneracies and incomplete spectroscopic coverage, we enhance this approach by incorporating clustering-based redshift constraints (clustering-z, or WZ) from angular cross-correlations with BOSS and eBOSS galaxies, and eBOSS quasar samples. We define a WZ likelihood and apply importance sampling to a large ensemble of SOM-derived $n(z)$ realizations, selecting those consistent with the clustering measurements to produce a posterior sample for each lens and source bin. The analysis uses angular scales of 1.5-5 Mpc to optimize signal-to-noise while mitigating modeling uncertainties, and marginalizes over redshift-dependent galaxy bias and other systematics informed by the N-body simulation Cardinal. While a sparser spectroscopic reference sample limits WZ constraining power at $z>1.1$, particularly for source bins, we demonstrate that combining SOMPZ with WZ improves redshift accuracy and enhances the overall cosmological constraining power of DES Y6. We estimate an improvement in $S_8$ of approximately 10\% for cosmic shear and $3\times2$pt analysis, primarily due to the WZ calibration of the source samples."
2510.23564,ReCode: Unify Plan and Action for Universal Granularity Control,"['cs.AI', 'cs.CL', 'cs.LG']","['Zhaoyang Yu', 'Jiayi Zhang', 'Huixue Su', 'Yufan Zhao', 'Yifan Wu', 'Mingyi Deng', 'Jinyu Xiang', 'Yizhang Lin', 'Lingxiao Tang', 'Yingchao Li', 'Yuyu Luo', 'Bang Liu', 'Chenglin Wu']","Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode."
2510.23563,Tests of independence for pairs of paths of non-stationary Gaussian processes,"['math.ST', 'math.PR']","['Philip A. Ernst', 'Frederi G. Viens', 'Shuo Yan']","In the current work, we provide theoretical results for testing (in)dependence between pairs of paths of most commonly studied non-stationary Gaussian processes - standard Brownian motion and fractional Brownian motion (fBm). Please see the PDF version of the paper for a full abstract."
2510.23562,Design of Backscatter Tailored Optical Fibers for distributed magnetic field sensing using Fiber Optic Pulsed Polarimetry,"['physics.plasm-ph', 'physics.acc-ph']",['Roger J Smith'],"Fiber optic pulsed polarimetry is a LIDAR-like fiber sensing technique that uses a backscatter enhanced single mode backscatter-tailored optical fiber(BTOF) to measure the distributed B fields on all Magnetic Fusion Energy devices. The BTOF has a series of wavelength resonant reflection fiber Bragg gratings written uniformly along its length. The fiber's Verdet constant determines the strength of the Faraday effect which effectuates the measurement of local B along the fiber placed intimately next to or within a magnetized plasma volume. A robust measurement of the field distribution along the fiber is obtained at high rep rates, 5 MHz, high spatial resolution(1-10cm), high B field accuracy(<1%) and temporal response (ns). Multipathing in the BTOF produces 3rd order reflections that contaminate the LIDAR signal. Algorithms are given for calculating the level of contamination for uniform and flat reflection designs, in particular and any reflection series in general. The contamination is bracketed giving confidence in designing and implementing a BTOF. Applications include magnetic fusion devices, rail guns, high temperature superconducting magnets and magnetized target fusion research."
2510.23561,Revising Second Order Terms in Deep Animation Video Coding,"['eess.IV', 'cs.CV']","['Konstantin Schmidt', 'Thomas Richter']","First Order Motion Model is a generative model that animates human heads based on very little motion information derived from keypoints. It is a promising solution for video communication because first it operates at very low bitrate and second its computational complexity is moderate compared to other learning based video codecs. However, it has strong limitations by design. Since it generates facial animations by warping source-images, it fails to recreate videos with strong head movements. This works concentrates on one specific kind of head movements, namely head rotations. We show that replacing the Jacobian transformations in FOMM by a global rotation helps the system to perform better on items with head-rotations while saving 40% to 80% of bitrate on P-frames. Moreover, we apply state-of-the-art normalization techniques to the discriminator to stabilize the adversarial training which is essential for generating visually appealing videos. We evaluate the performance by the learned metics LPIPS and DISTS to show the success our optimizations."
2510.23560,Briot-Bouquet differential subordinations of analytic functions involving the Mittag-Leffler function defined in Cardioid domain,['math.CV'],"['Asena Cetinkaya', 'Sahsene Altinkaya']","In this researh work, we establish a new subclass of analytic functions constructed by the Mittag-Leffler function that maps the open unit disc onto the region bounded by the Cardioid domain. Using a technique introduced by Miller and Mocanu, we investigate several Briot-Bouquet differential subordinations for this function class."
2510.23559,KongNet: A Multi-headed Deep Learning Model for Detection and Classification of Nuclei in Histopathology Images,['eess.IV'],"['Jiaqi Lv', 'Esha Sadia Nasir', 'Kesi Xu', 'Mostafa Jahanifar', 'Brinder Singh Chohan', 'Behnaz Elhaminia', 'Shan E Ahmed Raza']","Accurate detection and classification of nuclei in histopathology images are critical for diagnostic and research applications. We present KongNet, a multi-headed deep learning architecture featuring a shared encoder and parallel, cell-type-specialised decoders. Through multi-task learning, each decoder jointly predicts nuclei centroids, segmentation masks, and contours, aided by Spatial and Channel Squeeze-and-Excitation (SCSE) attention modules and a composite loss function. We validate KongNet in three Grand Challenges. The proposed model achieved first place on track 1 and second place on track 2 during the MONKEY Challenge. Its lightweight variant (KongNet-Det) secured first place in the 2025 MIDOG Challenge. KongNet pre-trained on the MONKEY dataset and fine-tuned on the PUMA dataset ranked among the top three in the PUMA Challenge without further optimisation. Furthermore, KongNet established state-of-the-art performance on the publicly available PanNuke and CoNIC datasets. Our results demonstrate that the specialised multi-decoder design is highly effective for nuclei detection and classification across diverse tissue and stain types. The pre-trained model weights along with the inference code have been publicly released to support future research."
2510.23558,ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models,"['cs.SD', 'cs.CL', 'eess.AS']","['Bohan Li', 'Wenbin Huang', 'Yuhang Qiu', 'Yiwei Guo', 'Hankun Wang', 'Zhihan Li', 'Jing Peng', 'Ziyang Ma', 'Xie Chen', 'Kai Yu']","Large Audio Language Models (LALMs), which couple acoustic perception with large language models (LLMs) to extract and understand diverse information from audio, have attracted intense interest from both academic and industrial communities. However, existing LALMs are highly sensitive to how instructions are phrased, affecting both (i) instruction-following rates and (ii) task performance. Yet, no existing benchmarks offer a systematic and comprehensive evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark evaluating instruction sensitivity for LALMs along three axes: instruction description, output format, and task composition. We assess recent open-source and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy under controlled instruction variations. Experimental results reveal that even state-of-the-art LALMs suffer significant instruction sensitivity, leading to degraded performance on fundamental audio understanding tasks. To mitigate this issue, we fine-tune Qwen2-Audio on a specifically constructed complex instruction-variant dataset, achieving a marked improvement in instruction-following performance. However, this also induces nontrivial catastrophic forgetting: the model loses some previously mastered task capabilities when exposed to new instruction styles. Our benchmark provides a standardized basis for assessing and improving instruction sensitivity in LALMs, underscoring the need for instruction-robust audio understanding in real-world pipelines."
2510.23557,Minimizing Human Intervention in Online Classification,"['stat.ML', 'cs.LG']","['William Réveillard', 'Vasileios Saketos', 'Alexandre Proutiere', 'Richard Combes']","We introduce and study an online problem arising in question answering systems. In this problem, an agent must sequentially classify user-submitted queries represented by $d$-dimensional embeddings drawn i.i.d. from an unknown distribution. The agent may consult a costly human expert for the correct label, or guess on her own without receiving feedback. The goal is to minimize regret against an oracle with free expert access. When the time horizon $T$ is at least exponential in the embedding dimension $d$, one can learn the geometry of the class regions: in this regime, we propose the Conservative Hull-based Classifier (CHC), which maintains convex hulls of expert-labeled queries and calls the expert as soon as a query lands outside all known hulls. CHC attains $\mathcal{O}(\log^d T)$ regret in $T$ and is minimax optimal for $d=1$. Otherwise, the geometry cannot be reliably learned without additional distributional assumptions. We show that when the queries are drawn from a subgaussian mixture, for $T \le e^d$, a Center-based Classifier (CC) achieves regret proportional to $N\log{N}$ where $N$ is the number of labels. To bridge these regimes, we introduce the Generalized Hull-based Classifier (GHC), a practical extension of CHC that allows for more aggressive guessing via a tunable threshold parameter. Our approach is validated with experiments, notably on real-world question-answering datasets using embeddings derived from state-of-the-art large language models."
2510.23556,Potential-based formalism for electrodynamics of media with weak spatial dispersion,['cond-mat.other'],['Yury Solyaev'],"In this work, we develop a potential-based formalism for Maxwell's equations in isotropic media with weak spatial dispersion within the electric quadrupole-magnetic dipole approximation. We introduce an operator form of the constitutive relations along with a modified Lorenz gauge condition, which enables the derivation of decoupled generalized wave equations for electromagnetic potentials. For time-harmonic processes, we derive the representation of general solution for these equations as a combination of solutions to Helmholtz-type equations, whose parameters are determined by both standard and hyper-susceptibilities of the medium. We show that the proposed approach can be extended to more general constitutive relations and it provides a convenient framework for solving various applied problems. Specifically, using a derived closed-form solution for the problem of plane wave incidence on a planar interface, we demonstrate that a correct definition of the Poynting vector within the multipole theory must incorporate quadrupole effects -- an aspect overlooked in some previous works that has led to inconsistent results. We further establish the necessity of accounting for both propagated and evanescent longitudinal components in reflected and transmitted waves. The presence of these components, which follow directly from the general solution for electromagnetic potentials, is essential for satisfying all classical and additional boundary conditions in media with quadrupolar response (e.g., in metamaterials or quadrupolar liquid mixtures). The complete set of these boundary conditions is derived based on the least action principle, ensuring variational consistency with the field equations and generalizing previously known formulations of multipole theory."
2510.23555,Prediction of a topological phase transition in exchange alternating spin-1 nanographene chains,"['cond-mat.mes-hall', 'cond-mat.mtrl-sci']","['João C. G. Henriques', 'Yelko del Castillo', 'Ricardo Segundo', 'Jan Phillips', 'Joaquín Fernández-Rossier']","The use of magnetic nanographenes as building blocks for artificial spin lattices is enabling the exploration of flagship model Hamiltonians of one-dimensional quantum magnetism with an unprecedented degree of control. The spin-1 Heisenberg model, incorporating both linear and quadratic exchange interactions, was first realized using [3]-triangulenes, where the hallmark Haldane phase with spin fractionalization was observed. Later, the spin-1/2 Heisenberg Hamiltonian with exchange alternation was realized with Clar's goblets, where two additional topological phases were identified. Here we show that spin-1 nanographenes can also be used to explore the topological phase transition between the Haldane phase and a dimerized phase predicted for spin-1 chains with bond-alternation. We first study how the boundary of the phase transition is modified by non-linear exchange, known to be present in spin-1 nanographenes, using density matrix renormalization group (DMRG). Combining multiconfigurational with first-principles calculations, we propose two candidates to realize different topological phases of the model: a recently synthesized extended Clar's goblet, and a passivated [4]-triangulene. Moreover, we show how these two phases can be identified experimentally using inelastic electron tunneling spectroscopy (IETS). This work paves the way for the experimental realization of these topological phases, which can be locally probed with scanning tunneling microscopy."
2510.23554,A U-Net and Transformer Pipeline for Multilingual Image Translation,"['cs.LG', 'cs.CL', 'cs.CV']","['Siddharth Sahay', 'Radhika Agarwal']","This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural Machine Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images."
2510.23553,OntoPret: An Ontology for the Interpretation of Human Behavior,['cs.AI'],"['Alexis Ellis', 'Stacie Severyn', 'Fjollë Novakazi', 'Hadi Banaee', 'Cogan Shimizu']","As human machine teaming becomes central to paradigms like Industry 5.0, a critical need arises for machines to safely and effectively interpret complex human behaviors. A research gap currently exists between techno centric robotic frameworks, which often lack nuanced models of human behavior, and descriptive behavioral ontologies, which are not designed for real time, collaborative interpretation. This paper addresses this gap by presenting OntoPret, an ontology for the interpretation of human behavior. Grounded in cognitive science and a modular engineering methodology, OntoPret provides a formal, machine processable framework for classifying behaviors, including task deviations and deceptive actions. We demonstrate its adaptability across two distinct use cases manufacturing and gameplay and establish the semantic foundations necessary for advanced reasoning about human intentions."
2510.23552,Generalized Kantorovich-Rubinstein Duality beyond Hausdorff and Kantorovich,"['cs.LO', 'math.PR']","['Paul Wild', 'Lutz Schröder', 'Karla Messing', 'Barbara König', 'Jonas Forster']","The classical Kantorovich-Rubinstein duality guarantees coincidence between metrics on the space of probability distributions defined on the one hand via transport plans (couplings) and on the other hand via price functions. Both constructions have been lifted to the level of generality of set functors, with the coupling-based construction referred to as the Wasserstein lifting, and the price-function-based construction as the Kantorovich lifting, both based on a choice of quantitative modalities for the given functor. It is known that every Wasserstein lifting can be expressed as a Kantorovich lifting; however, the latter in general needs to use additional modalities. We give an example showing that this cannot be avoided in general. We refer to cases in which the same modalities can be used as satisfying the generalized Kantorovich-Rubinstein duality. We establish the generalized Kantorovich-Rubinstein duality in this sense for two important cases: The Lévy-Prokhorov distance on distributions, which finds wide-spread applications in machine learning due to its favourable stability properties, and the standard metric on convex sets of distributions that arises by combining the Hausdorff and Wasserstein distances."
2510.23551,Towards Stochastic (N-1)-Secure Redispatch,['eess.SY'],"['Oleksii Molodchyk', 'Hendrik Drögehorn', 'Martin Lindner', 'Mario Kendziorski', 'Timm Faulwasser']","The intermittent nature of renewable power availability is one of the major sources of uncertainty in power systems. While markets can guarantee that the demand is covered by the available generation, transmission system operators have to often intervene via economic redispatch to ensure that the physical constraints of the network are satisfied. To account for uncertainty, the underlying optimal power flow (OPF) routines have to be modified. Recently, polynomial chaos expansion (PCE) has been suggested in the literature as a tool for stochastic OPF problems. However, the usage of PCE-based methods in security-constrained OPF for (N-1)-secure operations has not yet been explored. In this paper, we propose a procedure that iteratively solves a PCE-overloaded stochastic OPF problem by including line outage constraints until an (N-1)-secure solution is achieved. We demonstrate the efficacy of our method by comparing it with a Monte-Carlo simulation on a 118-bus example system."
2510.23550,Bayesian Nonlinear PDE Inference via Gaussian Process Collocation with Application to the Richards Equation,"['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']","['Yumo Yang', 'Anass Ben Bouazza', 'Xuejun Dong', 'Quan Zhou']","The estimation of unknown parameters in nonlinear partial differential equations (PDEs) offers valuable insights across a wide range of scientific domains. In this work, we focus on estimating plant root parameters in the Richards equation, which is essential for understanding the soil-plant system in agricultural studies. Since conventional methods are computationally intensive and often yield unstable estimates, we develop a new Gaussian process collocation method for efficient Bayesian inference. Unlike existing Gaussian process-based approaches, our method constructs an approximate posterior distribution using samples drawn from a Gaussian process model fitted to the observed data, which does not require any structural assumption about the underlying PDE. Further, we propose to use an importance sampling procedure to correct for the discrepancy between the approximate and true posterior distributions. As an alternative, we also devise a prior-guided Bayesian optimization algorithm leveraging the approximate posterior. Simulation studies demonstrate that our method yields robust estimates under various settings. Finally, we apply our method on a real agricultural data set and estimate the plant root parameters with uncertainty quantification."
2510.23549,Cosmic Vine: High abundance of massive galaxies and dark matter halos in a forming cluster at z=3.44,['astro-ph.GA'],"['Nikolaj B. Sillassen', 'Shuowen Jin', 'Georgios E. Magdis', 'Francesco Valentino', 'Emanuele Daddi', 'Raphael Gobat', 'Malte Brinch', 'Kei Ito', 'Tao Wang', 'Hanwen Sun', 'Gabriel Brammer', 'Sune Toft', 'Thomas Greve']","The Cosmic Vine is a massive protocluster at z=3.44 in the JWST CEERS field, offering an ideal laboratory for studying the early phases of cluster formation. Using the data from the DAWN JWST Archive, we conduct a comprehensive study on the large-scale structure, stellar mass function (SMF), quiescent members, and dark matter halos in the Cosmic Vine. First, we spectroscopically confirm 136 galaxies in the Vine at z=3.44, and an additional 47 galaxies belonging to a diffuse foreground structure at z=3.34 which we dub the Leaf. We identify four subgroups comprising the Cosmic Vine and two subgroups within the Leaf. Second, we identified 11 quiescent members with log(M*/Msun)=9.5-11.0, the largest sample of quiescent galaxies in overdense environments at z>3, which gives an enhanced quiescent galaxy number density 2x10^(-4)cMpc^(-3) that is three times above the field level at log(M*/Msun) > 10. Notably, these quiescent members form a tight red sequence on the color-magnitude diagram, making it one of the earliest red sequences known to date. Third, by constructing the SMFs for both star-forming and quiescent members, we find that both SMFs are top-heavy, with a significantly enhanced quiescent fraction at log(M*/Msun)>10.5 compared to field counterparts. The stellar mass-size analysis reveals that star-forming members are more compact at higher masses than their field counterparts. Finally, we estimate a halo mass of log(Mh/Msun)=13.2+-0.3 for the protocluster core, and log(Mh/Msun)=11.9-12.4 for satellite subgroups. The phase-space analysis indicates that three subgroups are likely infalling to the core. This work reveals a high abundance of massive galaxies and dark matter halos in a forming cluster, demonstrating the accelerated assembly of massive galaxies in massive halos when the Universe was less than 2 billion years old."
2510.23548,The MandelZoom project II: the impact of stellar feedback on black hole accretion through an $α$-disc in dwarf galaxies with a resolved interstellar medium,"['astro-ph.GA', 'astro-ph.CO']","['Eun-jin Shin', 'Matthew C. Smith', 'Debora Sijacki', 'Martin A. Bourne', 'Sophie Koudmani']","We present a suite of high-resolution simulations to study how different stellar feedback channels regulate the growth of central intermediate-mass black holes (IMBHs) in dwarf galaxies hosting nuclear star clusters (NSCs). We employ a super-Lagrangian refinement scheme to resolve the self-gravity radius of the $α$-accretion disc ($<0.01$~pc) and follow the gas inflows from the interstellar medium (ISM) to the black hole (BH), allowing for the self-consistent emergence of circumnuclear discs (CNDs). In the absence of stellar feedback, as expected, the galactic disc fragments excessively, producing a massive CND. When radiative stellar feedback is included, fragmentation is suppressed, with even more massive CNDs forming and feeding the IMBH. With supernova (SN) feedback only, clustered SNe strongly heat the ISM, yielding both the lowest CND masses and BH accretion rates. When both radiative stellar feedback and SNe are included, the CND becomes intermittent: it survives for $10$--$100$~Myr, and is then destroyed by feedback before being replenished by fresh galactic inflows, while substantial BH growth still takes place. These results highlight the critical importance of accurately modelling the combined effects of key stellar feedback processes to understand IMBH growth. Our simulation suite brackets the likely range of CND states, with IMBHs exhibiting significant growth and systematic spin-up in all dwarf galaxy models explored. These findings bode well for the detection of IMBHs with future observational facilities such as SKA, the Rubin Observatory, and LISA, and make them highly relevant progenitor candidates of the high-redshift supermassive BHs observed by JWST."
2510.23547,Homological freeness criterion for operadic modules and application to Cohen-Macalayness of posets,"['math.QA', 'math.AT', 'math.CO', 'math.KT']",['Paul Laubie'],"We show a variation of the usual homological freeness criterion for operadic modules over a Koszul operad. We then apply this result to decorated partition posets for some operads, showing that their augmentation is Cohen-Macaulay and computing its homology. This work answers several open questions asked by Bérénice Delcroix-Oger and Clément Dupont in a recent article."
2510.23546,Variational Thermal State Preparation on Digital Quantum Processors Assisted by Matrix Product States,['quant-ph'],"['Rui-Hao Li', 'Semeon Valgushev', 'Khadijeh Najafi']","The preparation of quantum Gibbs states at finite temperatures is a cornerstone of quantum computation, enabling applications in quantum simulation of many-body systems, machine learning via quantum Boltzmann machines, and optimization through thermal sampling techniques. In this work, we introduce a variational framework that leverages matrix product states for the efficient classical evaluation of the Helmholtz free energy, combining scalable entanglement entropy computation with a hardware efficient ansatz to accurately approximate thermal states in one- and two-dimensional systems. We conduct extensive benchmarking on key observables, including energy density, susceptibility, specific heat, and two-point correlations, comparing against exact analytical results for 1D systems and quantum Monte Carlo simulations for 2D lattices across various temperatures and ansatz configurations. Our large-scale numerical simulations demonstrate the capability to prepare high-quality Gibbs states for 1D lattice models with up to 30 sites and 2D systems with up to 6x6 sites, using up to 42 qubits. Finally, we demonstrate the framework's practical viability on a 156-qubit IBM Heron processor by preparing the approximate Gibbs state of a 30-site transverse-field Ising model. Leveraging a combination of error mitigation techniques, we reduce the relative errors in energy and susceptibility measurements by over 50% compared to unmitigated results."
2510.23545,Scattering of a massive quantum vortex-dipole from an obstacle,['cond-mat.quant-gas'],"['Alice Bellettini', 'Enrico Ortu', 'Vittorio Penna']","In binary mixtures of Bose-Einstein condensates, massive-vortex dipoles can arise, and undergo scattering processes against obstacles. These show an intriguing dynamics, governed by the strongly nonlinear character of the quantum vortex motion, where we are able to highlight the effects of the boundaries. We first characterize such scattering dynamics via some point-like models, for the cases of an unbounded plane and a confined geometry. Within this framework, we find two fundamental scattering behaviors of a vortex dipole, the ""fly-by"" and the ""go-around"" processes. By plotting the deflection angle of the dipole versus the impact parameter we are able to quantify the transition between different scattering behaviors. We then are able to introduce an analytical distinction of the two scenarios, basing on the point-like model for the plane geometry. Furthermore, another interesting result shows the emergence of an on-average massless dynamics whenever the nonlinear interactions with the obstacle become negligible. Alongside, we investigate the quantum dipole scattering via the numerical simulation of two coupled Gross-Pitaevskii equations, describing the quantum mixture at a mean-field level. In this way, we benchmark the point-like model against the mean-field simulations."
2510.23544,LimRank: Less is More for Reasoning-Intensive Information Reranking,"['cs.CL', 'cs.IR']","['Tingyu Song', 'Yilun Zhao', 'Siyue Zhang', 'Chen Zhao', 'Arman Cohan']","Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving."
2510.23543,The Erdős-Ginzburg-Ziv constant of rank-two-like $p$-groups,"['math.CO', 'math.GR', 'math.NT']","['Benjamin Girard', 'Sofia Zotova']","Adapting Reiher's proof of Kemnitz's conjecture, we obtain two refinements of a theorem of Schmid and Zhuang. Our main results provide improved upper bounds for the Erdős-Ginzburg-Ziv constant of rank-two-like $p$-groups, and their direct products with cyclic groups of order coprime to $p$. In particular, we determine the exact value of this constant, and also confirm a conjecture of Gao, for a new infinite family of groups of arbitrarily large rank."
2510.23542,Magnetic-field controlled organic spintronic memristor for neural network computation,"['cond-mat.mes-hall', 'doi', '10.1021/acsami.5c14275']","['Tongxin Chen', 'Yinyu Nie', 'Yafei Hao', 'Shengchun Shen', 'Jiajun Pan', 'Xiaoguang Li', 'Yuan Lu']","Memristors are emerging as key electronic components that retain resistance states without power. Their non-volatile nature and ability to mimic synaptic behavior make them ideal for next-generation memory technologies and neuromorphic computing systems inspired by the human brain. In this study, we present a novel organic spintronic memristor based on a La0.67Sr0.33MnO3 (LSMO)/poly(vinylidene fluoride) (PVDF)/Co heterostructure, exhibiting biologically inspired synaptic behavior. Driven by fluorine atom migration within the PVDF layer, the device demonstrates both long-term depression (LTD) and long-term potentiation (LTP) under controlled electrical polarization. Distinctively, the resistance states can also be modulated by an external magnetic field via the tunneling magnetoresistance (TMR) effect, introducing a non-electrical means of tuning synaptic plasticity. This magnetic control mechanism enables multi-state modulation without compromising device performance or endurance. Furthermore, convolutional neural network (CNN) simulations incorporating this magnetic tuning capability reveal enhanced pattern recognition accuracy and improved training stability, especially at high learning rates. These findings underscore the potential of organic spintronic memristors as high-performance, low-power neuromorphic elements, particularly suited for applications in flexible and wearable electronics."
2510.23541,SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity,"['eess.AS', 'cs.SD']","['Hanke Xie', 'Haopeng Lin', 'Wenxiao Cao', 'Dake Guo', 'Wenjie Tian', 'Jun Wu', 'Hanlin Wen', 'Ruixuan Shang', 'Hongmei Liu', 'Zhiqi Jiang', 'Yuepeng Jiang', 'Wenxi Chen', 'Ruiqi Yan', 'Jiale Qian', 'Yichao Yan', 'Shunshun Yin', 'Ming Tao', 'Xie Chen', 'Lei Xie', 'Xinsheng Wang']","Recent advances in text-to-speech (TTS) synthesis have significantly improved speech expressiveness and naturalness. However, most existing systems are tailored for single-speaker synthesis and fall short in generating coherent multi-speaker conversational speech. This technical report presents SoulX-Podcast, a system designed for podcast-style multi-turn, multi-speaker dialogic speech generation, while also achieving state-of-the-art performance in conventional TTS tasks.
  To meet the higher naturalness demands of multi-turn spoken dialogue, SoulX-Podcast integrates a range of paralinguistic controls and supports both Mandarin and English, as well as several Chinese dialects, including Sichuanese, Henanese, and Cantonese, enabling more personalized podcast-style speech generation. Experimental results demonstrate that SoulX-Podcast can continuously produce over 90 minutes of conversation with stable speaker timbre and smooth speaker transitions. Moreover, speakers exhibit contextually adaptive prosody, reflecting natural rhythm and intonation changes as dialogues progress. Across multiple evaluation metrics, SoulX-Podcast achieves state-of-the-art performance in both monologue TTS and multi-turn conversational speech synthesis."
2510.23540,The causal interpretation of panel vector autoregressions,['econ.EM'],['Raimondo Pala'],"This paper discusses the different contemporaneous causal interpretations of Panel Vector Autoregressions (PVAR). I show that the interpretation of PVARs depends on the distribution of the causing variable, and can range from average treatment effects, to average causal responses, to a combination of the two. If the researcher is willing to postulate a no residual autocorrelation assumption, and some units can be thought of as controls, PVAR can identify average treatment effects on the treated. This method complements the toolkits already present in the literature, such as staggered-DiD, or LP-DiD, as it formulates assumptions in the residuals, and not in the outcome variables. Such a method features a notable advantage: it allows units to be ``sparsely'' treated, capturing the impact of interventions on the innovation component of the outcome variables. I provide an example related to the evaluation of the effects of natural disasters economic activity at the weekly frequency in the US.I conclude by discussing solutions to potential violations of the SUTVA assumption arising from interference."
2510.23539,The Enigma of Delayed Choice Quantum Eraser,['quant-ph'],['Tabish Qureshi'],"The delayed-choice quantum eraser represents an interesting experiment that exemplifies Bohr's principle of complementarity in a beautiful way. According to the complementarity principle, in a two-path interference experiment, the knowledge of which path was taken by the particle and the appearance of interference are mutually exclusive. Even when the which-path information is merely retained in specific quantum path-markers, without being actually read, it suffices to eliminate interference. Nevertheless, if this path information is ``erased'' in some manner, the interference re-emerges, a phenomenon referred to as the quantum eraser. An intriguing aspect of this experiment is that if the path information is erased \emph{after} the particle has been detected on the screen, the interference still reappears, a phenomenon known as the delayed-choice quantum eraser. This observation has led to the interpretation that the particle can be influenced to exhibit characteristics of either a particle or a wave based on a decision made long after it has been registered on the screen. This idea has sparked considerable debate and discussions surrounding retrocausality. This controversy is reviewed here, and a detailed resolution provided."
2510.23538,JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence,"['cs.AI', 'cs.CL', 'cs.CV', 'cs.SE']","['Qiushi Sun', 'Jingyang Gong', 'Yang Liu', 'Qiaosheng Chen', 'Lei Li', 'Kai Chen', 'Qipeng Guo', 'Ben Kao', 'Fei Yuan']","The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder."
2510.23537,Approximately optimal distributed controls for high-dimensional stochastic systems with pairwise interaction through controls,['math.OC'],['Elise Devey'],"This paper investigates large-population stochastic control problems in which agents share their state information and cooperate to minimize a convex cost functional. The latter is decomposed into individual and coupling costs, with the distinctive feature that the coupling term is a pairwise interaction function between the controls. To address this setting, we follow closely (Jackson & Lacker, 2025): we introduce a related problem where each agent observes only its own state. We then establish a quantitative bound on the difference between the value functions associated with these two problems. We obtain this result by reformulating the problems analytically as Hamilton-Jacobi type equations and comparing their associated Hamiltonians. The main difficulty of our approach lies in establishing a precise comparison between the distributions of the corresponding optimal controls."
2510.23536,IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering,['cs.CL'],"['Jieyong Kim', 'Maryam Amirizaniani', 'Soojin Yoon', 'Dongha Lee']","Intent identification serves as the foundation for generating appropriate responses in personalized question answering (PQA). However, existing benchmarks evaluate only response quality or retrieval performance without directly measuring intent identification capabilities. This gap is critical because without understanding which intents users prioritize, systems cannot generate responses satisfying individual information needs. To address this, we introduce the concept of core intents: intents users prioritize when selecting answers to satisfy their information needs. To evaluate these core intents, we propose IPQA, a benchmark for core Intent identification in Personalized Question Answering. Since users do not explicitly state their prioritized intents, we derive core intents from observable behavior patterns in answer selection, grounded in satisficing theory where users choose answers meeting their acceptance thresholds. We construct a dataset with various domains through systematic filtering, LLM-based annotation, and rigorous quality control combining automated verification with human validation. Experimental evaluations across state-of-the-art language models reveal that current systems struggle with core intent identification in personalized contexts. Models fail to identify core intents from user histories, with performance degrading as question complexity increases. The code and dataset will be made publicly available to facilitate future research in this direction."
2510.23535,Sequential Multi-Agent Dynamic Algorithm Configuration,"['cs.LG', 'cs.NE']","['Chen Lu', 'Ke Xue', 'Lei Yuan', 'Yao Wang', 'Yaoyuan Wang', 'Sheng Fu', 'Chao Qian']","Dynamic algorithm configuration (DAC) is a recent trend in automated machine learning, which can dynamically adjust the algorithm's configuration during the execution process and relieve users from tedious trial-and-error tuning tasks. Recently, multi-agent reinforcement learning (MARL) approaches have improved the configuration of multiple heterogeneous hyperparameters, making various parameter configurations for complex algorithms possible. However, many complex algorithms have inherent inter-dependencies among multiple parameters (e.g., determining the operator type first and then the operator's parameter), which are, however, not considered in previous approaches, thus leading to sub-optimal results. In this paper, we propose the sequential multi-agent DAC (Seq-MADAC) framework to address this issue by considering the inherent inter-dependencies of multiple parameters. Specifically, we propose a sequential advantage decomposition network, which can leverage action-order information through sequential advantage decomposition. Experiments from synthetic functions to the configuration of multi-objective optimization algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art MARL methods and show strong generalization across problem classes. Seq-MADAC establishes a new paradigm for the widespread dependency-aware automated algorithm configuration. Our code is available at https://github.com/lamda-bbo/seq-madac."
2510.23534,Direct Debiased Machine Learning via Bregman Divergence Minimization,"['econ.EM', 'cs.LG', 'math.ST', 'stat.ME', 'stat.ML']",['Masahiro Kato'],"We develop a direct debiased machine learning framework comprising Neyman targeted estimation and generalized Riesz regression. Our framework unifies Riesz regression for automatic debiased machine learning, covariate balancing, targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In many problems involving causal effects or structural models, the parameters of interest depend on regression functions. Plugging regression functions estimated by machine learning methods into the identifying equations can yield poor performance because of first-stage bias. To reduce such bias, debiased machine learning employs Neyman orthogonal estimating equations. Debiased machine learning typically requires estimation of the Riesz representer and the regression function. For this problem, we develop a direct debiased machine learning framework with an end-to-end algorithm. We formulate estimation of the nuisance parameters, the regression function and the Riesz representer, as minimizing the discrepancy between Neyman orthogonal scores computed with known and unknown nuisance parameters, which we refer to as Neyman targeted estimation. Neyman targeted estimation includes Riesz representer estimation, and we measure discrepancies using the Bregman divergence. The Bregman divergence encompasses various loss functions as special cases, where the squared loss yields Riesz regression and the Kullback-Leibler divergence yields entropy balancing. We refer to this Riesz representer estimation as generalized Riesz regression. Neyman targeted estimation also yields TMLE as a special case for regression function estimation. Furthermore, for specific pairs of models and Riesz representer estimation methods, we can automatically obtain the covariate balancing property without explicitly solving the covariate balancing objective."
2510.23533,Discovery of SN 2025wny: a Strongly Gravitationally Lensed Superluminous Supernova at z = 2.01,['astro-ph.CO'],"['Joel Johansson', 'Daniel A. Perley', 'Ariel Goobar', 'Jacob L. Wise', 'Yu-Jing Qin', 'Zoë McGrath', 'Steve Schulze', 'Cameron Lemon', 'Anjasha Gangopadhyay', 'Konstantinos Tsalapatas', 'Igor Andreoni', 'Eric C. Bellm', 'Joshua S. Bloom', 'Richard Dekany', 'Suhail Dhawan', 'Christoffer Fremling', 'Matthew J. Graham', 'Steven L. Groom', 'Daniel Gruen', 'Xander J. Hall', 'Mansi Kasliwal', 'Russ R. Laher', 'Ragnhild Lunnan', 'Ashish A. Mahabal', 'Adam A. Miller']","We present the discovery of SN 2025wny (ZTF25abnjznp/GOTO25gtq) and spectroscopic classification of this event as the first gravitationally lensed Type I superluminous supernovae (SLSN-I). Deep ground-based follow-up observations resolves four images of the supernova with ~1.7"" angular separation from the main lens galaxy, each coincident with the lensed images of a background galaxy seen in archival imaging of the field. Spectroscopy of the brightest point image shows narrow features matching absorption lines at a redshift of z = 2.011 and broad features matching those seen in superluminous SNe with Far-UV coverage. We infer a magnification factor of 20 to 50 for the brightest image in the system, based on photometric and spectroscopic comparisons to other SLSNe-I. SN 2025wny demonstrates that gravitationally-lensed SNe are in reach of ground-based facilities out to redshifts far higher than what has been previously assumed, and provide a unique window into studying distant supernovae, internal properties of dwarf galaxies, as well as for time-delay cosmography."
2510.23532,When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning,"['cs.AI', 'cs.LG']","['Anirban Das', 'Irtaza Khalid', 'Rafael Peñaloza', 'Steven Schockaert']","Designing models that can learn to reason in a systematic way is an important and long-standing challenge. In recent years, a wide range of solutions have been proposed for the specific case of systematic relational reasoning, including Neuro-Symbolic approaches, variants of the Transformer architecture, and specialised Graph Neural Networks. However, existing benchmarks for systematic relational reasoning focus on an overly simplified setting, based on the assumption that reasoning can be reduced to composing relational paths. In fact, this assumption is hard-baked into the architecture of several recent models, leading to approaches that can perform well on existing benchmarks but are difficult to generalise to other settings. To support further progress in the field of systematic relational reasoning with neural networks, we introduce NoRA, a new benchmark which adds several levels of difficulty and requires models to go beyond path-based reasoning."
2510.23531,Classifying strict discrete opfibrations with lax morphisms,['math.CT'],"['Matteo Capucci', 'David Jaz Myers']","We study how discrete opfibration classifiers in a(n enhanced) 2-category can be endowed with the structure of a $T$-algebra and thereby lift to the enhanced 2-category of 2-algebras and lax morphisms. To support this study, we give a definition of discrete opfibration classifier in the enhanced setting in which tight (e.g. strict) discrete opfibrations are classified by loose (e.g. lax) maps.
  We then single out conditions on the 2-monad $T$ and the classifier that make this possible, and observe these hold in a wide range of examples: double categories (recovering the results of Parè and Lambert), (symmetric) monoidal categories, and all structures encoded by familial 2-monads. We also prove the properties needed on such 2-monads are stable under replacement by pseudo-algebra coclassifiers (when sufficient exactness conditions hold), allowing us to replace a pseudo-algebra structure on the classifier by a strict one.
  To get to our main theorem, we introduce the concepts of \emph{cartesian maps} and \emph{cartesian objects} of a 2-algebra, which generalize various other notions in category theory such as cartesian monoidal categories, extensive categories, categories with descent, and more. As a corollary, we characterize when representable copresheaves are pseudo rather than lax in terms of the cartesianity at their representing object."
2510.23530,Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization,"['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS']","['Bernardo Torres', 'Manuel Moussallam', 'Gabriel Meseguer-Brocal']","Audio autoencoders learn useful, compressed audio representations, but their non-linear latent spaces prevent intuitive algebraic manipulation such as mixing or scaling. We introduce a simple training methodology to induce linearity in a high-compression Consistency Autoencoder (CAE) by using data augmentation, thereby inducing homogeneity (equivariance to scalar gain) and additivity (the decoder preserves addition) without altering the model's architecture or loss function. When trained with our method, the CAE exhibits linear behavior in both the encoder and decoder while preserving reconstruction fidelity. We test the practical utility of our learned space on music source composition and separation via simple latent arithmetic. This work presents a straightforward technique for constructing structured latent spaces, enabling more intuitive and efficient audio processing."
2510.23529,On generalized inverses of matrices associated with certain graph classes,"['math.CO', 'math.RA', 'math.SP']","['Cláudia M. Araújo', 'Faustino A. Maciala', 'Pedro Patrício']","We investigate generalized inverses of matrices associated with two classes of digraphs: double star digraphs and D-linked stars digraphs. For double star digraphs, we determine the Drazin index and derive explicit formulas for the Drazin inverse. We also provide necessary and sufficient conditions for the existence of the Moore-Penrose inverse and give its explicit expression whenever it exists. For D-linked stars digraphs, we characterize when the group inverse exists and obtain its explicit form. In the singular case where BC = 0, we express the Drazin index of the matrix in terms of the Drazin index of the base digraph matrix. Additionally, we establish necessary and sufficient conditions for Moore--Penrose invertibility and derive explicit formulas in that case. Our results reveal a clear connection between the algebraic structure of generalized inverses and the combinatorial properties of these graph classes, providing a unified framework for group, Drazin, and Moore-Penrose invertibility."
2510.23528,Tracing Distribution Shifts with Causal System Maps,['cs.SE'],"['Joran Leest', 'Ilias Gerostathopoulos', 'Patricia Lago', 'Claudia Raibulet']","Monitoring machine learning (ML) systems is hard, with standard practice focusing on detecting distribution shifts rather than their causes. Root-cause analysis often relies on manual tracing to determine whether a shift is caused by software faults, data-quality issues, or natural change. We propose ML System Maps -- causal maps that, through layered views, make explicit the propagation paths between the environment and the ML system's internals, enabling systematic attribution of distribution shifts. We outline the approach and a research agenda for its development and evaluation."
2510.23527,$Γ$-convergence for higher order nonlocal phase transitions,['math.AP'],"['Hardy Chan', 'Serena Dipierro', 'Mattia Freguglia', 'Marco Inversi', 'Enrico Valdinoci']","For every $0 < s <3/4$, we study the asymptotic behavior of the $\varepsilon$-rescaled sum of the $s$-fractional Allen-Cahn energy and the squared $L^2$-norm of its first variation. We prove that the contribution of the first variation vanishes as $\varepsilon \to 0$. This implies the Gamma-convergence of the initial sum to either the classical perimeter or to the $2s$-fractional perimeter, depending on whether $s \ge 1/2$ or not. This contradicts the expectation of finding curvature-dependent terms in the limit, as suggested by the regime $3/4 \le s < 1$, and as known to hold in low dimensions in the local case."
2510.23526,Unveiling the collision between molecular outflows: observational evidence and hydrodynamic simulations,['astro-ph.GA'],"['E. Cohen Arazi', 'P. F. Velázquez', 'M. E. Ortega', 'A. Rodríguez-González', 'E. Alquicira-Peláez', 'S. Paron', 'P. Rivera-Ortiz', 'A. Esquivel']","We present an unexplored scenario for interpreting the outflows in the EGO G338.92+0.55 (b) region (hereafter, EGO G338). Within this framework, we investigate the hypothesis that the interaction between two outflows is responsible for the observed morphology and kinematics of this astrophysical object. To explore this possibility, we reanalyse the region using observational molecular line data. We base our analysis on maps of moments 0, 1, and 2 of the CO emission associated with the molecular outflows. Additionally, we conduct three-dimensional hydrodynamic simulations to examine the presence or absence of a collision between two jets. From our numerical results, we produce synthetic CO images to facilitate a direct comparison with observations. The findings of this study provide compelling evidence that the observed morphology and kinematics in the EGO G338 region are the result of a likely collision between two molecular outflows."
2510.23525,DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation,"['cs.CV', 'cs.RO']","['Wanmeng Li', 'Simone Mosco', 'Daniel Fusaro', 'Alberto Pretto']","Annotating real-world LiDAR point clouds for use in intelligent autonomous systems is costly. To overcome this limitation, self-training-based Unsupervised Domain Adaptation (UDA) has been widely used to improve point cloud semantic segmentation by leveraging synthetic point cloud data. However, we argue that existing methods do not effectively utilize unlabeled data, as they either rely on predefined or fixed confidence thresholds, resulting in suboptimal performance. In this paper, we propose a Dynamic Pseudo-Label Filtering (DPLF) scheme to enhance real data utilization in point cloud UDA semantic segmentation. Additionally, we design a simple and efficient Prior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift between synthetic and real-world point clouds. Finally, we utilize data mixing consistency loss to push the model to learn context-free representations. We implement and thoroughly evaluate our approach through extensive comparisons with state-of-the-art methods. Experiments on two challenging synthetic-to-real point cloud semantic segmentation tasks demonstrate that our approach achieves superior performance. Ablation studies confirm the effectiveness of the DPLF and PG-DAP modules. We release the code of our method in this paper."
2510.23524,"Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence","['cs.AI', 'cs.LG']","['KC Santosh', 'Rodrigue Rizk', 'Longwei Wang']","The rapid advancement of Artificial Intelligence (AI) has led to unprecedented computational demands, raising significant environmental and ethical concerns. This paper critiques the prevailing reliance on large-scale, static datasets and monolithic training paradigms, advocating for a shift toward human-inspired, sustainable AI solutions. We introduce a novel framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware optimization, and human-in-the-loop collaboration to enhance adaptability, efficiency, and accountability. By drawing parallels with biological cognition and leveraging dynamic architectures, HAI seeks to balance performance with ecological responsibility. We detail the theoretical foundations, system design, and operational principles that enable AI to learn continuously and contextually while minimizing carbon footprints and human annotation costs. Our approach addresses pressing challenges in active learning, continual adaptation, and energy-efficient model deployment, offering a pathway toward responsible, human-centered artificial intelligence."
2510.23523,From Perceived Effectiveness to Measured Impact: Identity-Aware Evaluation of Automated Counter-Stereotypes,['cs.CY'],"['Svetlana Kiritchenko', 'Anna Kerkhof', 'Isar Nejadgholi', 'Kathleen C. Fraser']","We investigate the effect of automatically generated counter-stereotypes on gender bias held by users of various demographics on social media. Building on recent NLP advancements and social psychology literature, we evaluate two counter-stereotype strategies -- counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of group membership) -- which have been identified as the most potentially effective in previous studies. We assess the real-world impact of these strategies on mitigating gender bias across user demographics (gender and age), through the Implicit Association Test and the self-reported measures of explicit bias and perceived utility. Our findings reveal that actual effectiveness does not align with perceived effectiveness, and the former is a nuanced and sometimes divergent phenomenon across demographic groups. While overall bias reduction was limited, certain groups (e.g., older, male participants) exhibited measurable improvements in implicit bias in response to some interventions. Conversely, younger participants, especially women, showed increasing bias in response to the same interventions. These results highlight the complex and identity-sensitive nature of stereotype mitigation and call for dynamic and context-aware evaluation and mitigation strategies."
2510.23522,Galactic bars and active galactic nucleus fuelling in the second half of cosmic history,['astro-ph.GA'],"['A. La Marca', 'M. T. Nardone', 'L. Wang', 'B. Margalef-Bentabol', 'S. Kruk', 'S. C. Trager']","We investigate the role of galactic bars in fuelling and triggering Active Galactic Nucleus (AGN) in disc galaxies up to $z\sim 0.8$. We utilise a Deep Learning model, fine-tuned on Galaxy Zoo volunteer classifications, to identify (strongly and weakly) barred and unbarred disc galaxies in Hyper Suprime-Cam Subaru Strategic Program $i$-band images. We select AGN using three independent diagnostics: mid-infrared colours, X-ray detections, and spectral energy distribution (SED) fitting. The SED analysis, performed using CIGALE, quantifies the relative AGN contribution to the total galaxy luminosity ($f_{\rm AGN}$) and the AGN luminosity ($L_{\rm disc}$). We assess the impact of bars by comparing AGN incidence and properties in barred galaxies against carefully constructed redshift-, stellar mass-, and colour-matched unbarred control samples. Our binary AGN classification experiment demonstrates that barred disc galaxies host a statistically detectable higher fraction of AGN compared to their unbarred counterparts, suggesting a contributing role for bars in the global AGN budget. The contribution of bars to AGN fuelling appears confined to systems where the AGN has a lower relative contribution to the host galaxy's emission ($f_{\rm AGN} < 0.75$). Crucially, we find a significant dearth of barred disc galaxies hosting AGN with $f_{\rm AGN} > 0.75$, independent of bar strength. Consistent with this, the fraction of barred galaxies among AGN hosts decreases with increasing $L_{\rm disc}$. Combined with previous results, we suggest that bars contribute to fuelling the population of low-to-moderate luminosity AGN, but major mergers are the principal mechanism for triggering the most powerful and dominant accretion events."
2510.23521,Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation,"['cs.RO', 'doi', '10.1109/LRA.2025.3619783']","['Anthony Opipari', 'Aravindhan K Krishnan', 'Shreekant Gayaka', 'Min Sun', 'Cheng-Hao Kuo', 'Arnie Sen', 'Odest Chadwicke Jenkins']","Remembering where object segments were predicted in the past is useful for improving the accuracy and consistency of class-agnostic video segmentation algorithms. Existing video segmentation algorithms typically use either no object-level memory (e.g. FastSAM) or they use implicit memories in the form of recurrent neural network features (e.g. SAM2). In this paper, we augment both types of segmentation models using an explicit 3D memory and show that the resulting models have more accurate and consistent predictions. For this, we develop an online 3D Gaussian Splatting (3DGS) technique to store predicted object-level segments generated throughout the duration of a video. Based on this 3DGS representation, a set of fusion techniques are developed, named FastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve their respective foundation models' predictions. Ablation experiments are used to validate the proposed techniques' design and hyperparameter settings. Results from both real-world and simulated benchmarking experiments show that models which use explicit 3D memories result in more accurate and consistent predictions than those which use no memory or only implicit neural network memories. Project Page: https://topipari.com/projects/FastSAM-Splat/"
2510.23520,The critical case for the concentration of eigenfunctions on singular Riemannian manifolds,"['math.SP', 'math-ph', 'math.AP']",['Charlotte Dietze'],We consider a compact Riemannian manifold with boundary with a certain class of critical singular Riemannian metrics that are singular at the boundary. The corresponding Laplace-Beltrami operator can be seen as a Grushin-type operator plus a potential. We show in the critical case that the average density of eigenfunctions for the Laplace-Beltrami operator with eigenvalues below $λ>0$ is distributed over all length scales between $λ^{-1/2}$ and $1$ near the boundary. We give a precise description of this distribution as $λ\to\infty$.
2510.23519,Architecting Scalable Trapped Ion Quantum Computers using Surface Codes,"['quant-ph', 'cs.AR']","['Scott Jones', 'Prakash Murali']","Trapped ion (TI) qubits are a leading quantum computing platform. Current TI systems have less than 60 qubits, but a modular architecture known as the Quantum Charge-Coupled Device (QCCD) is a promising path to scale up devices. There is a large gap between the error rates of near-term systems ($10^{-3}$ to $10^{-4}$) and the requirements of practical applications (below $10^{-9}$). To bridge this gap, we require Quantum Error Correction (QEC) to build \emph{logical qubits} that are composed of multiple physical qubits. While logical qubits have been demonstrated on TI qubits, these demonstrations are restricted to small codes and systems. There is no clarity on how QCCD systems should be designed to implement practical-scale QEC. This paper studies how surface codes, a standard QEC scheme, can be implemented efficiently on QCCD-based systems. To examine how architectural parameters of a QCCD system can be tuned for surface codes, we develop a near-optimal topology-aware compilation method that outperforms existing QCCD compilers by an average of 3.8X in terms of logical clock speed. We use this compiler to examine how hardware trap capacity, connectivity and electrode wiring choices can be optimised for surface code implementation. In particular, we demonstrate that small traps of two ions are surprisingly ideal from both a performance-optimal and hardware-efficiency standpoint. This result runs counter to prior intuition that larger traps (20-30 ions) would be preferable, and has the potential to inform design choices for upcoming systems."
2510.23518,"Group pairs, coherence and Farrell--Jones Conjecture for $K_0$",['math.GR'],"['Andrei Jaikin-Zapirain', 'Marco Linton', 'Pablo Sánchez-Peralta']","A group pair $(G, X)$ consists of a group $G$ together with a $G$-set $X$. Such a pair encodes properties of $G$ relative to the stabilisers of points in $X$. In this paper, we show how to combine properties of group pairs and their stabilisers to prove coherence results for $G$ and its group algebra, as well as to study the quotient of $G$ obtained by killing the stabilisers.
  In particular, we prove that a torsion-free one-relator product of locally indicable groups is coherent provided that both factor groups are coherent. Moreover, we show that the group algebra of such a group over a field of characteristic $0$ is coherent whenever the group algebras of the factors are coherent.
  As other consequences of our methods, we also show that extensions of coherent locally indicable hyperbolic groups by $\mathbb{Z}$ are coherent and that groups admitting a Cohen--Lyndon presentation satisfy the Farrell--Jones Conjecture for $K_{0}$."
2510.23517,"Linear effects, exceptions, and resource safety: a Curry-Howard correspondence for destructors","['cs.PL', 'cs.LO']","['Sidney Congard', 'Guillaume Munch-Maccagnoni', 'Rémi Douence']","We analyse the problem of combining linearity, effects, and exceptions, in abstract models of programming languages, as the issue of providing some kind of strength for a monad $T(- \oplus E)$ in a linear setting. We consider in particular for $T$ the allocation monad, which we introduce to model and study resource-safety properties. We apply these results to a series of two linear effectful calculi for which we establish their resource-safety properties.
  The first calculus is a linear call-by-push-value language with two allocation effects $\mathit{new}$ and $\mathit{delete}$. The resource-safety properties follow from the linear (and even ordered) character of the typing rules.
  We then explain how to integrate exceptions on top of linearity and effects by adjoining default destruction actions to types, as inspired by C++/Rust destructors. We see destructors as objects $δ: A\rightarrow TI$ in the slice category over $TI$. This construction gives rise to a second calculus, an affine ordered call-by-push-value language with exceptions and destructors, in which the weakening rule performs a side-effect. As in C++/Rust, a ``move'' operation is necessary to allow random-order release of resources, as opposed to last-in-first-out order. Moving resources is modelled as an exchange rule that performs a side-effect."
2510.23516,High-Speed Imagery Analysis of Droplet Impact on Van der Waals and Non-Van der Waals Soft-Textured Oil-Infused Surfaces,"['physics.flu-dyn', 'cond-mat.mtrl-sci']","['Shubham S. Ganar', 'Deepak J.', 'Arindam Das']","This study investigates the impact of surface functionalization, oil coating, and oil absorption on droplet impact behavior on textured polydimethylsiloxane(PDMS) substrates. The textured surfaces were fabricated with square micro-posts having spacings of 5 and 20 microns. The PDMS samples were functionalized with octadecyltrichlorosilane (OTS) to improve water repellency. Following, the surfaces were either coated with or allowed to absorb two different lubricants, silicone oil (SO-5cSt) and hexadecane. We performed detailed wetting measurements on both untreated and OTS-functionalized substrates. These measurements provided useful insights into how water and lubricants were retained and distributed under static conditions. High-speed imaging was used to capture droplet impact across a range of Weber numbers. On SO-5cSt-absorbed substrates, droplets consistently showed complete rebound at all Weber numbers, regardless of post spacing. This robust rebound was attributed to the oil's ability to fill the gaps between the posts through capillary action, while also forming a stable lubricating layer above the texture. This thin oil film reduced friction between the droplet and the surface, enabling the droplet to retain sufficient energy for complete rebound. In contrast, hexadecane-absorbed substrates displayed different dynamics. At low Weber numbers, only partial rebound was observed, while at intermediate values, droplets rebounded completely. However, droplets no longer rebounded at higher Weber numbers and remained deposited. Repeated droplet impacts further demonstrated that hexadecane-infused surfaces gradually lost oil from the textured gaps, resulting in a decline in rebound performance over time. This effect was not observed with SO-5cSt, underscoring the importance of lubricant affinity and stability."
2510.23515,FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time,['cs.CV'],"['Yaoli Liu', 'Yao-Xiang Ding', 'Kun Zhou']","This paper proposes FreeFuse, a novel training-free approach for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to existing methods that either focus on pre-inference LoRA weight merging or rely on segmentation models and complex techniques like noise blending to isolate LoRA outputs, our key insight is that context-aware dynamic subject masks can be automatically derived from cross-attention layer weights. Mathematical analysis shows that directly applying these masks to LoRA outputs during inference well approximates the case where the subject LoRA is integrated into the diffusion model and used individually for the masked region. FreeFuse demonstrates superior practicality and efficiency as it requires no additional training, no modification to LoRAs, no auxiliary models, and no user-defined prompt templates or region specifications. Alternatively, it only requires users to provide the LoRA activation words for seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both generation quality and usability under the multi-subject generation tasks. The project page is at https://future-item.github.io/FreeFuse/"
2510.23514,DeFecT-FF: Accelerated Modeling of Defects in Cd-Zn--Te-Se-S Compounds Combining High-Throughput DFT and Machine Learning Force Fields,"['cond-mat.mtrl-sci', 'cond-mat.dis-nn']","['Md Habibur Rahman', 'Arun Mannodi-Kanakkithodi']","We developed DeFecT-FF, a framework for predicting the energies and ground-state configurations of native point defects, extrinsic dopants, impurities, and defect complexes in zincblende-phase Cd/Zn-Te/Se/S compounds relevant to CdTe-based solar cells. The framework combines high-throughput DFT data with crystal graph-based machine learning force fields (MLFFs) trained to reproduce DFT energies and forces. Alloying at Cd or Te sites offers a route to tune the electronic and defect properties of CdTe absorbers for improved solar efficiency. Given the vast number of possible defect types, charge states, and symmetry-breaking configurations, traditional DFT approaches are computationally prohibitive. Our dataset includes GGA-PBE and HSE06-optimized structures for bulk, alloyed, interface, and grain-boundary systems. Using active learning, we expanded the dataset and trained MLFFs to accurately predict energies across charge states. The model enabled rapid screening and discovery of new low-energy defect configurations, validated through HSE06 calculations with spin-orbit coupling. The DeFecT-FF framework is publicly available as a nanoHUB tool, allowing users to upload crystallographic files, automatically generate defects, and compute defect formation energies versus Fermi level and chemical potentials, greatly reducing the need for expensive DFT simulations."
2510.23513,Point Convergence of Nesterov's Accelerated Gradient Method: An AI-Assisted Proof,['math.OC'],"['Uijeong Jang', 'Ernest K. Ryu']","The Nesterov accelerated gradient method, introduced in 1983, has been a cornerstone of optimization theory and practice. Yet the question of its point convergence had remained open. In this work, we resolve this longstanding open problem in the affirmative. The discovery of the proof was heavily assisted by ChatGPT, a proprietary large language model, and we describe the process through which its assistance was"
2510.23512,Localising under the drape: proprioception in the era of distributed surgical robotic system,"['cs.RO', 'cs.CV']","['Martin Huber', 'Nicola A. Cavalcanti', 'Ayoob Davoodi', 'Ruixuan Li', 'Christopher E. Mower', 'Fabio Carrillo', 'Christoph J. Laux', 'Francois Teyssere', 'Thibault Chandanson', 'Antoine Harlé', 'Elie Saghbiny', 'Mazda Farshad', 'Guillaume Morel', 'Emmanuel Vander Poorten', 'Philipp Fürnstahl', 'Sébastien Ourselin', 'Christos Bergeles', 'Tom Vercauteren']","Despite their mechanical sophistication, surgical robots remain blind to their surroundings. This lack of spatial awareness causes collisions, system recoveries, and workflow disruptions, issues that will intensify with the introduction of distributed robots with independent interacting arms. Existing tracking systems rely on bulky infrared cameras and reflective markers, providing only limited views of the surgical scene and adding hardware burden in crowded operating rooms. We present a marker-free proprioception method that enables precise localisation of surgical robots under their sterile draping despite associated obstruction of visual cues. Our method solely relies on lightweight stereo-RGB cameras and novel transformer-based deep learning models. It builds on the largest multi-centre spatial robotic surgery dataset to date (1.4M self-annotated images from human cadaveric and preclinical in vivo studies). By tracking the entire robot and surgical scene, rather than individual markers, our approach provides a holistic view robust to occlusions, supporting surgical scene understanding and context-aware control. We demonstrate an example of potential clinical benefits during in vivo breathing compensation with access to tissue dynamics, unobservable under state of the art tracking, and accurately locate in multi-robot systems for future intelligent interaction. In addition, and compared with existing systems, our method eliminates markers and improves tracking visibility by 25%. To our knowledge, this is the first demonstration of marker-free proprioception for fully draped surgical robots, reducing setup complexity, enhancing safety, and paving the way toward modular and autonomous robotic surgery."
2510.23511,Dexbotic: Open-Source Vision-Language-Action Toolbox,['cs.RO'],"['Bin Xie', 'Erjin Zhou', 'Fan Jia', 'Hao Shi', 'Haoqiang Fan', 'Haowei Zhang', 'Hebei Li', 'Jianjian Sun', 'Jie Bin', 'Junwen Huang', 'Kai Liu', 'Kaixin Liu', 'Kefan Gu', 'Lin Sun', 'Meng Zhang', 'Peilong Han', 'Ruitao Hao', 'Ruitao Zhang', 'Saike Huang', 'Songhan Xie', 'Tiancai Wang', 'Tianle Liu', 'Wenbin Tang', 'Wenqi Zhu', 'Yang Chen']","In this paper, we present Dexbotic, an open-source Vision-Language-Action (VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA research service for professionals in the field of embodied intelligence. It offers a codebase that supports multiple mainstream VLA policies simultaneously, allowing users to reproduce various VLA methods with just a single environment setup. The toolbox is experiment-centric, where the users can quickly develop new VLA experiments by simply modifying the Exp script. Moreover, we provide much stronger pretrained models to achieve great performance improvements for state-of-the-art VLA policies. Dexbotic will continuously update to include more of the latest pre-trained foundation models and cutting-edge VLA models in the industry."
2510.23510,How to build a sovereign network? - A proposal to measure network sovereignty,['cs.NI'],"['Shakthivelu Janardhanan', 'Ritanshi Agarwal', 'Wolfgang Kellerer', 'Carmen Mas-Machuca']","Network sovereignty is a network operator's ability to reduce the dependency on component manufacturers to minimize the impact of manufacturer failures. Network operators now face new design challenges to increase network sovereignty and avoid vendor lock-in problems because a high dependency on a manufacturer corresponds to low survivability if that manufacturer is unavailable. The main contribution of this work is the proposal of a novel metric to measure network sovereignty, the Cut Set Coloring (CSC) score. Based on the CSC core metric CSC-ILP, our Integer Linear Program formulation is presented to maximize network sovereignty. We compare CSC-ILP's performance with state of the art manufacturer assignment strategies."
2510.23509,Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model,['cs.RO'],"['Weizheng Wang', 'Obi Ike', 'Soyun Choi', 'Sungeun Hong', 'Byung-Cheol Min']","Social robot navigation increasingly relies on large language models for reasoning, path planning, and enabling movement in dynamic human spaces. However, relying solely on LLMs for planning often leads to unpredictable and unsafe behaviors, especially in dynamic human spaces, due to limited physical grounding and weak logical consistency. In this work, we introduce NaviWM, a socially-aware robot Navigation World Model that augments LLM reasoning with a structured world model and a logic-driven chain-of-thought process. NaviWM consists of two main components: (1) a spatial-temporal world model that captures the positions, velocities, and activities of agents in the environment, and (2) a deductive reasoning module that guides LLMs through a multi-step, logic-based inference process. This integration enables the robot to generate navigation decisions that are both socially compliant and physically safe, under well-defined constraints such as personal space, collision avoidance, and timing. Unlike previous methods based on prompting or fine-tuning, NaviWM encodes social norms as first-order logic, enabling interpretable and verifiable reasoning. Experiments show that NaviWM improves success rates and reduces social violations, particularly in crowded environments. These results demonstrate the benefit of combining formal reasoning with LLMs for robust social navigation. Additional experimental details and demo videos for this work can be found at: https://sites.google.com/view/NaviWM."
2510.23508,"M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset",['cs.CL'],"['Jiahui Geng', 'Jonathan Tonglet', 'Iryna Gurevych']","Existing real-world datasets for multimodal automated fact-checking have multiple limitations: they contain few instances, focus on only one or two languages and tasks, suffer from evidence leakage, or depend on external sets of news articles for sourcing true claims. To address these shortcomings, we introduce M4FC, a new real-world dataset comprising 4,982 images paired with 6,980 claims. The images, verified by professional fact-checkers from 22 organizations, represent diverse cultural and geographic contexts. Each claim is available in one or two out of ten languages. M4FC spans six multimodal fact-checking tasks: visual claim extraction, claimant intent prediction, fake detection, image contextualization, location verification, and verdict prediction. We provide baseline results for all tasks and analyze how combining intermediate tasks influence downstream verdict prediction performance. We make our dataset and code available."
2510.23507,A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off Perspective,"['cs.LG', 'cs.AI', 'cs.IT']","['Siamak Ghodsi', 'Amjad Seyedi', 'Tai Le Quy', 'Fariba Karimi', 'Eirini Ntoutsi']","Fair graph clustering seeks partitions that respect network structure while maintaining proportional representation across sensitive groups, with applications spanning community detection, team formation, resource allocation, and social network analysis. Many existing approaches enforce rigid constraints or rely on multi-stage pipelines (e.g., spectral embedding followed by $k$-means), limiting trade-off control, interpretability, and scalability. We introduce \emph{DFNMF}, an end-to-end deep nonnegative tri-factorization tailored to graphs that directly optimizes cluster assignments with a soft statistical-parity regularizer. A single parameter $λ$ tunes the fairness--utility balance, while nonnegativity yields parts-based factors and transparent soft memberships. The optimization uses sparse-friendly alternating updates and scales near-linearly with the number of edges. Across synthetic and real networks, DFNMF achieves substantially higher group balance at comparable modularity, often dominating state-of-the-art baselines on the Pareto front. The code is available at https://github.com/SiamakGhodsi/DFNMF.git."
2510.23506,Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier,"['cs.AI', 'cs.HC']","['Hyeongseop Rha', 'Jeong Hun Yeo', 'Yeonju Kim', 'Yong Man Ro']","The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems."
2510.23505,Unveiling stellar (and planetary) internal dynamics with the fully compressible MUSIC code,"['astro-ph.SR', 'astro-ph.EP', 'astro-ph.IM']","['Arthur Le Saux', 'Isabelle Baraffe', 'Thomas Guillet', 'Jane Pratt', 'Tom Goffrey', 'Dimitar Vlaykov', 'Adrien Morison', 'Jack Morton', 'Maxime Stuck', 'Mary Geer Dethero', 'Nils de Vries']","Multidimensional hydrodynamical simulations have transformed the study of stellar interiors over the past few decades. Most codes developed during that time use the anelastic approximation, which fixes the thermal structure of simulations and filters out sound waves. Many of them also use explicit time integration, which imposes severe constraints on the time step of the simulations. In this context, MUSIC is developed to overcome these limitations. Its main scientific objective is to improve the phenomenological approaches used in 1D stellar evolution codes to describe major hydrodynamical and MHD processes. Here, we review recent applications of the MUSIC code, that focus mainly on convection, convective boundary mixing and waves in stars that possess convective cores, shells and/or envelopes."
2510.23504,iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification,['cs.CV'],"['Usama Zidan', 'Mohamed Gaber', 'Mohammed M. Abdelsamea']","Graph neural networks have emerged as a promising paradigm for image processing, yet their performance in image classification tasks is hindered by a limited consideration of the underlying structure and relationships among visual entities. This work presents iPac, a novel approach to introduce a new graph representation of images to enhance graph neural network image classification by recognizing the importance of underlying structure and relationships in medical image classification. iPac integrates various stages, including patch partitioning, feature extraction, clustering, graph construction, and graph-based learning, into a unified network to advance graph neural network image classification. By capturing relevant features and organising them into clusters, we construct a meaningful graph representation that effectively encapsulates the semantics of the image. Experimental evaluation on diverse medical image datasets demonstrates the efficacy of iPac, exhibiting an average accuracy improvement of up to 5% over baseline methods. Our approach offers a versatile and generic solution for image classification, particularly in the realm of medical images, by leveraging the graph representation and accounting for the inherent structure and relationships among visual entities."
2510.23503,Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems,"['cs.DC', 'cs.LG', 'eess.SP']","['Fatemeh Zahra Safaeipour', 'Jacob Chakareski', 'Morteza Hashemi']","Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely inference tasks while operating with limited on-board computing and energy resources. In this paper, we investigate the problem of collaborative inference in wireless edge networks, where energy-constrained edge devices aim to complete inference tasks within given deadlines. These tasks are carried out using neural networks, and the edge device seeks to optimize inference performance under energy and delay constraints. The inference process can be split between the edge device and an edge server, thereby achieving collaborative inference over wireless networks. We formulate an inference utility optimization problem subject to energy and delay constraints, and propose a novel solution called Bayes-Split-Edge, which leverages Bayesian optimization for collaborative split inference over wireless edge networks. Our solution jointly optimizes the transmission power and the neural network split point. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition function that balances inference task utility, sample efficiency, and constraint violation penalties. We evaluate our approach using the VGG19 model on the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world mMobile wireless channel datasets. Numerical results demonstrate that Bayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to standard Bayesian optimization and achieves near-linear convergence. It also outperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and Proximal Policy Optimization (PPO), while matching exhaustive search performance under tight constraints. These results confirm that the proposed framework provides a sample-efficient solution requiring maximum 20 function evaluations and constraint-aware optimization for wireless split inference in edge computing systems."
2510.23502,Separation of gain fluctuations and continuum signals in total power spectrometers with application to COMAP,['astro-ph.IM'],"['J. G. S. Lunde', 'P. C. Breysse', 'D. T. Chung', 'K. A. Cleary', 'C. Dickinson', 'D. A. Dunne', 'J. O. Gundersen', 'S. E. Harper', 'G. A. Hoerning', 'H. T. Ihle', 'J. W. Lamb', 'T. J. Pearson', 'T. J. Rennie', 'N. -O. Stutzer']","We describe a time-domain technique for separating $1/f$ gain fluctuations and continuum signal for a total power spectrometer, such as the CO Mapping Array Project (COMAP) Pathfinder instrument. The $1/f$ gain fluctuations of such a system are expected to be common-mode across frequency channels. If the instrument's system temperature is not constant across channels, a continuum signal will exhibit a frequency dependence different from that of common-mode gain fluctuations. Our technique leverages this difference to fit a three-parameter frequency model to each time sample in the time-domain data, separating gain and continuum. We show that this technique can be applied to the COMAP Pathfinder instrument, which exhibits a series of temporally stable resonant noise spikes that effectively act as calibrators, breaking the gain degeneracy with continuum signals. Using both simulations and observations of Jupiter, we explore the effect of a $1/f$ prior for the gain model. We show that the model is capable of cleanly separating Jupiter, a bright continuum source, from the gain fluctuations in the scan. The technique has two applications to COMAP. For the COMAP observations performing line intensity mapping (LIM), the technique better suppresses atmospheric fluctuations and foregrounds than the COMAP LIM pipeline. For the Galactic COMAP observations, which map Galactic continuum signals, the technique can suppress $1/f$ gain fluctuations while retaining all continuum signals. This is demonstrated by the latest COMAP observations of $λ$-Orionis, where our method produces far cleaner maps than a destriper alone, typically reducing the noise power by a factor of 7 on beam scales and up to 15 on larger scales."
2510.23501,Towards Deep Physics-Informed Kolmogorov-Arnold Networks,"['cs.LG', 'physics.comp-ph']","['Spyros Rigas', 'Fotios Anagnostopoulos', 'Michalis Papachristou', 'Georgios Alexandridis']","Since their introduction, Kolmogorov-Arnold Networks (KANs) have been successfully applied across several domains, with physics-informed machine learning (PIML) emerging as one of the areas where they have thrived. In the PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the standard due to their computational efficiency. However, like their multilayer perceptron-based counterparts, cPIKANs face significant challenges when scaled to depth, leading to training instabilities that limit their applicability to several PDE problems. To address this, we propose a basis-agnostic, Glorot-like initialization scheme that preserves activation variance and yields substantial improvements in stability and accuracy over the default initialization of cPIKANs. Inspired by the PirateNet architecture, we further introduce Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in deep cPIKANs where initialization alone is not sufficient. Through empirical tests and information bottleneck analysis, we show that RGA KANs successfully traverse all training phases, unlike baseline cPIKANs, which stagnate in the diffusion phase in specific PDE settings. Evaluations on seven standard forward PDE benchmarks under a fixed training pipeline with adaptive components demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and PirateNets - often by several orders of magnitude - while remaining stable in settings where the others diverge."
2510.23500,Beyond the Trade-off Curve: Multivariate and Advanced Risk-Utility Maps for Evaluating Anonymized and Synthetic Data,"['stat.AP', 'stat.ME']","['Oscar Thees', 'Roman Müller', 'Matthias Templ']","Anonymizing microdata requires balancing the reduction of disclosure risk with the preservation of data utility. Traditional evaluations often rely on single measures or two-dimensional risk-utility (R-U) maps, but real-world assessments involve multiple, often correlated, indicators of both risk and utility. Pairwise comparisons of these measures can be inefficient and incomplete. We therefore systematically compare six visualization approaches for simultaneous evaluation of multiple risk and utility measures: heatmaps, dot plots, composite scatterplots, parallel coordinate plots, radial profile charts, and PCA-based biplots. We introduce blockwise PCA for composite scatterplots and joint PCA for biplots that simultaneously reveal method performance and measure interrelationships. Through systematic identification of Pareto-optimal methods in all approaches, we demonstrate how multivariate visualization supports a more informed selection of anonymization methods."
2510.23499,On the rate of convergence of cylindrical singularity in mean curvature flow,"['math.DG', 'math.AP']","['Yiqi Huang', 'Xinrui Zhao']","We prove that if a rescaled mean curvature flow is a global graph over the round cylinder with small gradient and converges super-exponentially fast, then it must coincide with the cylinder itself. We also show that the result is sharp with counter-examples of local graphs at arbitrarily super-exponential convergence rate with the domain expanding arbitrarily fast.
  The first part provides the first unique continuation result in the cylindrical setting, the generic singularity model in mean curvature flow. In sharp contrast, in the second part we construct infinite-dimensional families of Tikhonov-type examples for nonlinear equations, including the rescaled mean curvature flow, showing that unique continuation fails for local graphical solutions. These examples demonstrate the essential role of global graphical assumptions in rigidity and highlight new phenomena absent in the compact case. We also construct non-product mean curvature flows that develop singular sets as prescribed lower dimensional Euclidean space at arbitrary super-exponential rates. Our construction works in great generality for a large class of non-linear equations."
2510.23498,Mixed Precision Training of Neural ODEs,"['cs.LG', 'cs.AI', 'math.NA']","['Elena Celledoni', 'Brynjulf Owren', 'Lars Ruthotto', 'Tianjiao Nicole Yang']","Exploiting low-precision computations has become a standard strategy in deep learning to address the growing computational costs imposed by ever larger models and datasets. However, naively performing all computations in low precision can lead to roundoff errors and instabilities. Therefore, mixed precision training schemes usually store the weights in high precision and use low-precision computations only for whitelisted operations. Despite their success, these principles are currently not reliable for training continuous-time architectures such as neural ordinary differential equations (Neural ODEs). This paper presents a mixed precision training framework for neural ODEs, combining explicit ODE solvers with a custom backpropagation scheme, and demonstrates its effectiveness across a range of learning tasks. Our scheme uses low-precision computations for evaluating the velocity, parameterized by the neural network, and for storing intermediate states, while stability is provided by a custom dynamic adjoint scaling and by accumulating the solution and gradients in higher precision. These contributions address two key challenges in training neural ODE: the computational cost of repeated network evaluations and the growth of memory requirements with the number of time steps or layers. Along with the paper, we publish our extendable, open-source PyTorch package rampde, whose syntax resembles that of leading packages to provide a drop-in replacement in existing codes. We demonstrate the reliability and effectiveness of our scheme using challenging test cases and on neural ODE applications in image classification and generative models, achieving approximately 50% memory reduction and up to 2x speedup while maintaining accuracy comparable to single-precision training."
2510.23497,VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation,['cs.CV'],"['Walid Bousselham', 'Hilde Kuehne', 'Cordelia Schmid']","Training vision-language models (VLMs) for complex reasoning remains a challenging task, i.a. due to the scarcity of high-quality image-text reasoning data. Conversely, text-based reasoning resources are abundant and scalable, but it is still an open question how to leveraging them for VLM reasoning. To address this problem, we propose VOLD, a framework to transfer reasoning capabilities from text-only teacher models to VLM student models. To this end, VOLD combines reinforcement learning via Group Relative Policy Optimization (GRPO) with on-policy distillation, which allows the student reasoning traces to be guided by the teacher model, resulting in a significant gain over using GRPO alone. We further show that a cold-start alignment is essential for an effective transfer during the online training phase in this scenario and that without sufficient distributional alignment between teacher and student, on-policy distillation fails to provide meaningful guidance. We evaluate VOLD across diverse benchmarks including MMMU-Pro, MathVision, MathVista, and LogicVista, showing that VOLD outperforms the baseline model significantly and improves over the state of the art by a margin. Our ablation shows the importance of a cold-start alignment via SFT for on-policy distillation with a text-only teacher."
2510.23496,Crystallization of discrete $N$-particle systems at high temperature,"['math-ph', 'math.CO', 'math.PR', 'math.SP']","['Cesar Cuenca', 'Maciej Dołęga']","This is the second paper in a series studying the global asymptotics of discrete $N$-particle systems with inverse temperature parameter $θ$ in the high temperature regime. In the first paper, we established necessary and sufficient conditions for the Law of Large Numbers at high temperature in terms of Jack generating functions. In this paper, we derive a functional equation for the moment generating function of the limiting measure, which enables its analysis using analytic tools. We apply this functional equation to compute the densities of the high temperature limits of the pure Jack measures. As a special case, we obtain the high temperature limit of the large fixed-time distribution of the discrete-space $β$-Dyson Brownian motion of Gorin-Shkolnikov. Two special cases of our densities are the high temperature limits of discrete versions of the G$β$E, computed by Allez-Bouchaud-Guionnet in [Phys. Rev. Lett. 109 (2012), 094102; arXiv:1205.3598], and L$β$E, computed by Allez-Bouchaud-Majumdar-Vivo in [J. Phys. A, vol. 46, no. 1 (2013), 015001; arXiv:1209.6171]. Moreover, we prove the following crystallization phenomenon of the particles in the high temperature limit: the limiting measures are uniformly supported on disjoint intervals with unit gaps and their locations correspond to the zeros of explicit special functions with all roots located in the real line. We also show that these zeros correspond to the spectra of certain unbounded Jacobi operators."
2510.23495,COOPERA: Continual Open-Ended Human-Robot Assistance,['cs.RO'],"['Chenyang Ma', 'Kai Lu', 'Ruta Desai', 'Xavier Puig', 'Andrew Markham', 'Niki Trigoni']","To understand and collaborate with humans, robots must account for individual human traits, habits, and activities over time. However, most robotic assistants lack these abilities, as they primarily focus on predefined tasks in structured environments and lack a human model to learn from. This work introduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot Assistance, where simulated humans, driven by psychological traits and long-term intentions, interact with robots in complex environments. By integrating continuous human feedback, our framework, for the first time, enables the study of long-term, open-ended human-robot collaboration (HRC) in different collaborative tasks across various time-scales. Within COOPERA, we introduce a benchmark and an approach to personalize the robot's collaborative actions by learning human traits and context-dependent intents. Experiments validate the extent to which our simulated humans reflect realistic human behaviors and demonstrate the value of inferring and personalizing to human intents for open-ended and long-term HRC. Project Page: https://dannymcy.github.io/coopera/"
2510.23494,Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap,"['cs.CV', 'cs.GR']","['Elisabeth Jüttner', 'Leona Krath', 'Stefan Korfhage', 'Hannah Dröge', 'Matthias B. Hullin', 'Markus Plack']","Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting."
2510.23493,dynsight: an Open Python Platform for Simulation and Experimental Trajectory Data Analysis,['cond-mat.mtrl-sci'],"['Simone Martino', 'Matteo Becchi', 'Andrew Tarzia', 'Daniele Rapetti', 'Giovanni M. Pavan']","The study of complex many-body systems through the analysis of the trajectories of dynamically moving and interacting units is a non-trivial task. The workflow for extracting meaningful information from raw trajectory data typically involves several interconnected steps: (i) identifying and tracking objects, and resolving their trajectories (for example, in experimental systems where these are not automatically available as in molecular simulations); (ii) translating the trajectories into data that are easier to handle and analyze using suitable descriptors; and (iii) extracting meaningful information from these data. Each of these tasks often requires substantial programming skills, the use of different types of representations or methods, and the development of interfaces between them. Despite the progress made by new tools targeting individual steps, integrating them under a common framework would lower the barrier to use (especially for diverse communities of users), reduce fragmentation, and ultimately facilitate the development of new approaches in trajectory data analysis. To this end, we introduce dynsight, an open Python platform that streamlines the extraction and analysis of time-series data from simulation or experimentally resolved trajectories. Dynsight simplifies workflows, enhances accessibility, and supports the analysis of time-series and trajectory data to unravel the dynamic complexity of systems across different scales. Dynsight is open source (available at github.com/GMPavanLab/dynsight) and can be easily installed using pip."
2510.23492,"Learning the PTM Code through a Coarse-to-Fine, Mechanism-Aware Framework",['cs.CE'],"['Jingjie Zhang', 'Hanqun Cao', 'Zijun Gao', 'Yu Wang', 'Shaoning Li', 'Jun Xu', 'Cheng Tan', 'Jun Zhu', 'Chang-Yu Hsieh', 'Chunbin Gu', 'Pheng Ann Heng']","Post-translational modifications (PTMs) form a combinatorial ""code"" that regulates protein function, yet deciphering this code - linking modified sites to their catalytic enzymes - remains a central unsolved problem in understanding cellular signaling and disease. We introduce COMPASS-PTM, a mechanism-aware, coarse-to-fine learning framework that unifies residue-level PTM profiling with enzyme-substrate assignment. COMPASS-PTM integrates evolutionary representations from protein language models with physicochemical priors and a crosstalk-aware prompting mechanism that explicitly models inter-PTM dependencies. This design allows the model to learn biologically coherent patterns of cooperative and antagonistic modifications while addressing the dual long-tail distribution of PTM data. Across multiple proteome-scale benchmarks, COMPASS-PTM establishes new state-of-the-art performance, including a 122% relative F1 improvement in multi-label site prediction and a 54% gain in zero-shot enzyme assignment. Beyond accuracy, the model demonstrates interpretable generalization, recovering canonical kinase motifs and predicting disease-associated PTM rewiring caused by missense variants. By bridging statistical learning with biochemical mechanism, COMPASS-PTM unifies site-level and enzyme-level prediction into a single framework that learns the grammar underlying protein regulation and signaling."
2510.23491,An Error-Based Safety Buffer for Safe Adaptive Control (Extended Version),"['eess.SY', 'math.OC']","['Peter A. Fisher', 'Johannes Autenrieb', 'Anuradha M. Annaswamy']","We consider the problem of adaptive control of a class of feedback linearizable plants with matched parametric uncertainties whose states are accessible, subject to state constraints, which often arise due to safety considerations. In this paper, we combine adaptation and control barrier functions into a real-time control architecture that guarantees stability, ensures control performance, and remains safe even with the parametric uncertainties. Two problems are considered, differing in the nature of the parametric uncertainties. In both cases, the control barrier function is assumed to have an arbitrary relative degree. In addition to guaranteeing stability, it is proved that both the control objective and safety objective are met with near-zero conservatism. No excitation conditions are imposed on the command signal. Simulation results demonstrate the non-conservatism of all of the theoretical developments."
2510.23490,On the entailment problem for DL-Lite$_{core}$ ontologies and conjunctive queries with negation,['cs.LO'],"['Jerzy Marcinkowski', 'Piotr Ostropolski-Nalewaja']","We show that the entailment problem, for a given entailment problem for DL-Lite$_{core}$ ontology, and given conjunctive query with inequalities, is undecidable.
  We also show that this problem remains undecidable if conjunctive queries with safe negation are considered instead of conjunctive queries with inequalities."
2510.23489,Quantum Phase Classification of Rydberg Atom Systems Using Resource-Efficient Variational Quantum Circuits and Classical Shadows,"['quant-ph', 'cs.LG']","['Hemish Ahuja', 'Samradh Bhardwaj', 'Kirti Dhir', 'Roman Bagdasarian', 'Ziwoong Jang']","Quantum phase transitions in Rydberg atom arrays present significant opportunities for studying many-body physics, yet distinguishing between different ordered phases without explicit order parameters remains challenging. We present a resource-efficient quantum machine learning approach combining classical shadow tomography with variational quantum circuits (VQCs) for binary phase classification of Z2 and Z3 ordered phases. Our pipeline processes 500 randomized measurements per 51-atom chain state, reconstructs shadow operators, performs PCA dimensionality reduction (514 features), and encodes features using angle embedding onto a 2-qubit parameterized circuit. The circuit employs RY-RZ angle encoding, strong entanglement via all-to-all CZ gates, and a minimal 2-parameter ansatz achieving depth 7. Training via simultaneous perturbation stochastic approximation (SPSA) with hinge loss converged in 120 iterations. The model achieved 100% test accuracy with perfect precision, recall, and F1 scores, demonstrating that minimal quantum resources suffice for high-accuracy phase classification. This work establishes pathways for quantum-enhanced condensed matter physics on near-term quantum devices."
2510.23488,Opportunities at FCC-ee for quark & lepton flavour physics,"['hep-ph', 'hep-ex']",['Luiz Vale Silva'],"The FCC-ee phase of a Future Circular Collider is generating great interest due to its versatility, allowing the study of various electroweak thresholds, $Z$, $WW$, $ZH$, and $t \bar{t}$. Electroweak precision physics is complemented by flavour physics measurements based on the unprecedented statistics attainable at the $Z$ pole, and benefiting from the low-background experimental environment (similar to Belle II), and from the production of the full spectrum of hadron species together with large boosts (similar to LHCb). A wide range of measurements is possible, spanning a rich variety of physics cases in both quark and lepton flavour physics sectors. Other electroweak thresholds can also be considered in this endeavour. A commensurate effort from the theory community will be needed to interpret future measurements. I present an overview of the broad potential of the FCC-ee flavour physics program."
2510.23487,Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy,"['cs.AI', 'cs.FL']","['Roham Koohestani', 'Ziyou Li', 'Anton Podkopaev', 'Maliheh Izadi']","This paper establishes a formal equivalence between the architectural classes of modern agentic AI systems and the abstract machines of the Chomsky hierarchy. We posit that the memory architecture of an AI agent is the definitive feature determining its computational power and that it directly maps it to a corresponding class of automaton. Specifically, we demonstrate that simple reflex agents are equivalent to Finite Automata, hierarchical task-decomposition agents are equivalent to Pushdown Automata, and agents employing readable/writable memory for reflection are equivalent to TMs. This Automata-Agent Framework provides a principled methodology for right-sizing agent architectures to optimize computational efficiency and cost. More critically, it creates a direct pathway to formal verification, enables the application of mature techniques from automata theory to guarantee agent safety and predictability. By classifying agents, we can formally delineate the boundary between verifiable systems and those whose behavior is fundamentally undecidable. We address the inherent probabilistic nature of LLM-based agents by extending the framework to probabilistic automata that allow quantitative risk analysis. The paper concludes by outlining an agenda for developing static analysis tools and grammars for agentic frameworks."
2510.23486,Learning to Reason Efficiently with Discounted Reinforcement Learning,['cs.LG'],"['Alex Ayoub', 'Kavosh Asadi', 'Dale Schuurmans', 'Csaba Szepesvári', 'Karim Bouyarmane']","Large reasoning models (LRMs) often consume excessive tokens, inflating computational cost and latency. We challenge the assumption that longer responses improve accuracy. By penalizing reasoning tokens using a discounted reinforcement learning setup (interpretable as a small token cost) and analyzing Blackwell optimality in restricted policy classes, we encourage concise yet accurate reasoning. Experiments confirm our theoretical results that this approach shortens chains of thought while preserving accuracy."
2510.23485,Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization,"['stat.ML', 'cs.IT', 'cs.LG']","['Milad Sefidgaran', 'Kimia Nadjahi', 'Abdellatif Zaidi']","In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical learning algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of $\mathcal{O}(1/\sqrt{n})$, where $n$ is the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data ""memorization"" raised in those works, and which asserts that there are learning problem instances for which any learning algorithm that has good prediction there exist distributions under which the algorithm must ""memorize"" a big fraction of the training dataset. We show that for every learning algorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization."
2510.23484,T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning,"['cs.LG', 'cs.CG', 'cs.CV']","['Julie Mordacq', 'David Loiseaux', 'Vicky Kalogeiton', 'Steve Oudot']","Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without labeled data, often by enforcing invariance to input transformations such as rotations or blurring. Recent studies have highlighted two pivotal properties for effective representations: (i) avoiding dimensional collapse-where the learned features occupy only a low-dimensional subspace, and (ii) enhancing uniformity of the induced distribution. In this work, we introduce T-REGS, a simple regularization framework for SSL based on the length of the Minimum Spanning Tree (MST) over the learned representation. We provide theoretical analysis demonstrating that T-REGS simultaneously mitigates dimensional collapse and promotes distribution uniformity on arbitrary compact Riemannian manifolds. Several experiments on synthetic data and on classical SSL benchmarks validate the effectiveness of our approach at enhancing representation quality."
2510.23483,Towards a Functionally Complete and Parameterizable TFHE Processor,['cs.CR'],"['Valentin Reyes Häusler', 'Gabriel Ott', 'Aruna Jayasena', 'Andreas Peter']","Fully homomorphic encryption allows the evaluation of arbitrary functions on encrypted data. It can be leveraged to secure outsourced and multiparty computation. TFHE is a fast torus-based fully homomorphic encryption scheme that allows both linear operations, as well as the evaluation of arbitrary non-linear functions. It currently provides the fastest bootstrapping operation performance of any other FHE scheme. Despite its fast performance, TFHE suffers from a considerably higher computational overhead for the evaluation of homomorphic circuits. Computations in the encrypted domain are orders of magnitude slower than their unencrypted equivalents. This bottleneck hinders the widespread adoption of (T)FHE for the protection of sensitive data. While state-of-the-art implementations focused on accelerating and outsourcing single operations, their scalability and practicality are constrained by high memory bandwidth costs. In order to overcome this, we propose an FPGA-based hardware accelerator for the evaluation of homomorphic circuits. Specifically, we design a functionally complete TFHE processor for FPGA hardware capable of processing instructions on the data completely on the FPGA. In order to achieve a higher throughput from our TFHE processor, we implement an improved programmable bootstrapping module which outperforms the current state-of-the-art by 240\% to 480\% more bootstrappings per second. Our efficient, compact, and scalable design lays the foundation for implementing complete FPGA-based TFHE processor architectures."
2510.23482,On the Faithfulness of Visual Thinking: Measurement and Enhancement,"['cs.CV', 'cs.AI']","['Zujing Liu', 'Junwen Pan', 'Qi She', 'Yuan Gao', 'Guisong Xia']","Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image."
2510.23481,Dynamical friction shear and rotation in Chaplygin cosmology,['astro-ph.CO'],"['A. Del Popolo', 'Saeed Fakhry', 'Maryam Shiravand', 'Morgan Le Delliou']","In this study, we build upon the findings of Del Popolo et al. (2013) by further analyzing the influence of dynamical friction on the evolution of cosmological perturbations within the framework of the spherical collapse model (SCM) in a Universe dominated by generalized Chaplygin gas (GCG). Specifically, we investigate how dynamical friction alters the growth rate of density perturbations, the effective sound speed, the equation-of-state parameter www, and the evolution of the cosmic expansion rate. Our results demonstrate that dynamical friction significantly delays the collapse process compared to the standard SCM. Accurate computation of these parameters is crucial for obtaining consistent results and reliable physical interpretations when employing the GCG model. Furthermore, our analysis confirms that the suppression of perturbation growth due to dynamical friction is considerably more pronounced than that caused by shear and rotation, as previously indicated by Del Popolo et al. (2013). This enhanced suppression effectively addresses the instability issues, such as oscillations or exponential divergences in the dark-matter power spectrum, highlighted in linear perturbation studies, such as those by Sandvik et al. (2004)."
2510.23480,Bound entanglement in symmetric random induced states,['quant-ph'],"['Jonathan Louvet', 'François Damanet', 'Thierry Bastin']","Bound entanglement, a weak -- yet resourceful -- form of quantum entanglement, remains notoriously hard to detect and construct. We address this in this paper by leveraging symmetric random induced states, where positive partial transpose (PPT) bound entanglement arises naturally under partial tracing when proper parameters are selected. We investigate the probability of finding PPT bound entanglement in symmetric random induced states constructed via two methods: partial tracing of symmetric multiqubit pure states on the one hand (MI) and tracing out a qudit ancilla on the other hand (MII). For $N > 3$ qubits, we demonstrate that bound entanglement naturally emerges under optimal parameters, with a probability of occurrence very close to 1. We show that the two methods produce different varieties of PPT bound entangled states, and identify the contexts in which each method offers distinct advantages. These methods provide a versatile toolkit for the generation of large families of random PPT bound entangled states without complex numerical optimization."
2510.23479,MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding,['cs.CV'],"['Xin Jin', 'Siyuan Li', 'Siyong Jian', 'Kai Yu', 'Huan Wang']","Vision-language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs."
2510.23478,"UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception",['cs.CV'],"['Karthikeyan Chandra Sekaran', 'Markus Geisler', 'Dominik Rößle', 'Adithya Mohan', 'Daniel Cremers', 'Wolfgang Utschick', 'Michael Botsch', 'Werner Huber', 'Torsten Schön']","Recent cooperative perception datasets have played a crucial role in advancing smart mobility applications by enabling information exchange between intelligent agents, helping to overcome challenges such as occlusions and improving overall scene understanding. While some existing real-world datasets incorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions, they are typically limited to a single intersection or a single vehicle. A comprehensive perception dataset featuring multiple connected vehicles and infrastructure sensors across several intersections remains unavailable, limiting the benchmarking of algorithms in diverse traffic environments. Consequently, overfitting can occur, and models may demonstrate misleadingly high performance due to similar intersection layouts and traffic participant behavior. To address this gap, we introduce UrbanIng-V2X, the first large-scale, multi-modal dataset supporting cooperative perception involving vehicles and infrastructure sensors deployed across three urban intersections in Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and spatially calibrated sensor sequences, each lasting 20 seconds. All sequences contain recordings from one of three intersections, involving two vehicles and up to three infrastructure-mounted sensor poles operating in coordinated scenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB cameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12 infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with 3D bounding boxes spanning 13 object classes, resulting in approximately 712k annotated instances across the dataset. We provide comprehensive evaluations using state-of-the-art cooperative perception methods and publicly release the codebase, dataset, HD map, and a digital twin of the complete data collection environment."
2510.23477,MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring,['cs.CL'],"['Tengchao Yang', 'Sichen Guo', 'Mengzhao Jia', 'Jiaming Su', 'Yuanyang Liu', 'Zhihan Zhang', 'Meng Jiang']","Effective math tutoring requires not only solving problems but also diagnosing students' difficulties and guiding them step by step. While multimodal large language models (MLLMs) show promise, existing benchmarks largely overlook these tutoring skills. We introduce MMTutorBench, the first benchmark for AI math tutoring, consisting of 685 problems built around pedagogically significant key-steps. Each problem is paired with problem-specific rubrics that enable fine-grained evaluation across six dimensions, and structured into three tasks-Insight Discovery, Operation Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find clear performance gaps between proprietary and open-source systems, substantial room compared to human tutors, and consistent trends across input variants: OCR pipelines degrade tutoring quality, few-shot prompting yields limited gains, and our rubric-based LLM-as-a-Judge proves highly reliable. These results highlight both the difficulty and diagnostic value of MMTutorBench for advancing AI tutoring."
2510.23476,Human-AI Collaborative Uncertainty Quantification,"['cs.AI', 'cs.HC', 'stat.ML']","['Sima Noorani', 'Shayan Kiyani', 'George Pappas', 'Hamed Hassani']","AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and AI. This work advances this vision by identifying the fundamental principles of Human AI collaboration within uncertainty quantification, a key component of reliable decision making. We introduce Human AI Collaborative Uncertainty Quantification, a framework that formalizes how an AI model can refine a human expert's proposed prediction set with two goals: avoiding counterfactual harm, ensuring the AI does not degrade correct human judgments, and complementarity, enabling recovery of correct outcomes the human missed. At the population level, we show that the optimal collaborative prediction set follows an intuitive two threshold structure over a single score function, extending a classical result in conformal prediction. Building on this insight, we develop practical offline and online calibration algorithms with provable distribution free finite sample guarantees. The online method adapts to distribution shifts, including human behavior evolving through interaction with AI, a phenomenon we call Human to AI Adaptation. Experiments across image classification, regression, and text based medical decision making show that collaborative prediction sets consistently outperform either agent alone, achieving higher coverage and smaller set sizes across various conditions."
2510.23475,Shareholder Democracy with AI Representatives,['cs.HC'],"['Suyash Fulay', 'Sercan Demir', 'Galen Hines-Pierce', 'Hélène Landemore', 'Michiel Bakker']","A large share of retail investors hold public equities through mutual funds, yet lack adequate control over these investments. Indeed, mutual funds concentrate voting power in the hands of a few asset managers. These managers vote on behalf of shareholders despite having limited insight into their individual preferences, leaving them exposed to growing political and regulatory pressures, particularly amid rising shareholder activism. Pass-through voting has been proposed as a way to empower retail investors and provide asset managers with clearer guidance, but it faces challenges such as low participation rates and the difficulty of capturing highly individualized shareholder preferences for each specific vote. Randomly selected assemblies of shareholders, or ``investor assemblies,'' have also been proposed as more representative proxies than asset managers. As a third alternative, we propose artificial intelligence (AI) enabled representatives trained on individual shareholder preferences to act as proxies and vote on their behalf. Over time, these models could not only predict how retail investors would vote at any given moment but also how they might vote if they had significantly more time, knowledge, and resources to evaluate each proposal, leading to better overall decision-making. We argue that shareholder democracy offers a compelling real-world test bed for AI-enabled representation, providing valuable insights into both the potential benefits and risks of this approach more generally."
2510.23474,"Policy-Aware Generative AI for Safe, Auditable Data Access Governance",['cs.AI'],"['Shames Al Mandalawi', 'Muzakkiruddin Ahmed Mohammed', 'Hendrika Maclean', 'Mert Can Cakmak', 'John R. Talburt']","Enterprises need access decisions that satisfy least privilege, comply with regulations, and remain auditable. We present a policy aware controller that uses a large language model (LLM) to interpret natural language requests against written policies and metadata, not raw data. The system, implemented with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context interpretation, user validation, data classification, business purpose test, compliance mapping, and risk synthesis) with early hard policy gates and deny by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls and a machine readable rationale. We evaluate on fourteen canonical cases across seven scenario families using a privacy preserving benchmark. Results show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny families dropping to 0, and Functional Appropriateness and Compliance Adherence at 14/14. Expert ratings of rationale quality are high, and median latency is under one minute. These findings indicate that policy constrained LLM reasoning, combined with explicit gates and audit trails, can translate human readable policies into safe, compliant, and traceable machine decisions."
2510.23473,"Video-Thinker: Sparking ""Thinking with Videos"" via Reinforcement Learning",['cs.CV'],"['Shijian Wang', 'Jiarui Jin', 'Xingjian Wang', 'Linxin Song', 'Runhao Fu', 'Hecheng Wang', 'Zongyuan Ge', 'Yuan Lu', 'Xuelian Cheng']","Recent advances in image reasoning methods, particularly ""Thinking with Images"", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic ""grounding"" and ""captioning"" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs."
2510.23472,BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement,"['cs.LG', 'cs.AI', 'cs.AR', 'cs.NE']","['Ke Xue', 'Ruo-Tong Chen', 'Rong-Xi Tan', 'Xi Lin', 'Yunqi Shi', 'Siyuan Xu', 'Mingxuan Yuan', 'Chao Qian']","Chip placement is a vital stage in modern chip design as it has a substantial impact on the subsequent processes and the overall quality of the final chip. The use of black-box optimization (BBO) for chip placement has a history of several decades. However, early efforts were limited by immature problem formulations and inefficient algorithm designs. Recent progress has shown the effectiveness and efficiency of BBO for chip placement, proving its potential to achieve state-of-the-art results. Despite these advancements, the field lacks a unified, BBO-specific benchmark for thoroughly assessing various problem formulations and BBO algorithms. To fill this gap, we propose BBOPlace-Bench, the first benchmark designed specifically for evaluating and developing BBO algorithms for chip placement tasks. It integrates three problem formulations of BBO for chip placement, and offers a modular, decoupled, and flexible framework that enables users to seamlessly implement, test, and compare their own algorithms. BBOPlace-Bench integrates a wide variety of existing BBO algorithms, including simulated annealing (SA), evolutionary algorithms (EAs), and Bayesian optimization (BO). Experimental results show that the problem formulations of mask-guided optimization and hyperparameter optimization exhibit superior performance than the sequence pair problem formulation, while EAs demonstrate better overall performance than SA and BO, especially in high-dimensional search spaces, and also achieve state-of-the-art performance compared to the mainstream chip placement methods. BBOPlace-Bench not only facilitates the development of efficient BBO-driven solutions for chip placement but also broadens the practical application scenarios (which are urgently needed) for the BBO community. The code of BBOPlace-Bench is available at https://github.com/lamda-bbo/BBOPlace-Bench."
2510.23471,Robust Decision Making with Partially Calibrated Forecasts,"['stat.ML', 'cs.AI', 'cs.LG']","['Shayan Kiyani', 'Hamed Hassani', 'George Pappas', 'Aaron Roth']","Calibration has emerged as a foundational goal in ``trustworthy machine learning'', in part because of its strong decision theoretic semantics. Independent of the underlying distribution, and independent of the decision maker's utility function, calibration promises that amongst all policies mapping predictions to actions, the uniformly best policy is the one that ``trusts the predictions'' and acts as if they were correct. But this is true only of \emph{fully calibrated} forecasts, which are tractable to guarantee only for very low dimensional prediction problems. For higher dimensional prediction problems (e.g. when outcomes are multiclass), weaker forms of calibration have been studied that lack these decision theoretic properties. In this paper we study how a conservative decision maker should map predictions endowed with these weaker (``partial'') calibration guarantees to actions, in a way that is robust in a minimax sense: i.e. to maximize their expected utility in the worst case over distributions consistent with the calibration guarantees. We characterize their minimax optimal decision rule via a duality argument, and show that surprisingly, ``trusting the predictions and acting accordingly'' is recovered in this minimax sense by \emph{decision calibration} (and any strictly stronger notion of calibration), a substantially weaker and more tractable condition than full calibration. For calibration guarantees that fall short of decision calibration, the minimax optimal decision rule is still efficiently computable, and we provide an empirical evaluation of a natural one that applies to any regression model solved to optimize squared error."
2510.23470,A targeted radio survey of infrared-selected bow shock candidates,"['astro-ph.GA', 'astro-ph.SR']","['M. Moutzouri', 'J. Mackey', 'N. Castro', 'Y. Gong', 'P. Jiménez-Hernández', 'J. A. Toalá', 'C. Burger-Scheidlin', 'M. Rugel', 'C. Carrasco-González', 'R. Brose', 'K. M. Menten']","Bow shocks around massive stars have primarily been detected in IR emission, but radio detections are becoming more frequent with the commissioning of sensitive and large field-of-view interferometers. Radio data probes both thermal and non-thermal emission, thereby constraining the relativistic electron population. We undertook a radio survey for bow shocks based on IR catalogues of candidates, using the VLA and the 100-m Effelsberg Telescope, aiming for new detections and to better characterise the multi-wavelength emission. We used Gaia DR3 to re-calculate spatial motion of the driving stars with respect to the surrounding stellar population. We studied the radio emission from bow shocks using emission maps and spectral-index measurements, and compared our results with data from catalogues and multi-wavelength emission. Of the 24 targets observed with the VLA in the 4-12 GHz band, six were clearly detected (including two previously reported) and 5 possibly detected. A subset of these were also observed and detected with Effelsberg at 4-8 GHz. The VLA-derived spectral index maps indicate non-thermal emission for most sources, but the statistical uncertainties are large for most sources and all Effelsberg observations indicate thermal emission. Assuming thermal emission, we obtain upper limits on the electron density within the shocked layer. We obtained upper limits on radio emission from the bow shock of Zeta Oph at a similar flux level to predictions from MHD simulations. Our survey marks a significant addition to the ca. 10 previously known radio-emitting bow shocks in the literature, and demonstrates that deep, targeted radio surveys can effectively detect IR-selected bow shocks. Follow-up observations of these targets at lower and higher frequencies are encouraged to determine whether any are non-thermal emitters like the bow shocks of BD+43, BD+60 and LS2355. (abridged)"
2510.23469,Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks,['cs.LG'],"['Yuhan Yang', 'Xingbo Fu', 'Jundong Li']","In recent years, pre-training Graph Neural Networks (GNNs) through self-supervised learning on unlabeled graph data has emerged as a widely adopted paradigm in graph learning. Although the paradigm is effective for pre-training powerful GNN models, the objective gap often exists between pre-training and downstream tasks. To bridge this gap, graph prompting adapts pre-trained GNN models to specific downstream tasks with extra learnable prompts while keeping the pre-trained GNN models frozen. As recent graph prompting methods largely focus on enhancing model utility on downstream tasks, they often overlook fairness concerns when designing prompts for adaptation. In fact, pre-trained GNN models will produce discriminative node representations across demographic subgroups, as downstream graph data inherently contains biases in both node attributes and graph structures. To address this issue, we propose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairness for adapting pre-trained GNN models to downstream tasks. To mitigate attribute bias, we design an Adaptive Feature Rectification module that learns customized attribute prompts to suppress sensitive information at the input layer, reducing bias at the source. Afterward, we propose an Adaptive Message Calibration module that generates structure prompts at each layer, which adjust the message from neighboring nodes to enable dynamic and soft calibration of the information flow. Finally, ADPrompt jointly optimizes the two prompting modules to adapt the pre-trained GNN while enhancing fairness. We conduct extensive experiments on four datasets with four pre-training strategies to evaluate the performance of ADPrompt. The results demonstrate that our proposed ADPrompt outperforms seven baseline methods on node classification tasks."
2510.23468,Model-independent mass determination of near-threshold states from short-range production,"['hep-ph', 'hep-ex', 'nucl-th']","['Yong-Hui Lin', 'Hans-Werner Hammer', 'Ulf-G. Meißner']","We propose a novel observable for the precision measurements of a wide class of near-threshold dimer states: the short-range production rate of a dimer--spectator two-body system, composed of the given near-threshold state and one of its constituents. Within the framework of nonrelativistic effective field theory, these production rates exhibit characteristic line shapes for the specific partial wave and reach a model-independent minimum. This feature enables a precise extraction of their masses from experimental data, provided that the line shape can be resolved with sufficient accuracy. Applying this novel method to both the $T_{b\bar{b}1}(10610)B$ and $T_{b\bar{b}1}(10650)B^*$ systems allows for a precise determination of the binding energy $δ$ of the $T_{b\bar{b}1}(10610)$ and $T_{b\bar{b}1}(10650)$ via the relation of $δ=-{E_{\text{dip}}^{\text{exp}}}/{0.1983}$ once the respective dip position $E_{\text{dip}}^{\text{exp}}$ is experimentally identified."
2510.23467,Joint Uplink and Downlink Resource Allocation and Antenna Activation for Pinching Antenna Systems,['eess.SP'],"['Shreya Khisa', 'Ali Amhaz', 'Mohamed Elhattab', 'Chadi Assi', 'Sanaa Sharafeddine']","In this paper, we explore a novel joint uplink and downlink framework utilizing a pinching antenna system (PASS). We consider two waveguides, one dedicated to transmission and one to reception, and both of them are connected to a base station (BS). Each type of waveguide consists of several pinching antennas (PAs) in some preconfigured positions. In this framework, we assume the BS can serve downlink and uplink user equipments (UEs) at the same time using the same spectrum resources through the presented PASS. In this aspect, we formulate a sum rate optimization problem that jointly optimizes the antenna activation factor, the BS transmit power, and the UE's transmit power, subject to power budget constraints for the BS and the UEs, as well as minimum rate requirements for the UEs. The formulated problem is highly non-convex and difficult to solve directly. Hence, we divide the main problem into two sub-problems: the antenna activation sub-problem and the power allocation sub-problem. Then, we solve the antenna activation problem utilizing a distance and spatial correlation-based algorithm. Meanwhile, the resource allocation problem is solved using a successive convex approximation (SCA)-based algorithm. Numerical results show that our proposed framework can achieve around 60-90\% performance gains over its time division duplex (TDD) where the uplink and downlink transmissions are served in different orthogonal time slots."
2510.23466,Thermoelectric transport and the role of different scattering processes in the half-Heusler NbFeSb,"['cond-mat.mtrl-sci', 'doi', '10.1039/D5MH00228A']","['Bhawna Sahni', 'Yao Zhao', 'Zhen Li', 'Rajeev Dutt', 'Patrizio Graziosi', 'Neophytos Neophytou']","We perform an ab initio computational investigation of the electronic and thermoelectric transport properties of one of the best performance half-Heusler (HH) alloys, NbFeSb. We use Boltzmann Transport equation while taking into account the full energy/momentum/band dependence of all relevant electronic scattering rates, i.e. with acoustic phonons, non-polar optical phonons (intra- and inter-valley), polar optical phonons (POP), and ionized impurity scattering (IIS). We use a highly efficient and accurate computational approach, where the scattering rates are derived using only a few ab initio extracted matrix elements, while we account fully for intra-/inter valley/band transitions, screening from both electrons and holes, and bipolar transport effects. Our computed thermoelectric power-factor (PF) values show good agreement with experiments across densities and temperatures, while they indicate the upper limit of PF performance for this material. We show that the polar optical phonon and ionized impurity scattering (importantly including screening), influence significantly the transport properties, whereas the computationally expensive non-polar phonon scattering part (acoustic and non-polar optical) is somewhat weaker, especially for electrons, and at lower to intermediate temperatures. This insight is relevant in the study of half-Heusler and other polar thermoelectric materials in general. Although we use NbFeSb as an example, the method we employ is material agnostic and can be broadly applied efficiently for electronic and thermoelectric materials in general, with more than 10x reduction in computational cost compared to fully ab initio methods, while retaining ab-initio accuracy."
2510.23465,Trajectory-Aware Air-to-Ground Channel Characterization for Low-Altitude UAVs Using MaMIMO Measurements,['cs.NI'],"['Abdul Saboor', 'Zhuangzhuang Cui', 'Achiel Colpaert', 'Evgenii Vinogradov', 'Wout Joseph', 'Sofie Pollin']","This paper presents a comprehensive measurement-based trajectory-aware characterization of low-altitude Air-to-Ground (A2G) channels in a suburban environment. A 64-element Massive Multi-Input Multi-Output (MaMIMO) array was used to capture channels for three trajectories of an Uncrewed Aerial Vehicle (UAV), including two horizontal zig-zag flights at fixed altitudes and one vertical ascent, chosen to emulate AUE operations and to induce controlled azimuth and elevation sweeps for analyzing geometry-dependent propagation dynamics. We examine large-scale power variations and their correlation with geometric features, such as elevation, azimuth, and 3D distance, followed by an analysis of fading behavior through distribution fitting and Rician K-factor estimation. Furthermore, temporal non-stationarity is quantified using the Correlation Matrix Distance (CMD), and angular stationarity spans are utilized to demonstrate how channel characteristics change with the movement of the UAV. We also analyze Spectral Efficiency (SE) in relation to K-factor and Root Mean Square (RMS) delay spread, highlighting their combined influence on link performance. The results show that the elevation angle is the strongest predictor of the received power, with a correlation of more than 0.77 for each trajectory, while the Nakagami model best fits the small-scale fading. The K-factor increases from approximately 5 dB at low altitudes to over 15 dB at higher elevations, indicating stronger LoS dominance. Non-stationarity patterns are highly trajectory- and geometry-dependent, with azimuth most affected in horizontal flights and elevation during vertical flight. These findings offer valuable insights for modeling and improving UAV communication channels in 6G Non-Terrestrial Networks (NTNs)."
2510.23464,Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts,"['cs.CL', 'cs.AI']","['Nikesh Gyawali', 'Doina Caragea', 'Alex Vasenkov', 'Cornelia Caragea']","Financial narratives from U.S. Securities and Exchange Commission (SEC) filing reports and quarterly earnings call transcripts (ECTs) are very important for investors, auditors, and regulators. However, their length, financial jargon, and nuanced language make fine-grained analysis difficult. Prior sentiment analysis in the financial domain required a large, expensive labeled dataset, making the sentence-level stance towards specific financial targets challenging. In this work, we introduce a sentence-level corpus for stance detection focused on three core financial metrics: debt, earnings per share (EPS), and sales. The sentences were extracted from Form 10-K annual reports and ECTs, and labeled for stance (positive, negative, neutral) using the advanced ChatGPT-o3-pro model under rigorous human validation. Using this corpus, we conduct a systematic evaluation of modern large language models (LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting strategies. Our results show that few-shot with CoT prompting performs best compared to supervised baselines, and LLMs' performance varies across the SEC and ECT datasets. Our findings highlight the practical viability of leveraging LLMs for target-specific stance in the financial domain without requiring extensive labeled data."
2510.23463,Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station,"['cs.LG', 'cs.CR', 'stat.ML']","['Hao Liang', 'Haifeng Wen', 'Kaishun Wu', 'Hong Xing']","Federated Learning (FL) is a distributed learning paradigm that preserves privacy by eliminating the need to exchange raw data during training. In its prototypical edge instantiation with underlying wireless transmissions enabled by analog over-the-air computing (AirComp), referred to as \emph{over-the-air FL (AirFL)}, the inherent channel noise plays a unique role of \emph{frenemy} in the sense that it degrades training due to noisy global aggregation while providing a natural source of randomness for privacy-preserving mechanisms, formally quantified by \emph{differential privacy (DP)}. It remains, nevertheless, challenging to effectively harness such channel impairments, as prior arts, under assumptions of either simple channel models or restricted types of loss functions, mostly considering (local) DP enhancement with a single-round or non-convergent bound on privacy loss. In this paper, we study AirFL over multiple-access fading channels with a multi-antenna base station (BS) subject to user-level DP requirements. Despite a recent study, which claimed in similar settings that artificial noise (AN) must be injected to ensure DP in general, we demonstrate, on the contrary, that DP can be gained as a \emph{perk} even \emph{without} employing any AN. Specifically, we derive a novel bound on DP that converges under general bounded-domain assumptions on model parameters, along with a convergence bound with general smooth and non-convex loss functions. Next, we optimize over receive beamforming and power allocations to characterize the optimal convergence-privacy trade-offs, which also reveal explicit conditions in which DP is achievable without compromising training. Finally, our theoretical findings are validated by extensive numerical results."
2510.23462,SQOUT: A Risk-Based Threat Analysis Framework for Quantum Communication Systems,"['quant-ph', 'cs.CR']","['Michal Krelina', 'Tom Sorger', 'Bob Dirks']","This paper addresses the urgent need for a cybersecurity framework tailored to quantum communication systems as the world transitions to quantum-safe infrastructures. While quantum communication promises unbreakable security, real-world deployments are vulnerable to physical, protocol, and operational risks. Our work presents a structured framework for analysing these threats, combining a TTP-style (Tactic, Technique, Procedure) approach with a specific risk assessment methodology. We introduce SQOUT, a quantum threat intelligence platform, and illustrate its application using a Photon-Number-Splitting (PNS) attack kill chain. Furthermore, we apply established international standards and best practices for information security risk management to assess quantum-specific risk scenarios, providing practical guidance for safeguarding emerging quantum infrastructures."
2510.23461,Adaptive Multilevel Splitting: First Application to Rare-Event Derivative Pricing,"['q-fin.CP', 'math.NA']",['Riccardo Gozzo'],"This work analyzes the computational burden of pricing binary options in rare-event settings and introduces an adaptation of the adaptive multilevel splitting (AMS) method for financial derivatives. Standard Monte Carlo is inefficient for deep out of the money binaries due to discontinuous payoffs and low exercise probabilities, requiring very large samples for accurate estimates. An AMS scheme is developed for binary options under Black-Scholes and Heston dynamics, reformulating the rare-event problem as a sequence of conditional events. Numerical experiments compare the method to Monte Carlo and to other techniques such as antithetic variables and multilevel Monte Carlo (MLMC) across four contracts: European digital calls and puts, and Asian digital calls and puts. Results show up to a 200-fold computational gain for deep out-of-the-money cases while preserving unbiasedness. No evidence is found of prior applications of AMS to financial derivatives. The approach improves pricing efficiency for rare-event contracts such as parametric insurance and catastrophe linked securities. An open-source Rcpp implementation is provided, supporting multiple discretizations and importance functions."
2510.23460,On Hyperbolic Sombor index of graphs,['math.CO'],"['Kinkar Chandra Das', 'Sultan Ahmad']","The Hyperbolic Sombor index $HSO(G)$ of a graph $G$ is defined as \begin{align*} HSO(G) = \sum_{v_iv_j \in E(G)} \frac{\sqrt{d_i^{2}+d_j^{2}}}{\min\{d_i,d_j\}}, \end{align*} where $d_i$ and $d_j$ denote the degrees of the vertices $v_i$ and $v_j$, respectively. This index was recently introduced by Barman et al. [Geometric approach to degree-based topological index: Hyperbolic Sombor index, MATCH Commun. Math. Comput. Chem. 95 (2026) 63-94], who explored some of its mathematical properties and applications. However, their work contains several inaccuracies that require correction. In this paper, we first identify and rectify the errors found in the earlier study. We then extend the investigation by establishing new mathematical results for the Hyperbolic Sombor index across various classes of graphs, including trees, unicyclic graphs, and bicyclic graphs. In addition, we derive some lower and upper bounds for $HSO(G)$ in terms of the number of edges, maximum degree and minimum degree, and we characterize the graphs that attain these bounds. Finally, we conclude the paper by outlining potential directions for future research in this emerging area."
2510.23459,A Finite Element framework for bulk-surface coupled PDEs to solve moving boundary problems in biophysics,['math.NA'],"['Alessandro Contri', 'André Massing', 'Padmini Rangamani']","We consider moving boundary problems for biophysics and introduce a new computational framework to handle the complexity of the bulk-surface PDEs. In our framework, interpretability is maintained by adapting the fast, generalizable and accurate structure preservation scheme in [Q. Cheng and J. Shen, \textit{Computer Methods in Applied Mechanics and Engineering}, 391 (2022)]. We show that mesh distortion is mitigated by adopting the pioneering work of [B. Duan and B. Li, \textit{SIAM J. Sci. Comput.}, 46 (2024)], which is tied to an Arbitrary Lagrangian Eulerian (ALE) framework. We test our algorithms accuracy on moving surfaces with boundary for the following PDEs: advection-diffusion-reaction equations, phase-field models of Cahn-Hilliard type, and Helfrich energy gradient flows. We performed convergence studies for all the schemes introduced to demonstrate accuracy. We use a staggered approach to achieve coupling and further verify the convergence of this coupling using numerical experiments. Finally, we demonstrate broad applicability of our work by simulating state-of-the-art tests of biophysical models that involve membrane deformation."
2510.23458,BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents,"['cs.CL', 'cs.AI']","['Litu Ou', 'Kuan Li', 'Huifeng Yin', 'Liwen Zhang', 'Zhongwang Zhang', 'Xixi Wu', 'Rui Ye', 'Zile Qiao', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou']","Confidence in LLMs is a useful indicator of model uncertainty and answer reliability. Existing work mainly focused on single-turn scenarios, while research on confidence in complex multi-turn interactions is limited. In this paper, we investigate whether LLM-based search agents have the ability to communicate their own confidence through verbalized confidence scores after long sequences of actions, a significantly more challenging task compared to outputting confidence in a single interaction. Experimenting on open-source agentic models, we first find that models exhibit much higher task accuracy at high confidence while having near-zero accuracy when confidence is low. Based on this observation, we propose Test-Time Scaling (TTS) methods that use confidence scores to determine answer quality, encourage the model to try again until reaching a satisfactory confidence level. Results show that our proposed methods significantly reduce token consumption while demonstrating competitive performance compared to baseline fixed budget TTS methods."
2510.23457,"Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era",['cs.CR'],"['Saleh Darzi', 'Mirza Masfiqur Rahman', 'Imtiaz Karim', 'Rouzbeh Behnia', 'Attila A Yavuz', 'Elisa Bertino']","The 5G protocol lacks a robust base station authentication mechanism during the initial bootstrapping phase, leaving it susceptible to threats such as fake base station attacks. Conventional solutions, including digital signatures based on Public Key Infrastructures (PKIs) and identity-based signatures, are inadequate against quantum-capable adversaries. While integrating NIST's Post-Quantum Cryptography (PQC) standards is a leading approach for quantum resistance, their suitability for 5G base station authentication remains unexplored. Moreover, current solutions are predominantly centralized and lack security features such as distributed authentication. This work presents, to our knowledge, the first comprehensive network-level performance characterization of integrating NIST-PQC standards and conventional digital signatures (including threshold and identity-based schemes) into 5G base station authentication. Our findings reveal significant feasibility concerns, with direct PQC adoption hindered by protocol constraints and large signature sizes. We also highlight the performance limitations of conventional methods due to the overhead of certificate chains. To mitigate these challenges, we propose BORG, a transitional authentication solution based on a Hierarchical Identity-Based Threshold Signature scheme with a Fail-Stop property. BORG offers post-mortem post-quantum forgery detection and distributed trust via threshold and compact signatures, well-suited for 5G's stringent requirements. Our performance analysis underscores an important warning on the infeasibility of direct PQC integration and positions BORG as an effective transitional solution toward future quantum-resilient 5G authentication."
2510.23456,Full Benjamin-Feir instability of capillary-gravity Stokes waves in finite depth,['math.AP'],"['Ting-Yang Hsiao', 'Alberto Maspero']","We study the two-dimensional gravity-capillary water waves equations for a fluid of finite depth $\mathtt{h}>0$ under the combined effects of gravity and surface tension $κ\geq 0$. We analyze the linear stability and instability of small-amplitude, $2π$-periodic Stokes wave solutions, under the effect of longitudinal long-wave perturbations. The corresponding linearized operator has periodic coefficients and a defective zero eigenvalue of multiplicity four. Using Bloch-Floquet theory, we investigate the associated family of periodic eigenvalue problems. For all surface tension values $κ\geq 0$ and depths $\mathtt{h} > 0$, we establish the complete splitting of the four eigenvalues near zero when both the wave amplitude and the Floquet parameter are small. Specifically, we rigorously prove that in the regions of unstable depth and capillarity identified formally by Djordjevic-Redekopp and Ablowitz-Segur in the 1970's, the spectrum of the linearized operator near the origin depicts a ""figure 8"" pattern."
2510.23455,SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning,['cs.LG'],"['Khoa Nguyen', 'Khang Tran', 'NhatHai Phan', 'Cristian Borcea', 'Rouming Jin', 'Issa Khalil']","This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel training algorithm to leverage the geographic information of mobile users in Federated Learning (FL). SGFusion maps the data collected by mobile devices onto geographical zones and trains one FL model per zone, which adapts well to the data and behaviors of users in that zone. SGFusion models the local data-based correlation among geographical zones as a hierarchical random graph (HRG) optimized by Markov Chain Monte Carlo sampling. At each training step, every zone fuses its local gradient with gradients derived from a small set of other zones sampled from the HRG. This approach enables knowledge fusion and sharing among geographical zones in a probabilistic and stochastic gradient fusion process with self-attention weights, such that ""more similar"" zones have ""higher probabilities"" of sharing gradients with ""larger attention weights."" SGFusion remarkably improves model utility without introducing undue computational cost. Extensive theoretical and empirical results using a heart-rate prediction dataset collected across 6 countries show that models trained with SGFusion converge with upper-bounded expected errors and significantly improve utility in all countries compared to existing approaches without notable cost in system scalability."
2510.23454,Individual Minima-Informed Multi-Objective Model Predictive Control for Fixed Point Stabilization,['math.OC'],"['Markus Herrmann-Wicklmayr', 'Kathrin Flaßkamp']","Multi-objective model predictive control (MOMPC) for fixed point stabilization requires an automated a priori decision-making mechanism to translate a high-level preference into a single solution to be implemented. To this aim, we introduce an approach called individual minima-informed decision-making. This class of methods can be implemented through two sequential optimizations, regardless of the number of objectives, thereby improving the real-time capability of MOMPC. These methods operate on Pareto fronts and leverage the individual minima (IM), which are characteristic Pareto-optimal points. By this, we aim to produce a robust translation of a high-level preference to a suitable point on the Pareto front. However, guaranteeing the closed-loop stability of the resulting MOMPC scheme remains an open challenge.
  This paper addresses this gap by developing a novel MOMPC framework that integrates IM-informed decision-making while formally guaranteeing asymptotic stability. Our contribution is twofold. First, we propose and systematically analyze six variants of IM-informed decision-making methods -- including two novel methods -- designed to achieve the above-mentioned translation. Second, we embed these methods into a quasi-infinite horizon MOMPC framework and provide a rigorous proof of closed-loop asymptotic stability. The proof holds for any of the presented decision-making methods and relies on a descent condition that is less restrictive than those in prior literature. The practical applicability and effectiveness of the proposed framework are demonstrated in a numerical case study."
2510.23453,What are the odds? Risk and uncertainty about AI existential risk,['cs.AI'],['Marco Grossi'],"This work is a commentary of the article \href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and Hawthorne. It is not just a commentary though, but a useful reminder of the philosophical limitations of \say{linear} models of risk. The article will focus on the model employed by the authors: first, I discuss some differences between standard Swiss Cheese models and this one. I then argue that in a situation of epistemic indifference the probability of P(D) is higher than what one might first suggest, given the structural relationships between layers. I then distinguish between risk and uncertainty, and argue that any estimation of P(D) is structurally affected by two kinds of uncertainty: option uncertainty and state-space uncertainty. Incorporating these dimensions of uncertainty into our qualitative discussion on AI existential risk can provide a better understanding of the likeliness of P(D)."
2510.23452,Convolution features of univalent meromorphic functions generated by Barnes-Mittag-Leffler function,['math.CV'],"['Tuğba Yavuz', 'Şahsene Altınkaya']","The Mittag-Leffler function plays an important role in Geometric Function Theory, particularly in the study of analytic and meromorphic function classes. Among its various generalizations, the Barnes-Mittag-Leffler function stands out due to its intricate structure and applications in diverse mathematical fields. In this paper, our main focus is to investigate the convolution properties of these functions and establish conditions that ensure specific geometric characteristics. Additionally, we explore membership relations for functions in these classes. The results obtained in this work are novel, and their significance is demonstrated through various illustrative consequences and corollaries, emphasizing their potential impact in function theory and its applications."
2510.23451,Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences,"['cs.CL', 'cs.AI', 'cs.CV']","['Zhuoran Jin', 'Hongbang Yuan', 'Kejian Zhu', 'Jiachun Li', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao']","Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks."
2510.23450,Sharp angle estimates for second order divergence operators,"['math.FA', 'math.AP']","['Hannes Meinlschmidt', 'Joachim Rehberg']","This article is about the (minimal) sector containing the numerical range of the principal part of a linear second-order elliptic differential operator defined by a form on closed subspaces V of the first-order Sobolev space $W^{1,2}(Ω)$ incorporating mixed boundary conditions. We collect a comprehensive array of results on the angle of sectoriality and the $H^\infty$-angle attached to realizations of the elliptic operator. We thereby consider the operator in several scales of Banach spaces: the Lebesgue space, the negative Sobolev space, and their interpolation scale. For the latter two types of spaces, we rely on recent results regarding the Kato square root property. We focus on minimal assumptions on geometry, and we consider both real and complex coefficients. Not all results presented are new, but we strive for a streamlined and comprehensive overall picture from several branches of operator theory, and we complement the existing results with several new ones, in particular aiming at explicit estimates built on readily accessible problem data. This concerns for example a new estimate on the angle of the sector containing the numerical range of a linear, continuous and coercive Hilbert space operator, but also an explicit estimate for the angle of sectoriality for the elliptic operator on $L^p(Ω)$ with complex coefficients without any assumptions on geometry and a general transfer principle for the Crouzeix-Delyon theorem from bounded operators to sectorial ones, keeping the explicit constant."
2510.23449,Schrodinger Neural Network and Uncertainty Quantification: Quantum Machine,['cs.LG'],['M. M. Hammad'],"We introduce the Schrodinger Neural Network (SNN), a principled architecture for conditional density estimation and uncertainty quantification inspired by quantum mechanics. The SNN maps each input to a normalized wave function on the output domain and computes predictive probabilities via the Born rule. The SNN departs from standard parametric likelihood heads by learning complex coefficients of a spectral expansion (e . g ., Chebyshev polynomials) whose squared modulus yields the conditional density $p(y|x)=\left| ψ_x(y)\right| {}^2$ with analytic normalization. This representation confers three practical advantages: positivity and exact normalization by construction, native multimodality through interference among basis modes without explicit mixture bookkeeping, and yields closed-form (or efficiently computable) functionals$-$such as moments and several calibration diagnostics$-$as quadratic forms in coefficient space. We develop the statistical and computational foundations of the SNN, including (i) training by exact maximum-likelihood with unit-sphere coefficient parameterization, (ii) physics-inspired quadratic regularizers (kinetic and potential energies) motivated by uncertainty relations between localization and spectral complexity, (iii) scalable low-rank and separable extensions for multivariate outputs, (iv) operator-based extensions that represent observables, constraints, and weak labels as self-adjoint matrices acting on the amplitude space, and (v) a comprehensive framework for evaluating multimodal predictions. The SNN provides a coherent, tractable framework to elevate probabilistic prediction from point estimates to physically inspired amplitude-based distributions."
2510.23448,An Information-Theoretic Analysis of Out-of-Distribution Generalization in Meta-Learning with Applications to Meta-RL,"['cs.LG', 'stat.ML']",['Xingtu Liu'],"In this work, we study out-of-distribution generalization in meta-learning from an information-theoretic perspective. We focus on two scenarios: (i) when the testing environment mismatches the training environment, and (ii) when the training environment is broader than the testing environment. The first corresponds to the standard distribution mismatch setting, while the second reflects a broad-to-narrow training scenario. We further formalize the generalization problem in meta-reinforcement learning and establish corresponding generalization bounds. Finally, we analyze the generalization performance of a gradient-based meta-reinforcement learning algorithm."
2510.23447,Model Proficiency in Centralized Multi-Agent Systems: A Performance Study,"['stat.AP', 'cs.MA']","['Anna Guerra', 'Francesco Guidi', 'Pau Closas', 'Davide Dardari', 'Petar M. Djuric']","Autonomous agents are increasingly deployed in dynamic environments where their ability to perform a given task depends on both individual and team-level proficiency. While proficiency self-assessment (PSA) has been studied for single agents, its extension to a team of agents remains underexplored. This letter addresses this gap by presenting a framework for team PSA in centralized settings. We investigate three metrics for centralized team PSA: the measurement prediction bound (MPB), the Kolmogorov-Smirnov (KS) statistic, and the Kullback-Leibler (KL) divergence. These metrics quantify the discrepancy between predicted and actual measurements. We use the KL divergence as a reference metric since it compares the true and predictive distributions, whereas the MPB and KS provide efficient indicators for in situ assessment. Simulation results in a target tracking scenario demonstrate that both MPB and KS metrics accurately capture model mismatches, align with the KL divergence reference, and enable real-time proficiency assessment."
2510.23446,Tree-Cotree-Based IETI-DP for Eddy Current Problems in Time-Domain,"['math.NA', 'cs.CE']","['Mario Mally', 'Rafael Vázquez', 'Sebastian Schöps']","For low-frequency electromagnetic problems, where wave-propagation effects can be neglected, eddy current formulations are commonly used as a simplification of the full Maxwell's equations. In this setup, time-domain simulations, needed to capture transient startup responses or nonlinear behavior, are often computationally expensive. We propose a novel tearing and interconnecting approach for eddy currents in time-domain and investigate its scalability."
2510.23445,Non-Markovian quantum Mpemba effect in strongly correlated quantum dots,['cond-mat.str-el'],['YuanDong Wang'],"Harnessing non-Markovian effects has emerged as a resource for quantum control, where a structured environment can act as a quantum memory. We investigate the quench dynamics from specific initial states to equilibrium steady states in strongly correlated quantum dot systems. The distance between quantum states is quantified using the Bures metric, which endows the space of reduced density matrices with a Riemannian geometric structure. Using the numerically exact hierarchical equations of motion (HEOM) method, we demonstrate a quantum Mpemba effect arising from non-Markovianity. This effect is characterized by a relaxation slowdown due to information backflow from the bath to the system, which induces a pronounced memory effect. We show that the emergence of the non-Markovian quantum Mpemba effect on the approach to a strongly correlated steady state is determined by the interplay between the initial-state-dependent non-Markovianity and the initial geometric distance between states. Our results underscore the critical role of memory effects in quantum quench dynamics and suggest new pathways for controlling anomalous relaxation in open quantum systems."
2510.23444,FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network,"['cs.CV', 'cs.AI']","['Fangtong Sun', 'Congyu Li', 'Ke Yang', 'Yuchen Pan', 'Hanwen Yu', 'Xichuan Zhang', 'Yiying Li']","Low-light vision remains a fundamental challenge in computer vision due to severe illumination degradation, which significantly affects the performance of downstream tasks such as detection and segmentation. While recent state-of-the-art methods have improved performance through invariant feature learning modules, they still fall short due to incomplete modeling of low-light conditions. Therefore, we revisit low-light image formation and extend the classical Lambertian model to better characterize low-light conditions. By shifting our analysis to the frequency domain, we theoretically prove that the frequency-domain channel ratio can be leveraged to extract illumination-invariant features via a structured filtering process. We then propose a novel and end-to-end trainable module named \textbf{F}requency-domain \textbf{R}adial \textbf{B}asis \textbf{Net}work (\textbf{FRBNet}), which integrates the frequency-domain channel ratio operation with a learnable frequency domain filter for the overall illumination-invariant feature enhancement. As a plug-and-play module, FRBNet can be integrated into existing networks for low-light downstream tasks without modifying loss functions. Extensive experiments across various downstream tasks demonstrate that FRBNet achieves superior performance, including +2.2 mAP for dark object detection and +2.9 mIoU for nighttime segmentation. Code is available at: https://github.com/Sing-Forevet/FRBNet."
2510.23443,A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration,"['cs.AI', 'cs.CL', 'cs.CR', 'cs.MA']","['Chiara Bonfanti', 'Alessandro Druetto', 'Cataldo Basile', 'Tharindu Ranasinghe', 'Marcos Zampieri']","The growing intersection of cybersecurity and law creates a complex information space where traditional legal research tools struggle to deal with nuanced connections between cases, statutes, and technical vulnerabilities. This knowledge divide hinders collaboration between legal experts and cybersecurity professionals. To address this important gap, this work provides a first step towards intelligent systems capable of navigating the increasingly intricate cyber-legal domain. We demonstrate promising initial results on multilingual tasks."
2510.23442,CURVETE: Curriculum Learning and Progressive Self-supervised Training for Medical Image Classification,['cs.CV'],"['Asmaa Abbas', 'Mohamed Gaber', 'Mohammed M. Abdelsamea']","Identifying high-quality and easily accessible annotated samples poses a notable challenge in medical image analysis. Transfer learning techniques, leveraging pre-training data, offer a flexible solution to this issue. However, the impact of fine-tuning diminishes when the dataset exhibits an irregular distribution between classes. This paper introduces a novel deep convolutional neural network, named Curriculum Learning and Progressive Self-supervised Training (CURVETE). CURVETE addresses challenges related to limited samples, enhances model generalisability, and improves overall classification performance. It achieves this by employing a curriculum learning strategy based on the granularity of sample decomposition during the training of generic unlabelled samples. Moreover, CURVETE address the challenge of irregular class distribution by incorporating a class decomposition approach in the downstream task. The proposed method undergoes evaluation on three distinct medical image datasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. We investigate the classification performance using a generic self-supervised sample decomposition approach with and without the curriculum learning component in training the pretext task. Experimental results demonstrate that the CURVETE model achieves superior performance on test sets with an accuracy of 96.60% on the brain tumour dataset, 75.60% on the digital knee x-ray dataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50. Furthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%, 80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSM datasets, respectively, outperforming other training strategies."
2510.23441,The complete classification of triply-transitive strongly regular graphs,['math.CO'],"['Weicong Li', 'Hanlin Zou']","This paper completes the classification of triply-transitive strongly regular graphs, a program recently initiated by Herman, Maleki, and Razafimahatratra. By proving that the collinearity graph of the polar space $\mathcal{Q}^{-}(5,q)$ and the affine polar graph $\mathrm{VO}^{\varepsilon}_{2m}(2)$ are triply-transitive, we resolve the final open cases in the classification. The result is a definitive list of all strongly regular graphs that exhibit this exceptional form of local symmetry, characterized by the equality $T_{0,ω}=T_ω=\widetilde{T}_ω$ of their Terwilliger algebras."
2510.23440,Randomized Space-Time Coded Stacked Intelligent Metasurfaces for Massive Multiuser Downlink Connectivity,['eess.SP'],"['Donatella Darsena', 'Ivan Iudice', 'Vincenzo Galdi', 'Francesco Verde']","Stacked intelligent metasurfaces (SIMs) represent a key enabler for next-generation wireless networks, offering beamforming gains while significantly reducing radio-frequency chain requirements. In conventional space-only SIM architectures, the rate of reconfigurability of the SIM is equal to the inverse of the channel coherence time. This paper investigates a novel beamforming strategy for massive downlink connectivity using a randomized space-time (ST) coded SIM. In addition to conventional space-only metasurface layers, the proposed design integrates a ST metasurface layer at the input stage of the SIM that introduces random time variations over each channel coherence time interval. These artificial time variations enable opportunistic user scheduling and exploitation of multiuser diversity under slow channel dynamics. To mitigate the prohibitive overhead associated with full channel state information at the transmitter (CSIT), we propose a partial-CSIT-based beamforming scheme that leverages randomized steering vectors and limited user-side feedback based on signal quality measurements. Numerical results demonstrate that the proposed ST-SIM architecture achieves satisfactory sum-rate performance while significantly reducing CSIT acquisition and feedback overhead, thereby enabling scalable downlink connectivity in dense networks."
2510.23439,Interrelation between precisions on integrated currents and on recurrence times in Markov jump processes,"['cond-mat.stat-mech', 'doi', '10.1103/27gn-7w5d']","['Alberto Garilli', 'Diego Frezzato']","For Markov jump processes on irreducible networks with finite number of sites, we derive a general and explicit expression of the squared coefficient of variation for the net number of transitions from one site to a connected site in a given time window of observation (i.e., an `integrated current' as dynamical output). Such expression, which in itself is particularly useful for numerical calculations, is then elaborated to obtain the interrelation with the precision on the intrinsic timing of the recurrences of the forward and backward transitions. In biochemical ambits, such as enzyme catalysis and molecular motors, the precision on the timing is quantified by the so-called randomness parameter and the above connection is established in the long time limit of monitoring and for an irreversible site-site transition; the present extension to finite time and reversibility adds a new dimension. Some kinetic and thermodynamic inequalities are also derived."
2510.23438,Coresets for Clustering Under Stochastic Noise,"['cs.LG', 'cs.CG', 'cs.DS', 'stat.ML']","['Lingxiao Huang', 'Zhize Li', 'Nisheeth K. Vishnoi', 'Runkai Yang', 'Haoyu Zhao']","We study the problem of constructing coresets for $(k, z)$-clustering when the input dataset is corrupted by stochastic noise drawn from a known distribution. In this setting, evaluating the quality of a coreset is inherently challenging, as the true underlying dataset is unobserved. To address this, we investigate coreset construction using surrogate error metrics that are tractable and provably related to the true clustering cost. We analyze a traditional metric from prior work and introduce a new error metric that more closely aligns with the true cost. Although our metric is defined independently of the noise distribution, it enables approximation guarantees that scale with the noise level. We design a coreset construction algorithm based on this metric and show that, under mild assumptions on the data and noise, enforcing an $\varepsilon$-bound under our metric yields smaller coresets and tighter guarantees on the true clustering cost than those obtained via classical metrics. In particular, we prove that the coreset size can improve by a factor of up to $\mathrm{poly}(k)$, where $n$ is the dataset size. Experiments on real-world datasets support our theoretical findings and demonstrate the practical advantages of our approach."
2510.23437,A Physics-Informed Variational Inference Framework for Identifying Attributions of Extreme Stress Events in Low-Grain Polycrystals,"['stat.AP', 'physics.comp-ph']","['Yinling Zhang', 'Samuel D. Dunham', 'Curt A. Bronkhorst', 'Nan Chen']","Polycrystalline metal failure often begins with stress concentration at grain boundaries. Identifying which microstructural features trigger these events is important but challenging because these extreme damage events are rare and the failure mechanisms involve multiple complex processes across scales. Most existing inference methods focus on average behavior rather than rare events, whereas standard sample-based methods are computationally expensive for high-dimensional complex systems. In this paper, we develop a new variational inference framework that integrates a recently developed computationally efficient physics-informed statistical model with extreme value statistics to significantly facilitate the identification of material failure attributions. First, we reformulate the objective to emphasize observed exceedances by incorporating extreme-value theory into the likelihood, thereby highlighting tail behavior. Second, we constrain inference via a physics-informed statistical model that characterizes microstructure-stress relationships, which uniquely provides physically consistent predictions for these rare events. Third, mixture models in a reduced latent space are developed to capture the non-Gaussian characteristics of microstructural features, allowing the identification of multiple underlying mechanisms. In both controlled and realistic experimental tests for the bicrystal configuration, the framework achieves reliable extreme-event prediction and reveals the microstructural features associated with material failure, providing physical insights for material design with uncertainty quantification."
2510.23436,Education Paradigm Shift To Maintain Human Competitive Advantage Over AI,"['cs.GL', 'cs.HC', 'doi', '10.2514/6.2024-4902']","['Stanislav Selitskiy', 'Chihiro Inoue']","Discussion about the replacement of intellectual human labour by ``thinking machines'' has been present in the public and expert discourse since the creation of Artificial Intelligence (AI) as an idea and terminology since the middle of the twentieth century. Until recently, it was more of a hypothetical concern. However, in recent years, with the rise of Generative AI, especially Large Language Models (LLM), and particularly with the widespread popularity of the ChatGPT model, that concern became practical. Many domains of human intellectual labour have to adapt to the new AI tools that give humans new functionality and opportunity, but also question the viability and necessity of some human work that used to be considered intellectual yet has now become an easily automatable commodity. Education, unexpectedly, has now become burdened by an especially crucial role of charting long-range strategies for discovering viable human skills that would guarantee their place in the world of the ubiquitous use of AI in the intellectual sphere. We highlight weaknesses of the current AI and, especially, of its LLM-based core, show that root causes of LLMs' weaknesses are unfixable by the current technologies, and propose directions in the constructivist paradigm for the changes in Education that ensure long-term advantages of humans over AI tools."
2510.23435,Improved UVIS Aperture Corrections derived from Focus Diverse PSF Maps,['astro-ph.IM'],"['K. Huynh', 'V. Bajaj', 'J. Mack', 'A. Calamida']","In crowded fields, small-aperture photometry can reduce contamination errors from neighboring sources compared to larger aperture photometry. However, the UVIS encircled energy (EE) varies with detector position and focus variations on orbital timescales for aperture radii less than 10 pixels ($\sim$0.4 arcseconds). Using a set of focus-diverse empirical PSFs by Anderson (2018), we compute 2D spatial maps of the aperture correction between 5-10 pixels and find a maximum change of $\sim$0.01 mag over all focus levels for a given detector position. The upper-left and lower-right corners of the UVIS detector are more focus-sensitive than the rest of the field of view, where the mean correction is systematically $\sim$0.01 mag higher in Amp A for bluer filters (F275W, F336W, F438W) and $\sim$0.01 mag higher in Amp D for redder filters (F606W, F814W) at all focus levels. We test the new aperture correction maps in globular clusters, and we find reduced scatter, better agreement between the two CCDs, and a small shift in the absolute photometry when compared to a single (constant) aperture correction per image. These improvements are specific to photometry with apertures $<$ 10 pixels in radius; results from larger apertures are not affected. Using published EE tables can introduce systematic uncertainties in absolute photometry due to its tendency to vary with detector position and focus level, with larger errors for smaller apertures. Users requiring photometric accuracy better than $\sim$1% for small apertures can use isolated stars in the individual FLT/FLC frames (or PSF cutouts at a similar detector position and focus level) to compute encircled energy corrections and accurately account for the amount of flux at radii larger than their photometric apertures."
2510.23434,Choosing What to Learn: Experimental Design when Combining Experimental with Observational Evidence,"['econ.EM', 'math.ST', 'stat.ME']","['Aristotelis Epanomeritakis', 'Davide Viviano']","Experiments deliver credible but often localized effects, tied to specific sites, populations, or mechanisms. When such estimates are insufficient to extrapolate effects for broader policy questions, such as external validity and general-equilibrium (GE) effects, researchers combine trials with external evidence from reduced-form or structural observational estimates, or prior experiments. We develop a unified framework for designing experiments in this setting: the researcher selects which parameters to identify experimentally from a feasible set (which treatment arms and/or individuals to include in the experiment), allocates sample size, and specifies how to weight experimental and observational estimators. Because observational inputs may be biased in ways unknown ex ante, we develop a minimax proportional regret objective that evaluates any candidate design relative to an oracle that knows the bias and jointly chooses the design and estimator. This yields a transparent bias-variance trade-off that requires no prespecified bias bound and depends only on information about the precision of the estimators and the estimand's sensitivity to the underlying parameters. We illustrate the framework by (i) designing small-scale cash transfer experiments aimed at estimating GE effects and (ii) optimizing site selection for microfinance interventions."
2510.23433,Associative ternary algebras and ternary Lie algebras at cube roots of unity,['math.RA'],"['Anti Maria Aader', 'Viktor Abramov', 'Olga Liivapuu']","We propose an approach to extending the concept of a Lie algebra to ternary structures based on $ω$-symmetry, where $ω$ is a primitive cube root of unity. We give a definition of a corresponding structure, called a ternary Lie algebra at cube roots of unity, or a ternary $ω$-Lie algebra. A method for constructing ternary associative algebras has been developed. For ternary algebras, the notions of the ternary $ω$-associator and the ternary $ω$-commutator are introduced. It is shown that if a ternary algebra possesses the property of associativity of the first or second kind, then the ternary $ω$-commutator on this algebra determines the structure of a ternary $ω$-Lie algebra. Ternary algebras of cubic matrices with associative ternary multiplication of the second kind are considered. The structure of the 8-dimensional ternary $ω$-Lie algebra of cubic matrices of the second order is studied, and all its subalgebras of dimensions 2 and 3 are determined."
2510.23432,Solving Biot poroelasticity by coupling OPM Flow with the two-point stress approximation finite volume method,['math.NA'],"['Wietse M. Boon', 'Sarah Gasda', 'Tor Harald Sandve', 'Svenn Tveit']","Finite volume methods are prevalent in reservoir simulation due to their mass conservation properties and their ability to handle complex grids. However, a simple and consistent finite volume method for elasticity was unavailable until the recently developed two-point stress approximation finite volume method (TPSA). In this work, we show how to couple TPSA to an established flow simulator, using OPM Flow as our primary example. Due to this choice of numerical methods, the coupling is naturally handled at the cell centers, without requiring interpolation operators. We propose a fixed stress coupling scheme and reuse algebraic multi-grid preconditioners, which are known to be effective for two-point flux finite volume methods. Numerical examples illustrate the flexibility of the approach and we showcase how the introduction of solid mechanics impacts the behavior of compartmentalized flow systems."
2510.23431,A Newton-Kantorovich Inverse Function Theorem in Quasi-Metric Spaces,['math.OC'],['Titus Pinta'],"The purpose of this work is to investigate root finding problems defined on (quasi-)metric spaces, and ranging in Euclidean spaces. The motivation for this line of inquiry stems from recent models in biology and phylogenetics, where problems of great practical significance are cast as optimization problems on (quasi-)metric spaces. We investigate a minimal algebraic setup that allows us to study a notion of differentiability suitable for Newton-type methods, called Newton differentiability. This notion of differentiability benefits from calculus rules and is sufficient to prove superlinear convergence of a Newton-type method. Finally, a Newton-Kantorovich-type theorem provides an inverse function result, applicable on (quasi-)metric spaces."
2510.23430,On the Martin boundary for discrete TASEP,"['math.PR', 'math-ph', 'math.CO', 'math.RT']","['Vadim Gorin', 'Sergei Korotkikh']","We study a problem with three equivalent formulations: describing Gibbs measures for five-vertex model in quadrant; classifying coherent systems on a p-deformation of the Gelfand-Tsetlin graph related to Grothendieck polynomials; finding the Martin boundary for discrete time TASEP with p-geometric jumps. We find a wide family of the Gibbs measures, parameterized by certain analytic functions. A subset of our measures have probabilistic interpretation as interacting particle systems with fixed particles speeds. In contrast to previous related boundary problems, we find that admissible speeds are not arbitrary, but must be larger than $\frac{p}{1-p}$. For this subset we further establish Law of Large Numbers and Central Limit Theorem, connecting the fluctuations to families of independent GUE eigenvalues. As a consequence, the measures from the subset are extreme points of the Martin boundary. It remains open whether our list of measures is exhaustive."
2510.23429,MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from 3D Scans,['cs.CV'],"['Ahmet Serdar Karadeniz', 'Dimitrios Mallis', 'Danila Rukhovich', 'Kseniya Cherenkova', 'Anis Kacem', 'Djamila Aouada']","Computer-Aided Design (CAD) plays a foundational role in modern manufacturing and product development, often requiring designers to modify or build upon existing models. Converting 3D scans into parametric CAD representations--a process known as CAD reverse engineering--remains a significant challenge due to the high precision and structural complexity of CAD models. Existing deep learning-based approaches typically fall into two categories: bottom-up, geometry-driven methods, which often fail to produce fully parametric outputs, and top-down strategies, which tend to overlook fine-grained geometric details. Moreover, current methods neglect an essential aspect of CAD modeling: sketch-level constraints. In this work, we introduce a novel approach to CAD reverse engineering inspired by how human designers manually perform the task. Our method leverages multi-plane cross-sections to extract 2D patterns and capture fine parametric details more effectively. It enables the reconstruction of detailed and editable CAD models, outperforming state-of-the-art methods and, for the first time, incorporating sketch constraints directly into the reconstruction process."
2510.23428,Improving Predictions of Molecular Properties with Graph Featurisation and Heterogeneous Ensemble Models,"['cs.LG', 'doi', '10.1021/acs.jcim.5c01844']","['Michael L. Parker', 'Samar Mahmoud', 'Bailey Montefiore', 'Mario Öeren', 'Himani Tandon', 'Charlotte Wharrick', 'Matthew D. Segall']","We explore a ""best-of-both"" approach to modelling molecular properties by combining learned molecular descriptors from a graph neural network (GNN) with general-purpose descriptors and a mixed ensemble of machine learning (ML) models. We introduce a MetaModel framework to aggregate predictions from a diverse set of leading ML models. We present a featurisation scheme for combining task-specific GNN-derived features with conventional molecular descriptors.
  We demonstrate that our framework outperforms the cutting-edge ChemProp model on all regression datasets tested and 6 of 9 classification datasets. We further show that including the GNN features derived from ChemProp boosts the ensemble model's performance on several datasets where it otherwise would have underperformed. We conclude that to achieve optimal performance across a wide set of problems, it is vital to combine general-purpose descriptors with task-specific learned features and use a diverse set of ML models to make the predictions."
2510.23427,PrivacyGuard: A Modular Framework for Privacy Auditing in Machine Learning,['cs.LG'],"['Luca Melis', 'Matthew Grange', 'Iden Kalemaj', 'Karan Chadha', 'Shengyuan Hu', 'Elena Kashtelyan', 'Will Bullock']","The increasing deployment of Machine Learning (ML) models in sensitive domains motivates the need for robust, practical privacy assessment tools. PrivacyGuard is a comprehensive tool for empirical differential privacy (DP) analysis, designed to evaluate privacy risks in ML models through state-of-the-art inference attacks and advanced privacy measurement techniques. To this end, PrivacyGuard implements a diverse suite of privacy attack-- including membership inference , extraction, and reconstruction attacks -- enabling both off-the-shelf and highly configurable privacy analyses. Its modular architecture allows for the seamless integration of new attacks, and privacy metrics, supporting rapid adaptation to emerging research advances. We make PrivacyGuard available at https://github.com/facebookresearch/PrivacyGuard."
2510.23426,Anti-Flatness and Non-Local Magic in Two-Particle Scattering Processes,"['quant-ph', 'hep-ph', 'nucl-th']","['C. E. P. Robin', 'M. J. Savage']","Non-local magic and anti-flatness provide a measure of the quantum complexity in the wavefunction of a physical system. Supported by entanglement, they cannot be removed by local unitary operations, thus providing basis-independent measures, and sufficiently large values underpin the need for quantum computers in order to perform precise simulations of the system at scale. Towards a better understanding of the quantum-complexity generation by fundamental interactions, the building blocks of many-body systems, we consider non-local magic and anti-flatness in two-particle scattering processes, specifically focusing on low-energy nucleon-nucleon scattering and high-energy Moller scattering. We find that the non-local magic induced in both interactions is four times the anti-flatness (which is found to be true for any two-qubit wavefunction), and verify the relation between the Clifford-averaged anti-flatness and total magic. For these processes, the anti-flatness is a more experimentally accessible quantity as it can be determined from one of the final-state particles, and does not require spin correlations. While the MOLLER experiment at the Thomas Jefferson National Accelerator Facility does not include final-state spin measurements, the results presented here may add motivation to consider their future inclusion."
2510.23425,A grad-curl conforming virtual element method for a grad-curl problem linking the 3D quad-curl problem and Stokes system,['math.NA'],"['Xiaojing Dong', 'Yibing Han', 'Yunqing Huang']","Based on the Stokes complex with vanishing boundary conditions and its dual complex, we reinterpret a grad-curl problem arising from the quad-curl problem as a new vector potential formulation of the three-dimensional Stokes system. By extending the analysis to the corresponding non-homogeneous problems and the accompanying trace complex, we construct a novel $\boldsymbol{H}(\operatorname{grad-curl})$-conforming virtual element space with arbitrary approximation order that satisfies the exactness of the associated discrete Stokes complex. In the lowest-order case, three degrees of freedom are assigned to each vertex and one to each edge. For the grad-curl problem, we rigorously establish the interpolation error estimates, the stability of discrete bilinear forms, and the convergence of the proposed element on polyhedral meshes. As a discrete vector potential formulation of the Stokes problem, the resulting system is pressure-decoupled and symmetric positive definite. Some numerical examples are presented to verify the theoretical results."
2510.23424,Causal Deep Q Network,['cs.AI'],"['Elouanes Khelifi', 'Amir Saki', 'Usef Faghihi']","Deep Q Networks (DQN) have shown remarkable success in various reinforcement learning tasks. However, their reliance on associative learning often leads to the acquisition of spurious correlations, hindering their problem-solving capabilities. In this paper, we introduce a novel approach to integrate causal principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational Causal Effect) formula for estimating causal effects. By incorporating causal reasoning during training, our proposed framework enhances the DQN's understanding of the underlying causal structure of the environment, thereby mitigating the influence of confounding factors and spurious correlations. We demonstrate that integrating DQNs with causal capabilities significantly enhances their problem-solving capabilities without compromising performance. Experimental results on standard benchmark environments showcase that our approach outperforms conventional DQNs, highlighting the effectiveness of causal reasoning in reinforcement learning. Overall, our work presents a promising avenue for advancing the capabilities of deep reinforcement learning agents through principled causal inference."
2510.23423,One-arm exponents of the high-dimensional Ising model,"['math.PR', 'math-ph']","['Diederik van Engelenburg', 'Christophe Garban', 'Romain Panis', 'Franco Severo']","We study the probability that the origin is connected to the boundary of the box of size $n$ (the one-arm probability) in several percolation models related to the Ising model. We prove that different universality classes emerge at criticality.
  - For the FK-Ising measure in a box of size $n$ with wired boundary conditions, we prove that this probability decays as $1/n$ in dimensions $d>4$, and as $1/n^{1+o(1)}$ when $d=4$.
  - For the infinite volume FK-Ising measure, we prove that this probability decays as $1/n^2$ in dimensions $d>6$, and as $1/n^{2+o(1)}$ when $d=6$.
  - For the sourceless double random current measure, we prove that this probability decays as $1/n^{d-2}$ in dimensions $d>4$.
  Additionally, for the infinite volume FK-Ising measure, we show that the one-arm probability is $1/n^{1+o(1)}$ in dimension $d=4$, and at least $1/n^{3/2}$ in dimension $d=5$. This establishes that the FK-Ising model has upper-critical dimension equal to $6$, in contrast to the Ising model, where it is known to be less or equal to $4$, thus solving a conjecture of Chayes, Coniglio, Machta, and Schtengel."
2510.23422,Prospects towards Paired Electrolysis at Industrial Currents,"['physics.chem-ph', 'doi', '10.1016/j.joule.2025.102049']","['Lu Xia', 'Kaiqi Zhao', 'Sunil Kadam', 'M. Dolores Blanco-González', 'María D. Hernández Alonso', 'F. Pelayo García de Arquer']","Paired electrolysis at industrial current densities offers an energy-efficient and sustainable alternative to thermocatalytic chemical synthesis by leveraging anodic and cathodic valorization. However, its industrial feasibility remains constrained by system integration, including reactor assembly, asymmetric electron transfer kinetics, membrane selection, mass transport limitations, and techno-economic bottlenecks. Addressing these challenges requires an engineering-driven approach that integrates reactor architecture, electrode-electrolyte interactions, reaction pairing, and process optimization. Here, we discuss scale-specific electrochemical reactor assembly strategies, transitioning from half-cell research to full-scale stack validation. We develop reaction pairing frameworks that align electrocatalyst design with electrochemical kinetics, enhancing efficiency and selectivity under industrial operating conditions. We also establish application-dependent key performance indicators (KPIs) and benchmark propylene oxidation coupled with hydrogen evolution reaction (HER) or oxygen reduction reaction (ORR) against existing industrial routes to evaluate process viability. Finally, we propose hybrid integration models that embed paired electrolysis into existing industrial workflows, overcoming adoption barriers."
2510.23421,Exploring Vulnerability in AI Industry,"['econ.GN', 'cs.AI']","['Claudio Pirrone', 'Stefano Fricano', 'Gioacchino Fazio']","The rapid ascent of Foundation Models (FMs), enabled by the Transformer architecture, drives the current AI ecosystem. Characterized by large-scale training and downstream adaptability, FMs (as GPT family) have achieved massive public adoption, fueling a turbulent market shaped by platform economics and intense investment. Assessing the vulnerability of this fast-evolving industry is critical yet challenging due to data limitations. This paper proposes a synthetic AI Vulnerability Index (AIVI) focusing on the upstream value chain for FM production, prioritizing publicly available data. We model FM output as a function of five inputs: Compute, Data, Talent, Capital, and Energy, hypothesizing that supply vulnerability in any input threatens the industry. Key vulnerabilities include compute concentration, data scarcity and legal risks, talent bottlenecks, capital intensity and strategic dependencies, as well as escalating energy demands. Acknowledging imperfect input substitutability, we propose a weighted geometrical average of aggregate subindexes, normalized using theoretical or empirical benchmarks. Despite limitations and room for improvement, this preliminary index aims to quantify systemic risks in AI's core production engine, and implicitly shed a light on the risks for downstream value chain."
2510.23420,On the Hamiltonian Bicirculants,['math.CO'],"['S. Bonvicini', 'T. Pisanski', 'A. Žitnik']","A bicirculant is a regular graph that admits a semi-regular automorphism with two vertex-orbits of the same size. By $m$ we denote the size of vertex-orbits and by $d$ the valence of a bicirculant. Furthermore, we denote by $s$ the valence of the bipartite graph joining the two vertex-orbits. In 1983, Brian Alspach proved that the only non-hamiltonian generalized Petersen graphs are $G(m,2)$ with $m \equiv 5 \pmod 6$. In a recent paper we conjectured that this is the only exception among regular, connected bicirculants of degree $d > 1$ and we have verified the conjecture for the quartic bicirculants with $s=2$, also known as the generalized rose window graphs.
  In this paper we develop tools and apply them for a partial verification of the conjecture. We show that the conjecture holds for all bicirculants with $s \leq 2$. As a consequence we obtain that every connected bicirculant with $s \ge 3$ is hamiltonian if $m$ is a product of at most three prime powers. In particular, every connected bicirculant with $s \ge 3$ is hamiltonian for even $m<210$ and odd $m < 1155$. Our results imply that many other families of bicirculants are hamiltonian. For example, all bicirculants with $d-s$ odd are hamiltonian."
2510.23419,Probabilistic Computing Optimization of Complex Spin-Glass Topologies,"['cond-mat.dis-nn', 'cs.ET', 'quant-ph']","['Fredrik Hasselgren', 'Max O. Al-Hasso', 'Amy Searle', 'Joseph Tindall', 'Marko von der Leyen']","Spin glass systems as lattices of disordered magnets with random interactions have important implications within the theory of magnetization and applications to a wide-range of hard combinatorial optimization problems. Nevertheless, despite sustained efforts, algorithms that attain both high accuracy and efficiency remain elusive. Due to their topologies being low-$k$-partite such systems are well suited to a probabilistic computing (PC) approach using probabilistic bits (P-bits). Here we present complex spin glass topologies solved on a simulated PC realization of an Ising machine. First, we considered a number of three dimensional Edwards-Anderson cubic spin-glasses randomly generated as well as found in the literature as a benchmark. Second, biclique topologies were identified as a likely candidate for a comparative advantage compared to other state-of-the-art techniques, with a range of sizes simulated. We find that the number of iterations necessary to find solutions of a given quality has constant scaling with system size past a saturation point if one assumes perfect parallelization of the hardware. Therefore a PC architecture can trade the computational depth of other methods for parallelized width by connecting a number of P-bits that scales linearly in system size. This constant scaling is shown to persist across a number of solution qualities, up to a certain limit beyond which resource constraints limited further investigation. The saturation point varies between topologies and qualities and becomes exponentially hard in the limit of finding the ground truth. Furthermore we demonstrate that our PC architecture can solve spin-glass topologies to the same quality as the most advanced quantum annealer in minutes, making modest assumptions about their implementation on hardware."
2510.23418,Lagrangian skeleta of very affine complete intersections,"['math.SG', 'math.AG']",['Danil Koževnikov'],"Let $Z^\circ$ be a complete intersection inside $(\mathbb{C}^*)^n$ that compactifies to a smooth Calabi-Yau subvariety $Z$ inside a Fano toric variety $X$. We compute the skeleton of $Z^\circ$ and describe its decomposition into standard pieces that are mirror to toric varieties, which generalises the existing results in the case of hypersurfaces. This set-up was first considered by Batyrev and Borisov, who used combinatorial techniques to construct a mirror pair $(Z,\check{Z})$ of such complete intersections. We use our main result to establish homological mirror symmetry for Batyrev-Borisov pairs in the large-volume limit."
2510.23417,Filling the Gap: Atom Probe Tomography of Porous Structures Enabled by Site Specific SEMGlu Curing,"['physics.ins-det', 'cond-mat.mtrl-sci']","['Lukas Worch', 'James O. Douglas', 'Kavin Arunasalam', 'Baptiste Gault', 'Valeria Nicolosi', 'Michele Shelly Conroy']","Porous microstructures, while central to many functional materials, remain difficult to characterize quantitatively by atom probe tomography (APT). Although several strategies have been proposed over the past decade, most remain constrained by significant practical or technical limitations. Here, we introduce an in situ pore filling approach that integrates seamlessly into conventional specimen preparation workflows. The method employs a vacuum compatible resin that is rapidly cured by the electron beam during standard ion beam based preparation, eliminating the need for additional instrumentation or extensive sample handling. We demonstrate the effectiveness of this approach using a porous SnSe + MXene electrode, a material system otherwise difficult to analyze via APT characterisation. This method offers a robust, accessible solution for extending APT analysis to porous materials"
2510.23416,Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation,"['cs.CV', 'eess.SP']","['Marco Antonio Ortiz Rincon', 'Yihui Yang', 'Christoph Holst']","This study presents a novel workflow designed to efficiently and accurately register large-scale mobile laser scanning (MLS) point clouds to a target model point cloud in urban street scenarios. This workflow specifically targets the complexities inherent in urban environments and adeptly addresses the challenges of integrating point clouds that vary in density, noise characteristics, and occlusion scenarios, which are common in bustling city centers. Two methodological advancements are introduced. First, the proposed Semi-sphere Check (SSC) preprocessing technique optimally fragments MLS trajectory data by identifying mutually orthogonal planar surfaces. This step reduces the impact of MLS drift on the accuracy of the entire point cloud registration, while ensuring sufficient geometric features within each fragment to avoid local minima. Second, we propose Planar Voxel-based Generalized Iterative Closest Point (PV-GICP), a fine registration method that selectively utilizes planar surfaces within voxel partitions. This pre-process strategy not only improves registration accuracy but also reduces computation time by more than 50% compared to conventional point-to-plane ICP methods. Experiments on real-world datasets from Munich's inner city demonstrate that our workflow achieves sub-0.01 m average registration accuracy while significantly shortening processing times. The results underscore the potential of the proposed methods to advance automated 3D urban modeling and updating, with direct applications in urban planning, infrastructure management, and dynamic city monitoring."
2510.23415,Towards Generalisable Foundation Models for 3D Brain MRI,['cs.CV'],"['Moona Mazher', 'Geoff J. M. Parker', 'Daniel C. Alexander']","Foundation models in artificial intelligence (AI) are transforming medical imaging by enabling general-purpose feature learning from large-scale, unlabeled datasets. In this work, we introduce BrainFound, a self-supervised foundation model for brain MRI, built by extending DINO-v2, a vision transformer originally designed for 2D natural images. BrainFound adapts DINO-v2 to model full 3D brain anatomy by incorporating volumetric information from sequential MRI slices, moving beyond conventional single-slice paradigms. It supports both single- and multimodal inputs, enabling a broad range of downstream tasks, including disease detection and image segmentation, while generalising across varied imaging protocols and clinical scenarios. We show that BrainFound consistently outperforms existing self-supervised pretraining strategies and supervised baselines, particularly in label-scarce and multi-contrast settings. By integrating information from diverse 3D MRI modalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces dependency on extensive expert annotations. This flexibility makes BrainFound a scalable and practical solution for 3D neuroimaging pipelines, with significant potential for clinical deployment and research innovation."
2510.23414,Symmetria: A Synthetic Dataset for Learning in Point Clouds,['cs.CV'],"['Ivan Sipiran', 'Gustavo Santelices', 'Lucas Oyarzún', 'Andrea Ranieri', 'Chiara Romanengo', 'Silvia Biasotti', 'Bianca Falcidieno']","Unlike image or text domains that benefit from an abundance of large-scale datasets, point cloud learning techniques frequently encounter limitations due to the scarcity of extensive datasets. To overcome this limitation, we present Symmetria, a formula-driven dataset that can be generated at any arbitrary scale. By construction, it ensures the absolute availability of precise ground truth, promotes data-efficient experimentation by requiring fewer samples, enables broad generalization across diverse geometric settings, and offers easy extensibility to new tasks and modalities. Using the concept of symmetry, we create shapes with known structure and high variability, enabling neural networks to learn point cloud features effectively. Our results demonstrate that this dataset is highly effective for point cloud self-supervised pre-training, yielding models with strong performance in downstream tasks such as classification and segmentation, which also show good few-shot learning capabilities. Additionally, our dataset can support fine-tuning models to classify real-world objects, highlighting our approach's practical utility and application. We also introduce a challenging task for symmetry detection and provide a benchmark for baseline comparisons. A significant advantage of our approach is the public availability of the dataset, the accompanying code, and the ability to generate very large collections, promoting further research and innovation in point cloud learning."
2510.23413,Parametric Iteration in Resource Theories,['cs.LO'],"['Alessandro Di Giorgio', 'Pawel Sobocinski', 'Niels Voorneveld']","Many algorithms are specified with respect to a fixed but unspecified parameter. Examples of this are especially common in cryptography, where protocols often feature a security parameter such as the bit length of a secret key.
  Our aim is to capture this phenomenon in a more abstract setting. We focus on resource theories -- general calculi of processes with a string diagrammatic syntax -- introducing a general parametric iteration construction. By instantiating this construction within the Markov category of probabilistic Boolean circuits and equipping it with a suitable metric, we are able to capture the notion of negligibility via asymptotic equivalence, in a compositional way. This allows us to use diagrammatic reasoning to prove simple cryptographic theorems -- for instance, proving that guessing a randomly generated key has negligible success."
2510.23412,Quantum fluctuations determine the spin-flop transition in hematite,"['cond-mat.str-el', 'cond-mat.mtrl-sci']","['Tobias Dannegger', 'Imre Hagymási', 'Levente Rózsa', 'Ulrich Nowak']","Magnetic phase transitions between ordered phases are often understood on the basis of semi-classical spin models. Deviations from the classical description due to the quantum nature of the atomic spins as well as quantum fluctuations are usually treated as negligible if long-range order is preserved, and are rarely quantified for actual materials. Here, we demonstrate that a fully quantum-mechanical framework is required for a quantitatively correct description of the spin-flop transition in the insulating altermagnet hematite between the collinear antiferromagnetic and the weakly ferromagnetic spin-flop phase at low temperature. By applying both exact diagonalization and density-matrix renormalization group theory to the quantum Heisenberg Hamiltonian, we show how a quantum-mechanical treatment of an ab initio parametrized spin model can significantly improve the predicted low-temperature spin-flop field over a classical description when compared to measurements. Our results imply that quantum fluctuations have a measurable influence on selecting the ground state of a system out of competing ordered magnetic phases at low temperature."
2510.23411,Border Bases in the Rational Weyl Algebra,"['math.AG', 'cs.SC', 'hep-th']","['Carlos Rodriguez', 'Anna-Laura Sattelberger']","Border bases are a generalization of Gröbner bases for polynomial rings. In this article, we introduce border bases for a non-commutative ring of linear differential operators, namely the rational Weyl algebra. We elaborate on their properties and present algorithms to compute with them. We apply this theory to represent integrable connections as cyclic $D$-modules explicitly. As an application, we visit differential equations behind a stringy, a Feynman as well as a cosmological integral. We also address the classification of particular $D$-ideals of a fixed holonomic rank, namely the case of linear PDEs with constant coefficients as well as Frobenius ideals. Our approach rests on the theory of Hilbert schemes of points in affine space."
2510.23410,Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens,['cs.AI'],"['Jiahao Ji', 'Tianyu Wang', 'Yeshu Li', 'Yushen Huo', 'Zhilin Zhang', 'Chuan Yu', 'Jian Xu', 'Bo Zheng']","Auto-bidding is crucial in facilitating online advertising by automatically providing bids for advertisers. While previous work has made great efforts to model bidding environments for better ad performance, it has limitations in generalizability across environments since these models are typically tailored for specific bidding scenarios. To this end, we approach the scenario-independent principles through a unified function that estimates the achieved effect under specific bids, such as budget consumption, gross merchandise volume (GMV), page views, etc. Then, we propose a bidding foundation model Bid2X to learn this fundamental function from data in various scenarios. Our Bid2X is built over uniform series embeddings that encode heterogeneous data through tailored embedding methods. To capture complex inter-variable and dynamic temporal dependencies in bidding data, we propose two attention mechanisms separately treating embeddings of different variables and embeddings at different times as attention tokens for representation learning. On top of the learned variable and temporal representations, a variable-aware fusion module is used to perform adaptive bidding outcome prediction. To model the unique bidding data distribution, we devise a zero-inflated projection module to incorporate the estimated non-zero probability into its value prediction, which makes up a joint optimization objective containing classification and regression. The objective is proven to converge to the zero-inflated distribution. Our model has been deployed on the ad platform in Taobao, one of the world's largest e-commerce platforms. Offline evaluation on eight datasets exhibits Bid2X's superiority compared to various baselines and its generality across different scenarios. Bid2X increased GMV by 4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding foundation model in computational advertising."
2510.23409,Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based Approach,"['cs.LG', 'cs.AI']","['Youngjun Choi', 'Joonseong Kang', 'Sungjun Lim', 'Kyungwoo Song']","Data valuation has become central in the era of data-centric AI. It drives efficient training pipelines and enables objective pricing in data markets by assigning a numeric value to each data point. Most existing data valuation methods estimate the effect of removing individual data points by evaluating changes in model validation performance under in-distribution (ID) settings, as opposed to out-of-distribution (OOD) scenarios where data follow different patterns. Since ID and OOD data behave differently, data valuation methods based on ID loss often fail to generalize to OOD settings, particularly when the validation set contains no OOD data. Furthermore, although OOD-aware methods exist, they involve heavy computational costs, which hinder practical deployment. To address these challenges, we introduce \emph{Eigen-Value} (EV), a plug-and-play data valuation framework for OOD robustness that uses only an ID data subset, including during validation. EV provides a new spectral approximation of domain discrepancy, which is the gap of loss between ID and OOD using ratios of eigenvalues of ID data's covariance matrix. EV then estimates the marginal contribution of each data point to this discrepancy via perturbation theory, alleviating the computational burden. Subsequently, EV plugs into ID loss-based methods by adding an EV term without any additional training loop. We demonstrate that EV achieves improved OOD robustness and stable value rankings across real-world datasets, while remaining computationally lightweight. These results indicate that EV is practical for large-scale settings with domain shift, offering an efficient path to OOD-robust data valuation."
2510.23408,AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines,"['cs.AI', 'cs.DC', 'cs.ET', 'cs.LG', 'cs.MA']","['Abolfazl Younesi', 'Zahra Najafabadi Samani', 'Thomas Fahringer']","Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between high-level user intent and platform-specific implementations across distributed stream processing systems for structured multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an extended version of GoT. AutoStreamPipe combines resilient execution strategies, advanced query analysis, and HGoT to deliver pipelines with good accuracy. Experimental evaluations on diverse pipelines demonstrate that AutoStreamPipe significantly reduces development time (x6.3) and error rates (x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM code-generation methods."
2510.23407,Multi-Task Surrogate-Assisted Search with Bayesian Competitive Knowledge Transfer for Expensive Optimization,['cs.NE'],"['Yi Lu', 'Xiaoming Xue', 'Kai Zhang', 'Liming Zhang', 'Guodong Chen', 'Chenming Cao', 'Piyang Liu', 'Kay Chen Tan']","Expensive optimization problems (EOPs) present significant challenges for traditional evolutionary optimization due to their limited evaluation calls. Although surrogate-assisted search (SAS) has become a popular paradigm for addressing EOPs, it still suffers from the cold-start issue. In response to this challenge, knowledge transfer has been gaining popularity for its ability to leverage search experience from potentially related instances, ultimately facilitating head-start optimization for more efficient decision-making. However, the curse of negative transfer persists when applying knowledge transfer to EOPs, primarily due to the inherent limitations of existing methods in assessing knowledge transferability. On the one hand, a priori transferability assessment criteria are intrinsically inaccurate due to their imprecise understandings. On the other hand, a posteriori methods often necessitate sufficient observations to make correct inferences, rendering them inefficient when applied to EOPs. Considering the above, this paper introduces a Bayesian competitive knowledge transfer (BCKT) method developed to improve multi-task SAS (MSAS) when addressing multiple EOPs simultaneously. Specifically, the transferability of knowledge is estimated from a Bayesian perspective that accommodates both prior beliefs and empirical evidence, enabling accurate competition between inner-task and inter-task solutions, ultimately leading to the adaptive use of promising solutions while effectively suppressing inferior ones. The effectiveness of our method in boosting various SAS algorithms for both multi-task and many-task problems is empirically validated, complemented by comparative studies that demonstrate its superiority over peer algorithms and its applicability to real-world scenarios. The source code of our method is available at https://github.com/XmingHsueh/MSAS-BCKT."
2510.23406,MIGHTEE-HI: The HI mass-stellar mass relation of massive galaxies and the HI mass function at 0.25<z<0.5,"['astro-ph.GA', 'astro-ph.CO']","['Hengxing Pan', 'Matt J. Jarvis', 'Ian Heywood', 'Tariq Yasin', 'Natasha Maddox', 'Mario G. Santos', 'Maarten Baes', 'Anastasia A. Ponomareva', 'Sambatriniaina H. A. Rajohnson']","The relationship between the already formed stellar mass in a galaxy and the gas reservoir of neutral atomic hydrogen, is a key element in our understanding of how gas is turned into stars in galaxy haloes. In this paper, we measure the $M_{\rm HI}-M_{\star}$ relation based on a stellar-mass selected sample at $0.25 < z < 0.5$ and the MIGHTEE-HI DR1 spectral data. Using a powerful Bayesian stacking technique, for the first time we are also able to measure the underlying bivariate distribution of HI mass and stellar mass of galaxies with $M_\star > 10^{9.5}$ M$_{\odot}$, finding that an asymmetric underlying HI distribution is strongly preferred by our complete samples. We define the concepts of the average of the logarithmic HI mass, $\langle\log_{10}(M_{\rm HI})\rangle$, and the logarithmic average of the HI mass, $\log_{10}(\langle M_{\rm HI}\rangle)$, and find that the difference between $\langle\log_{10}(M_{\rm HI})\rangle$ and $\log_{10}(\langle M_{\rm HI}\rangle)$ can be as large as $\sim$0.5 dex for the preferred asymmetric HI distribution. We observe shallow slopes in the underlying $M_{\rm HI}-M_{\star}$ scaling relations, suggesting the presence of an upper HI mass limit beyond which a galaxy can no longer retain further HI gas. From our bivariate distribution we also infer the HI mass function at this redshift and find tentative evidence for a decrease of 2-10 times in the co-moving space density of the most HI massive galaxies up to $z\sim 0.5$."
2510.23405,Observable Signatures of Quarkyonic Phase in Neutron Stars,"['nucl-th', 'astro-ph.HE', 'gr-qc']","['Probit J Kalita', 'Tuhin Malik', 'Tianqi Zhao', 'Bharat Kumar', 'James M. Lattimer']","In this letter, we perform Bayesian inference on quarkyonic equation-of-state models and find they remain viable under all current astrophysical constraints. Crucially, we identify a novel observational diagnostic- the slope of the M-R relation at fixed mass versus the central sound speed. Quarkyonic stars occupy a distinct region (high central $c_s^2$ and positive $\frac{dR}{dM}$) from purely nucleonic stars. This could be tested by future radius measurements of two stars of different mass. Our results indicate that if a neutron star falls in the quarkyonic region, it would be strong evidence for quarkyonic matter in its core."
2510.23404,"Pentaquarks on the light front, and their mixture with baryons","['hep-ph', 'nucl-th']","['Nicholas Miesch', 'Edward Shuryak', 'Ismail Zahed']","In previous papers we developed the light front formulation for Hamiltonians and wave functions (WFs) for mesons and baryons, with both confinement and chiral symmetry breaking. For baryons limited to the lowest Fock component with three quarks, the longitudinal WF is valued in an equilateral triangle with momentum fractions $x_i,i=1,2,3$. The WF was developed both numerically and using a basis function that diagonalizes the Laplacian with Dirichlet boundary conditions. In this paper we extend this analysis to $n$ quark states, and specialize to pentaquarks ($n=5$). We determine their masses and WFs, and address the mixing between baryons and pentaquarks, the issue central to understanding the observed antiquark sea of baryons."
2510.23403,Evaluation of Spherical Wavelet Framework in Comparsion with Ambisonics,['eess.AS'],"['Ş. Ekmen', 'H. Lee']","Recently, the Spherical Wavelet Framework (SWF) was proposed to combine the benefits of Ambisonics and Object-Based Audio (OBA) by utilising highly localised basis functions. SWF can enhance the sweet-spot area and reduce localisation blur while still enabling a sparse representation of the complete sound field, making storage and transmission more efficient. Initial vector analysis and listening test of SWF have shown promising results; however, these findings are limited to very specific conditions and do not include perceptual metrics. The present study investigates SWF in greater detail, comparing it with Ambisonics. The comparison was carried out using IACC, ITD, and ILD estimations, as well as listening tests with ecologically valid sound sources. Various reproduction layouts: regular polyhedron, t-design, and Lebedev grid with their corresponding Ambisonics orders and channel counts were evaluated. Results indicate that SWF is rated significantly more similar to the reference than Ambisonics is, in terms of overall spatial and timbral fidelity; however, it is considerably dependent on the subdivison of the sphere. Moreover, it cannot natively represent a wave arriving at a continuous direction. Possible solutions are proposed."
2510.23402,A Sequential Planning Framework for the Operational Reality of Interacting Air Traffic Flow Regulations and Traffic Flow Programs,['math.OC'],"['Thinh Hoang', 'Daniel Delahaye']","Air Traffic Flow Management (ATFM) traffic regulations are being increasingly used as rising demand meets persistent workforce shortages. This operational strain has amplified a critical phenomenon that we call \emph{regulation cascading}: the compounding, non-linear interactions that occur when multiple regulations influence one another in unpredictable ways. As the number and complexity of regulations grow, cascading effects become more pronounced, undermining the network operator's ability to protect sectors reliably. To address this challenge, we introduce RegulationZero, a sequential planning framework that natively operates in the regulation space, optimizing over ordered sequences of flow-level regulations that remain fully compatible with existing slot-allocation systems such as CASA and RBS++. At its core, the method employs a hierarchical Monte Carlo Tree Search (MCTS) that first samples congestion hotspots and then selects candidate regulations synthesized by a local proposal engine. Each proposal is evaluated by a fast First-Planned-First-Served (FPFS) allocator to estimate its reward, with these feedbacks guiding the subsequent MCTS exploration."
2510.23401,Determination of the Muon Lifetime in $^{76}$Se with the MONUMENT experiment,"['nucl-ex', 'nucl-th']","['G. R. Araujo', 'D. Bajpai', 'L. Baudis', 'V. Belov', 'E. Bossio', 'T. E. Cocolios', 'H. Ejiri', 'M. Fomina', 'K. Gusev', 'I. H. Hashim', 'M. Heines', 'S. Kazartsev', 'A. Knecht', 'E. Mondragón', 'Z. W. Ng', 'I. Ostrovskiy', 'N. Rumyantseva', 'S. Schönert', 'M. Schwarz', 'A. Shehada', 'E. Shevchik', 'M. Shirchenko', 'Y. Shitov', 'J. Suhonen', 'S. M. Vogiatzi']","Ordinary muon capture provides a benchmark for the nuclear physics models of neutrinoless double beta decay under comparable momentum transfer conditions. The total capture strength defines the lifetime of the muonic atom. The muon lifetime in $^{76}$Se, the daughter nucleus of $^{76}$Ge, was determined with improved accuracy by the MONUMENT collaboration, using an array of high-purity germanium detectors and a set of scintillator counters at the $π$E1 muon beam line of the Paul Scherrer Institute. The new value of (135.1 $\pm$ 0.5) ns agrees with phenomenological calculations based on the quasiparticle random phase approximation with unquenched axial-vector coupling."
2510.23400,"Solar flare forecasting with foundational transformer models across image, video, and time-series modalities","['astro-ph.IM', 'astro-ph.SR']","['S. Riggi', 'P. Romano', 'A. Pilzer', 'U. Becciani']","We present a comparative study of transformer-based architectures for solar flare forecasting using heterogeneous data modalities, including images, video sequences, and time-series observations. Our analysis evaluates three recent foundational models - SigLIP2 for image encoding, VideoMAE for spatio-temporal video representation, and Moirai2 for multivariate time-series forecasting - applied to publicly available datasets of solar magnetograms from the SDO/HMI mission and soft X-ray fluxes acquired by GOES satellites. All models are trained and validated under consistent data splits and evaluation criteria, with the goal of assessing the strengths and limitations of transformer backbones across spatial and temporal representations of solar activity. We investigate multiple loss formulations (weighted BCE, focal, and score-oriented) and training balance strategies to mitigate class imbalance typical of flare datasets. Results show that while both SigLIP2 and VideoMAE achieve typical performance on image and video data (True Skill Statistic TSS~0.60-0.65), the time-series model Moirai2 reaches superior forecasting skill (TSS~0.74) using irradiance-based temporal evolution alone. These findings highlight the potential of pretrained transformer architectures and cross-modal learning for advancing operational space weather forecasting, paving the way toward unified multimodal models that integrate visual and temporal information."
2510.23399,Color and Frequency Correction for Image Colorization,['cs.CV'],['Yun Kai Zhuang'],"The project has carried out the re-optimization of image coloring in accordance with the existing Autocolorization direction model DDColor. For the experiments on the existing weights of DDColor, we found that it has limitations in some frequency bands and the color cast problem caused by insufficient input dimension. We construct two optimization schemes and combine them, which achieves the performance improvement of indicators such as PSNR and SSIM of the images after DDColor."
2510.23398,Free-space quantum interface of a single atomic tweezer array with light,['quant-ph'],"['Yakov Solomons', 'Roni Ben-Maimon', 'Arpit Behera', 'Ofer Firstenberg', 'Nir Davidson', 'Ephraim Shahmoon']","We present a practical approach for interfacing light with a two-dimensional atomic tweezer array. Typical paraxial fields are poorly matched to the array's multi-diffraction-order radiation pattern, thus severely limiting the interface coupling efficiency. Instead, we propose to design a field mode that naturally couples to the array: it consists of a unique superposition of multiple beams corresponding to the array's diffraction orders. This composite mode can be generated from a single Gaussian beam using standard free-space optics, including spatial light modulators and a single objective lens. For a triangular array with lattice spacing about twice the wavelength, all diffraction angles remain below 35 degrees, making the scheme compatible with standard objectives of numerical aperture NA <= 0.7. Our analytical theory and scattering simulations reveal that the interface efficiency r0 for quantum information tasks scales favorably with the array atom number N: reaching >0.99 (>0.9999) for N = 149 (N approximately 1000) and scaling as 1 - r0 scales as 1/N for large N. The scheme is robust to optical imperfections and atomic-position errors, offering a viable path for quantum light-matter applications and state readout in current tweezer-array platforms."
2510.23397,VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations,['cs.CV'],"['Lu Dong', 'Haiyu Zhang', 'Han Lin', 'Ziang Yan', 'Xiangyu Zeng', 'Hongjie Zhang', 'Yifei Huang', 'Yi Wang', 'Zhen-Hua Ling', 'Limin Wang', 'Yali Wang']","Video temporal grounding (VTG) aims to locate precise segments in videos based on language queries, which is a fundamental challenge in video understanding. While recent Multimodal Large Language Models (MLLMs) have shown promise in tackling VTG through reinforcement learning (RL), they overlook the challenges arising from both the quality and difficulty of training samples. (1) Partially annotated samples. Many samples contain relevant segments beyond the annotated interval, introducing ambiguous supervision. (2) Hard-to-ground samples. Samples with poor zero-shot performance produce consistently low and indistinguishable rewards during RL training, exhibiting no clear preference among multiple outputs and thus hindering learning efficiency. To address these challenges, we propose VideoTG-R1, a novel curriculum RL framework with reflected boundary annotations, enabling data-efficient training. Specifically, we propose a Boundary Reflection Agent that utilizes MLLMs to predict query-relevant timestamps outside the annotated intervals, allowing us to identify and filter out partially annotated samples, thereby reducing ambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess the training difficulty of each sample and design a curriculum RL strategy that dynamically masks the videos of hard-to-ground samples according to the training steps, easing the training difficulty and providing clearer preference. Experiments on the VTG and grounded VideoQA tasks demonstrate the effectiveness of our method. Remarkably, with only 10% of the training samples and 21% of the computational budget, VideoTG-R1 outperforms full-data counterparts under both group relative policy optimization (GRPO) and supervised fine-tuning (SFT). The code is available at https://github.com/ldong1111/VideoTG-R1."
2510.23396,EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting,"['cs.CL', 'cs.AI']","['Musleh Alharthi', 'Kaleel Mahmood', 'Sarosh Patel', 'Ausif Mahmood']","The immense success of the Transformer architecture
  in Natural Language Processing has led to its adoption in Time Se ries Forecasting (TSF), where superior performance has been shown.
  However, a recent important paper questioned their effectiveness by
  demonstrating that a simple single layer linear model outperforms
  Transformer-based models. This was soon shown to be not as valid,
  by a better transformer-based model termed PatchTST. More re cently, TimeLLM demonstrated even better results by repurposing a
  Large Language Model (LLM) for the TSF domain. Again, a follow
  up paper challenged this by demonstrating that removing the LLM
  component or replacing it with a basic attention layer in fact yields
  better performance. One of the challenges in forecasting is the fact
  that TSF data favors the more recent past, and is sometimes subject
  to unpredictable events. Based upon these recent insights in TSF, we
  propose a strong Mixture of Experts (MoE) framework. Our method
  combines the state-of-the-art (SOTA) models including xLSTM, en hanced Linear, PatchTST, and minGRU, among others. This set of
  complimentary and diverse models for TSF are integrated in a Trans former based MoE gating network. Our proposed model outperforms
  all existing TSF models on standard benchmarks, surpassing even the
  latest approaches based on MoE frameworks."
2510.23395,Detecting Religious Language in Climate Discourse,"['cs.CL', 'cs.AI']","['Evy Beijen', 'Pien Pieterse', 'Yusuf Çelik', 'Willem Th. van Peursen', 'Sandjai Bhulai', 'Meike Morren']","Religious language continues to permeate contemporary discourse, even in ostensibly secular domains such as environmental activism and climate change debates. This paper investigates how explicit and implicit forms of religious language appear in climate-related texts produced by secular and religious nongovernmental organizations (NGOs). We introduce a dual methodological approach: a rule-based model using a hierarchical tree of religious terms derived from ecotheology literature, and large language models (LLMs) operating in a zero-shot setting. Using a dataset of more than 880,000 sentences, we compare how these methods detect religious language and analyze points of agreement and divergence. The results show that the rule-based method consistently labels more sentences as religious than LLMs. These findings highlight not only the methodological challenges of computationally detecting religious language but also the broader tension over whether religious language should be defined by vocabulary alone or by contextual meaning. This study contributes to digital methods in religious studies by demonstrating both the potential and the limitations of approaches for analyzing how the sacred persists in climate discourse."
2510.23394,fair_data.py: implementing FAIR data compliance in Tribchem,['cond-mat.mtrl-sci'],"['Lucrezia Berghenti', 'Elisa Damiani', 'Margherita Marsili', 'Maria Clelia Righi']","The increasing complexity and volume of data generated by high-throughput computational materials science require robust tools to ensure their accessibility, reproducibility, and reuse. In particular, integrating the FAIR Guiding Principles (Findable, Accessible, Interoperable, and Reusable) into computational workflows is essential to enable open science practices. TribChem is an open source Python software developed for the automated simulation of solid-solid interfaces using density functional theory (DFT). While TribChem already incorporates several FAIR-aligned features, we present here a dedicated FAIR utility designed to transform TribChem results into FAIR-compliant datasets. This utility comprises two tools: fair_data.py, which automatically generates standardized machine- and human-readable outputs from the TribChem database, and retrieve_data.py, which facilitates efficient data extraction through a keyword-based interface. In this paper we show the capabilities of the fair utility with examples for bulk, surface, and interface systems. The implementation allows seamless integration with public repositories such as Zenodo, paving the way for reproducible research and fostering data-driven materials discovery."
2510.23393,The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation,['cs.LG'],"['Farid Bagirov', 'Mikhail Arkhipov', 'Ksenia Sycheva', 'Evgeniy Glukhov', 'Egor Bogomolov']","The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models. Despite its success in single generation problem solving, the reinforcement learning fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of generations and a resulting degradation of performance during Best-of-N sampling for large N values. In this work, we focus on optimizing the max@k metric, a continuous generalization of pass@k. We derive an unbiased on-policy gradient estimate for direct optimization of this metric. Furthermore, we extend our derivations to the off-policy updates, a common element in modern RLVR algorithms, that allows better sample efficiency. Empirically, we show that our objective effectively optimizes max@k metric in off-policy scenarios, aligning the model with the Best-of-N inference strategy."
2510.23392,Sturm-Liouville problems on graphs with Robin boundary conditions,"['math.SP', 'math-ph']","['Yuri Latushkin', 'Vyacheslav Pivovarchik', 'Alesia Supranovych']","We study characteristic functions and describe asymptotics of the eigenvalues for the spectral Sturm-Liouville problem on graphs equipped with Robin-Kirhhoff boundary conditions. Also, we show how to recover the coefficients in the Robin conditions for the quantum graphs provided the shape of the graphs and some Robin eigenvalues are known."
2510.23391,Conduction velocity of intracortical axons in monkey primary visual cortex grows with distance: implications for computation,['q-bio.NC'],['Li Zhaoping'],"A critical visual computation is to construct global scene properties from activities of early visual cortical neurons which have small receptive fields. Such a computation is enabled by contextual influences, through which a neuron's response to visual inputs is influenced by contextual inputs outside its classical receptive fields. Accordingly, neurons can signal global properties including visual saliencies and figure-ground relationships. Many believe that intracortical axons conduct signals too slowly to bring the contextual information from receptive fields of other neurons. A popular opinion is that much of the contextual influences arise from feedback from higher visual areas whose neurons have larger receptive fields. This paper re-examines pre-existing data to reveal these unexpected findings: the conduction speed of V1 intracortical axons increases approximately linearly with the conduction distance, and is sufficiently high for conveying the contextual influences. Recognizing the importance of intracortical contribution to critical visual computations should enable fresh progress in answering long-standing questions."
2510.23390,The Near-Ultraviolet Spectra of FU Orionis Accretion Disks,"['astro-ph.SR', 'astro-ph.EP']","['Adolfo S. Carvalho', 'Lynne A. Hillenbrand', 'Gregory J. Herczeg', 'Kevin France']","We present the results of the first high-sensitivity NUV (1800 to 3200 Å) survey of FU Ori objects, using the \textit{Hubble Space Telescope} (HST) STIS spectrograph. We compare new low resolution spectra for 6 sources with predictions from accretion disk models and find that all show emission in excess of the disk model spectrum. The physical properties of the NUV emission excess are very consistent among the sample, with a mean luminosity of $10^{-1.11 \pm 0.4} \ L_\odot$ and temperature of $16400 \pm 2600$ K -- despite spanning 0.9 dex in $M_*$, 1.3 dex in $\dot{M}$, and 0.7 dex in $L_\mathrm{acc}$. We use the spectra to conclusively rule out the existence of a hot boundary layer in FU Ori accretion disks. We then discuss the source of the excess emission in the context of recent simulations of FU Ori outbursts and boundary layer accretion. The UV spectra also show the often-seen \ion{C}{2}] 2326 Å multiplet and \ion{Mg}{2} 2796/2803 Å doublet, as well as the unusual \ion{Fe}{2}] 2507/2509 Å doublet, a feature that is not seen in the existing UV spectra of other young stellar objects. We measure and compare the luminosities of these lines in outbursting with those in non-outbursting objects."
2510.23389,Floating-Point Neural Network Verification at the Software Level,"['cs.SE', 'cs.CR', 'cs.LG']","['Edoardo Manino', 'Bruno Farias', 'Rafael Sá Menezes', 'Fedor Shmarov', 'Lucas C. Cordeiro']","The behaviour of neural network components must be proven correct before deployment in safety-critical systems. Unfortunately, existing neural network verification techniques cannot certify the absence of faults at the software level. In this paper, we show how to specify and verify that neural networks are safe, by explicitly reasoning about their floating-point implementation. In doing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural network verification examples that cover activation functions, common layers, and full neural networks of up to 170K parameters. Our verification suite is written in plain C and is compatible with the format of the International Competition on Software Verification (SV-COMP). Thanks to it, we can conduct the first rigorous evaluation of eight state-of-the-art software verifiers on neural network code. The results show that existing automated verification tools can correctly solve an average of 11% of our benchmark, while producing around 3% incorrect verdicts. At the same time, a historical analysis reveals that the release of our benchmark has already had a significantly positive impact on the latter."
2510.23388,Pseudo-Nambu-Goldstone Dark Matter in Flux Compactification,"['hep-ph', 'hep-th']","['Kento Akamatsu', 'Takuya Hirose', 'Nobuhito Maru', 'Akio Nago']","We study a six-dimensional U(1)$_χ$ gauge theory compactified on a magnetized torus, where the zero mode of the extra-dimensional gauge field (a Wilson-line (WL) scalar field) plays the role of a pseudo-Nambu-Goldstone (pNG) dark matter (DM) candidate. The pNG DM is naturally included by construction without introducing an additional scalar field. We show that the leading spin-independent DM-nucleus amplitude is suppressed by momentum transfer in our model as expected from the pNG DM model. This suppression allows the model to evade the current severe direct-detection bounds while achieving the observed thermal relic abundance in well-defined regions of parameter space."
2510.23387,Probing phase transitions of regular black holes in anti-de Sitter space with Lyapunov exponent,"['gr-qc', 'hep-th']","['Hao Xie', 'Si-Jiang Yang']","We investigate the relationship between thermodynamic phase transitions and the Lyapunov exponent of charged regular anti-de Sitter black holes in quasi-topological gravity. Our results show that the Lyapunov exponent displays oscillatory behavior during phase transitions. Moreover, along the coexistence curve the Lyapunov exponent changes discontinously and continuously at the critical point. Near the critical point, the Lyapunov exponent follows a power-law behavior with a critical exponent of 1/2, suggesting its role as an order parameter and encodes information on black hole phase transitions."
2510.23386,Full-Dynamics Real-Time Nonlinear Model Predictive Control of Heavy-Duty Hydraulic Manipulator for Trajectory Tracking Tasks,['cs.RO'],"['Alvaro Paz', 'Mahdi Hejrati', 'Pauli Mustalahti', 'Jouni Mattila']","Heavy-duty hydraulic manipulators (HHMs) operate under strict physical and safety-critical constraints due to their large size, high power, and complex nonlinear dynamics. Ensuring that both joint-level and end-effector trajectories remain compliant with actuator capabilities, such as force, velocity, and position limits, is essential for safe and reliable operation, yet remains largely underexplored in real-time control frameworks. This paper presents a nonlinear model predictive control (NMPC) framework designed to guarantee constraint satisfaction throughout the full nonlinear dynamics of HHMs, while running at a real-time control frequency of 1 kHz. The proposed method combines a multiple-shooting strategy with real-time sensor feedback, and is supported by a robust low-level controller based on virtual decomposition control (VDC) for precise joint tracking. Experimental validation on a full-scale hydraulic manipulator shows that the NMPC framework not only enforces actuator constraints at the joint level, but also ensures constraint-compliant motion in Cartesian space for the end-effector. These results demonstrate the method's capability to deliver high-accuracy trajectory tracking while strictly respecting safety-critical limits, setting a new benchmark for real-time control in large-scale hydraulic systems."
2510.23385,On the choking mechanism in supersonic ejectors: a one-dimensional analysis of Reynolds-Averaged Navier Stokes simulations,['physics.flu-dyn'],"['Jan Van den Berghe', 'Miguel A. Mendez', 'Yann Bartosiewicz']","Ejectors are passive devices used in refrigeration, propulsion, and process industries to compress a secondary stream without moving parts. The engineering modeling of choking in these devices remains an open question, with two mechanisms-Fabri and compound choking-proposed in the literature. This work develops a unified one-dimensional framework that implements both mechanisms and compares them with axisymmetric Reynolds-Averaged Navier Stokes (RANS) data processed by cross-sectional averaging. The compound formulation incorporates wall and inter-stream friction and a local pressure-equalization procedure that enables stable integration through the sonic point, together with a normal shock reconstruction. The Fabri formulation is assessed by imposing the dividing streamline extracted from RANS, isolating the sonic condition while avoiding additional modeling assumptions. The calibrated compound model predicts on-design secondary mass flow typically within 2 % with respect to the RANS simulations, rising to 5 % for a strongly under-expanded primary jet due to the equal-pressure constraint. The Fabri analysis attains less than 1 % error in on-design entrainment but exhibits high sensitivity to the dividing streamline and closure, which limits predictive use beyond on-design. Overall, the results show that Fabri and compound mechanisms can coexist within the same device and operating map, each capturing distinct aspects of the physics and offering complementary modeling value. Nevertheless, compound choking emerges as the more general mechanism governing flow rate blockage, as evidenced by choked flows with a subsonic secondary stream."
2510.23384,Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach,"['cs.AI', 'cs.LG', 'doi', '10.5281/ZENODO.17389005']","['Pratik N. Kalamkar', 'A. G. Phakatkar']","Opinions are central to almost all human activities and are key influencers of our behaviors. In current times due to growth of social networking website and increase in number of e-commerce site huge amount of opinions are now available on web. Given a set of evaluative statements that contain opinions (or sentiments) about an Entity, opinion mining aims to extract attributes and components of the object that have been commented on in each statement and to determine whether the comments are positive, negative or neutral. While lot of research recently has been done in field of opinion mining and some of it dealing with ranking of entities based on review or opinion set, classifying opinions into finer granularity level and then ranking entities has never been done before. In this paper method for opinion mining from statements at a deeper level of granularity is proposed. This is done by using fuzzy logic reasoning, after which entities are ranked as per this information."
2510.23383,One-Timestep is Enough: Achieving High-performance ANN-to-SNN Conversion via Scale-and-Fire Neurons,['cs.NE'],"['Qiuyang Chen', 'Huiqi Yang', 'Qingyan Meng', 'Zhengyu Ma']","Spiking Neural Networks (SNNs) are gaining attention as energy-efficient alternatives to Artificial Neural Networks (ANNs), especially in resource-constrained settings. While ANN-to-SNN conversion (ANN2SNN) achieves high accuracy without end-to-end SNN training, existing methods rely on large time steps, leading to high inference latency and computational cost. In this paper, we propose a theoretical and practical framework for single-timestep ANN2SNN. We establish the Temporal-to-Spatial Equivalence Theory, proving that multi-timestep integrate-and-fire (IF) neurons can be equivalently replaced by single-timestep multi-threshold neurons (MTN). Based on this theory, we introduce the Scale-and-Fire Neuron (SFN), which enables effective single-timestep ($T=1$) spiking through adaptive scaling and firing. Furthermore, we develop the SFN-based Spiking Transformer (SFormer), a specialized instantiation of SFN within Transformer architectures, where spike patterns are aligned with attention distributions to mitigate the computational, energy, and hardware overhead of the multi-threshold design. Extensive experiments on image classification, object detection, and instance segmentation demonstrate that our method achieves state-of-the-art performance under single-timestep inference. Notably, we achieve 88.8% top-1 accuracy on ImageNet-1K at $T=1$, surpassing existing conversion methods."
2510.23382,An Efficient Remote Sensing Super Resolution Method Exploring Diffusion Priors and Multi-Modal Constraints for Crop Type Mapping,['cs.CV'],"['Songxi Yang', 'Tang Sui', 'Qunying Huang']","Super resolution offers a way to harness medium even lowresolution but historically valuable remote sensing image archives. Generative models, especially diffusion models, have recently been applied to remote sensing super resolution (RSSR), yet several challenges exist. First, diffusion models are effective but require expensive training from scratch resources and have slow inference speeds. Second, current methods have limited utilization of auxiliary information as real-world constraints to reconstruct scientifically realistic images. Finally, most current methods lack evaluation on downstream tasks. In this study, we present a efficient LSSR framework for RSSR, supported by a new multimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built on frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention with auxiliary knowledge (Digital Elevation Model, land cover, month) and Synthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier NDVI loss to balance spatial details and spectral fidelity. Extensive experiments demonstrate that LSSR significantly improves crop boundary delineation and recovery, achieving state-of-the-art performance with Peak Signal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB) and 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while maintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers effectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution, yielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1: 0.85). These results highlight the potential of RSSR to advance precision agriculture."
2510.23381,Unified Learning of the Profile Function in Discrete Keller-Segel Models,['math.NA'],"['Chi-An Chen', 'Chun Liu', 'Ming Zhong']","We propose a unified learning framework for identifying the profile function in discrete Keller-Segel equations, which are widely used mathematical models for understanding chemotaxis. Training data are obtained via either a rigorously developed particle method designed for stable simulation of high-dimensional Keller-Segel systems, or stochastic differential equations approximating the continuous Keller-Segel PDE. Our approach addresses key challenges, including data instability in dimensions higher than two and the accurate capture of singular behavior in the profile function. Additionally, we introduce an adaptive learning strategy to enhance performance. Extensive numerical experiments are presented to validate the effectiveness of our method."
2510.23380,Borel Complexity of the set of vectors normal for a fixed recurrence sequence,"['math.LO', 'math.NT']","['Hajime Kaneko', 'Bill Mance']","In this paper, we consider recurrence sequences $x_n=ξ_1 α_1^n+ξ_2 α_2^n$ ($n=0,1,\ldots$) with companion polynomial $P(X)$. For example, the sequence $x_n=ξ_1(4+\sqrt{2})^n+ξ_2(4-\sqrt{2})^n$ satisfies the recurrence $x_{n+2}-8x_{n+1}+14x_n=0$ and has companion polynomial $P(X)=X^2-8X+14=(X-4-\sqrt{2})(X-4+\sqrt{2})$. We call $(ξ_1,ξ_2)$ normal with respect to the recurrence relation determined by $P(X)$ when $(x_n)_{n\ge 0}$ is uniformly distributed modulo one.
  Determining the Borel complexity of the set of normal vectors for a fixed recurrence sequence is unresolved even for most geometric progressions. Under certain assumptions, we prove that the set of normal vectors is $\boldsymbolΠ_3^0$-complete. A special case is the new result that the sets of numbers normal in base $α$, i.e. $\{ξ\in \mathbb{R}\mid (ξα^n)_{n\geq 0}\mbox{ is u.d. modulo one.} \}$, are $\boldsymbolΠ_3^0$-complete for every real number $α$ with $|α|$ Pisot. We analyze the fractional parts of recurrence sequences in terms of finite words via certain numeration systems. One of the difficulties in proving the main result is that even when recurrence sequences are uniformly distributed modulo one, it is not known what the average frequencies of the digits in the corresponding digital expansions are or if they even must exist."
2510.23379,Symbolic Neural Generation with Applications to Lead Discovery in Drug Design,"['cs.LG', 'cs.AI', 'cs.NE', 'q-bio.BM']","['Ashwin Srinivasan', 'A Baskar', 'Tirtharaj Dash', 'Michael Bain', 'Sanjay Kumar Dey', 'Mainak Banerjee']","We investigate a relatively underexplored class of hybrid neurosymbolic models integrating symbolic learning with neural reasoning to construct data generators meeting formal correctness criteria. In \textit{Symbolic Neural Generators} (SNGs), symbolic learners examine logical specifications of feasible data from a small set of instances -- sometimes just one. Each specification in turn constrains the conditional information supplied to a neural-based generator, which rejects any instance violating the symbolic specification. Like other neurosymbolic approaches, SNG exploits the complementary strengths of symbolic and neural methods. The outcome of an SNG is a triple $(H, X, W)$, where $H$ is a symbolic description of feasible instances constructed from data, $X$ a set of generated new instances that satisfy the description, and $W$ an associated weight. We introduce a semantics for such systems, based on the construction of appropriate \textit{base} and \textit{fibre} partially-ordered sets combined into an overall partial order, and outline a probabilistic extension relevant to practical applications. In this extension, SNGs result from searching over a weighted partial ordering. We implement an SNG combining a restricted form of Inductive Logic Programming (ILP) with a large language model (LLM) and evaluate it on early-stage drug design. Our main interest is the description and the set of potential inhibitor molecules generated by the SNG. On benchmark problems -- where drug targets are well understood -- SNG performance is statistically comparable to state-of-the-art methods. On exploratory problems with poorly understood targets, generated molecules exhibit binding affinities on par with leading clinical candidates. Experts further find the symbolic specifications useful as preliminary filters, with several generated molecules identified as viable for synthesis and wet-lab testing."
2510.23378,Investigation of Resonances in the $Σ({1/2}^{-})$ System Based on the Chiral Quark Model,['hep-ph'],"['Yu Yao', 'Xuejie Liu', 'Xiaoyun Chen', 'Yuheng Wu', 'Jialun Ping', 'Yue Tan', 'Qi Huang']","In this work, we investigate the resonance structures in the $Σ(1/2^-)$ system from both three-quark and five-quark perspectives within the framework of the chiral quark model. An accurate few-body computational approach, the Gaussian Expansion Method, is employed to construct the orbital wave functions of multiquark states. To reduce the model dependence on parameters, we fit two sets of parameters to check the stability of the results. The calculations show that our results remain stable despite changes in the parameters. In the three-quark calculations, two $Σ(1/2^-)$ states are obtained with energies around 1.8~GeV, which are good candidates for the experimentally observed $Σ(1750)$ and $Σ(1900)$. In the five-quark configuration, several stable resonance states are identified, including $Σπ$, $N \bar{K}$, and $N \bar{K}^{*}$. These resonance states survive the channel-coupling calculations under the complex-scaling framework and manifest as stable structures. Our results support the existence of a two-pole structure for the $Σ(1/2^-)$ system, predominantly composed of $Σπ$ and $N \bar{K}$ configurations, analogous to the well-known $Λ(1380)$-$Λ(1405)$ ($Σπ$-$N \bar{K}$) system. On the other hand, although the energy of the $N \bar{K}^{*}$ configuration is close to that of $Σ(1750)$ and $Σ(1900)$, the obtained width is not consistent with the experimental values. This suggests that the $N \bar{K}^{*}$ state needs to mix with three-quark components to better explain the experimental $Σ(1750)$ and $Σ(1900)$ states. According to our decay width calculations, the predicted two resonance states are primarily composed of $Σπ$ and $N \bar{K}$, with their main decay channel being $Λπ$."
2510.23377,Primordial Non-Gaussianity from a String-Inspired Cosmology,['hep-th'],['M. Meo'],"The interplay between string theory and early-universe cosmology offers promising avenues to explore high-energy regimes where the standard single-field slow-roll model may no longer provide an accurate description. One intriguing scenario emerges from certain non-supersymmetric string models, where supersymmetry breaking induces a non-trivial vacuum energy, or more precisely an exponential potential for scalar fields, and primarily for the dilaton. This setup gives rise to the so-called ""climbing scalar"" phenomenon, whereby the scalar is forced to emerge from the initial singularity while climbing up the potential, if sufficiently steep. This phase precedes a turning point, and the subsequent descent can support inflation. The resulting pre-inflationary dynamics can leave imprints in cosmological observables. To begin with, it induces a low-frequency cut in the primordial power spectrum that resonates with the lack of power present in the first few CMB multipoles. The main theme of this work is to clarify its possible effect on non-Gaussianities."
2510.23376,Ground-state phase diagram of S = 1/2 Heisenberg model on 2D square-hexagon-octagon lattice,['cond-mat.str-el'],"['Yumeng Luo', 'Yuehong Li', 'Mengfan Jiang', 'Muwei Wu', 'Jian-Jian Yang', 'Dao-Xin Yao', 'Han-Qing Wu']","Using stochastic series expansion quantum Monte Carlo method and density matrix renormalization group, we study the ground-state phase diagram of $S=1/2$ Heisenberg model on 2D square-hexagon-octagon (SHO) lattice. In this model, we consider two kinds of nearest-neighbor interaction (intra-hexagon interaction $J_1$ and inter-hexagon $J_2$) and the selected third nearest-neighbor interaction $J_3$ along $x$ direction. From our calculations, there are five phases in the parameters regime $0<λ_1=J_2/J_1<4, 0<λ_2=J_3/J_1<4$, including a Néel antiferromagentic phase, a Haldane-like symmetry protected topological phase, a hexagon phase and two dimer phases. In the Haldane-like SPT phase, we characterized its topological nature using the degeneracy of ground-state energy under open boundary condition and the entanglement spectrum. To characterize the phase boundaries, we use spin stiffness and Binder cumulant to do the comprehensive finite-size scalings. From data collapse, the critical behaviors of all the nonmagnetic phases to the antiferromagnetic phase belong to the 3D $O(3)$ Heisenberg universality class. As a theoretical exploration, our work establishes a foundational framework for understanding 2D magnetism on the SHO lattice."
2510.23375,Validating Open Cluster Candidates with Photometric Bayesian Evidence,['astro-ph.GA'],"['Lu Li', 'Zhaozhou Li', 'Zhengyi Shao']","The thousands of open cluster (OC) candidates identified by the Gaia mission are significantly contaminated by false positives from field star fluctuations, posing a major validation challenge. Based on the Mixture Model for OCs (MiMO), we present a Bayesian framework for validating OC candidates in the color--magnitude diagram. The method compares the Bayesian evidence of two competing models: a single stellar population with field contamination versus a pure field population. Their ratio, the Bayes factor (BF), quantifies the statistical support for cluster existence. Tests on confirmed clusters and random fields show that a threshold of BF > 100 effectively distinguishes genuine clusters from chance field overdensities. This approach provides a robust, quantitative tool for OC validation and catalog refinement. The framework is extendable to multi-dimensional validation incorporating kinematics and is broadly applicable to other resolved stellar systems, including candidate moving groups, stellar streams, and dwarf satellites."
2510.23374,"The MiMO Catalog: Physical Parameters and Stellar Mass Functions of 1,232 Open Clusters from Gaia DR3","['astro-ph.GA', 'astro-ph.SR']","['Lu Li', 'Zhengyi Shao', 'Zhaozhou Li', 'Xiaoting Fu']","We present a homogeneous catalog of 1,232 open clusters with precisely determined ages, metallicities, distances, extinctions, and stellar mass function (MF) slopes, derived from Gaia DR3 data. The parameters are inferred using the Mixture Model for Open clusters (MiMO), a novel Bayesian framework for modeling clusters in the color-magnitude diagram. By explicitly accounting for field-star contamination as a model component, MiMO removes the conventional need for stringent membership preselection, allowing for a more complete inclusion of member stars and thereby enhancing both precision and robustness. Our results broadly agree with existing catalogs but offer improved precision. For each cluster, we provide the best-fit age, metallicity, distance, extinction, and MF slope, along with their full likelihood chains and photometric membership probabilities for individual stars. We further identify an ``MF Prime'' subsample of 163 clusters with high-quality data, for which the MF estimates are considered most reliable. The catalog and an open-source implementation of MiMO are made publicly available to the community."
2510.23373,Expected Length of the Euclidean Minimum Spanning Tree and 1-norms of Chromatic Persistence Diagrams in the Plane,"['math.PR', 'cs.CG', 'math.CO']","['Ondřej Draganov', 'Herbert Edelsbrunner', 'Sophie Rosenmeier', 'Morteza Saghafian']","Let $c$ be the constant such that the expected length of the Euclidean minimum spanning tree of $n$ random points in the unit square is $c \sqrt{n}$ in the limit, when $n$ goes to infinity. We improve the prior best lower bound of $0.6008 \leq c$ by Avram and Bertsimas to $0.6289 \leq c$. The proof is a by-product of studying the persistent homology of randomly $2$-colored point sets. Specifically, we consider the filtration induced by the inclusions of the two mono-chromatic sublevel sets of the Euclidean distance function into the bi-chromatic sublevel set of that function. Assigning colors randomly, and with equal probability, we show that the expected $1$-norm of each chromatic persistence diagram is a constant times $\sqrt{n}$ in the limit, and we determine the constant in terms of $c$ and another constant, $c_L$, which arises for a novel type of Euclidean minimum spanning tree of $2$-colored point sets."
2510.23372,Constraints on effective field theories via quadruple-differential angular decay rates from $t$-channel single-top-quark production at $\sqrt{s}=13$ TeV with the ATLAS detector,['hep-ex'],['ATLAS Collaboration'],"Events with $t$-channel single top quarks are used to probe effective field theory operators in $\sqrt{s}=13$ TeV proton-proton collision data corresponding to 140 fb$^{-1}$ recorded by the ATLAS detector at the Large Hadron Collider. An analysis method leveraging Fourier techniques applied to quadruple-differential decay rates based on observables containing angular information about the decays of the top quarks is used to achieve high sensitivity. The relevant effective field theory operators are those sensitive to top-quark decay and $t$-channel production vertices. Their Wilson coefficients are tightly constrained, with results compatible with Standard Model predictions."
2510.23371,Towards a Generalizable AI for Materials Discovery: Validation through Immersion Coolant Screening,"['cs.LG', 'cs.CE']","['Hyunseung Kim', 'Dae-Woong Jeong', 'Changyoung Park', 'Won-Ji Lee', 'Ha-Eun Lee', 'Ji-Hye Lee', 'Rodrigo Hormazabal', 'Sung Moon Ko', 'Sumin Lee', 'Soorin Yim', 'Chanhui Lee', 'Sehui Han', 'Sang-Ho Cha', 'Woohyung Lim']","Artificial intelligence (AI) has emerged as a powerful accelerator of materials discovery, yet most existing models remain problem-specific, requiring additional data collection and retraining for each new property. Here we introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a generalizable AI framework that jointly learns 34 physicochemical properties spanning thermal, electrical, mechanical, and optical domains. By aligning these properties within a shared geometric space, GATE captures cross-property correlations that reduce disjoint-property bias -- a key factor causing false negatives in multi-criteria screening. To demonstrate its generalizability, GATE -- without any problem-specific reconfiguration -- was directly applied to the discovery of immersion cooling fluids for data centers, a stringent real-world challenge defined by the Open Compute Project (OCP). Screening billions of candidates, GATE identified 92,861 molecules as promising for practical deployment. Four were experimentally or literarily validated, showing strong agreement with wet-lab measurements and performance comparable to or exceeding a commercial coolant. These results establish GATE as a ready-to-use, generalizable AI platform readily applicable across diverse materials discovery tasks."
2510.23370,"Production of Hyperons, Charmed Baryons, and Hadronic Molecule Candidates in Neutrino-Proton Reaction",['hep-ph'],"['Kai-sa Qiao', 'Bing-song Zou']","We investigate the production of hyperons, charmed baryons, and potential hadronic molecular states in neutrino-proton ($\barν_μp$) reaction, a process characterized by a particularly clean final state. Employing effective Lagrangians, chiral perturbation theory, and a hadronic molecular model, we perform theoretical calculations for several relevant channels, including those leading to the formation of the hadronic molecular candidate $(\bar{D}N)$. Our results indicate that future neutrino facilities could serve as a complementary platform for exploring exotic baryonic states and provide valuable insights into the dynamics of strong interactions in the strange and charm sectors."
2510.23369,On the equivalence between the existence of $n$-kernels and $n$-cokernels,"['math.CT', 'math.RA', 'math.RT']","['Vitor Gulisz', 'Wolfgang Rump']","We give an elementary proof of the statement that if an idempotent complete additive category has weak kernels and weak cokernels, then it has $n$-kernels if and only if it has $n$-cokernels, where $n$ is a nonnegative integer. As a consequence, elementary proofs of two results concerning the equality between the global dimensions of certain right and left module categories are obtained."
2510.23368,PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking,['cs.CV'],"['Yifan Jiao', 'Xinran Liu', 'Xiaoqiong Liu', 'Xiaohui Yuan', 'Heng Fan', 'Libo Zhang']","Planar tracking has drawn increasing interest owing to its key roles in robotics and augmented reality. Despite recent great advancement, further development of planar tracking, particularly in the deep learning era, is largely limited compared to generic tracking due to the lack of large-scale platforms. To mitigate this, we propose PlanarTrack, a large-scale high-quality and challenging benchmark for planar tracking. Specifically, PlanarTrack consists of 1,150 sequences with over 733K frames, including 1,000 short-term and 150 new long-term videos, which enables comprehensive evaluation of short- and long-term tracking performance. All videos in PlanarTrack are recorded in unconstrained conditions from the wild, which makes PlanarTrack challenging but more realistic for real-world applications. To ensure high-quality annotations, each video frame is manually annotated by four corner points with multi-round meticulous inspection and refinement. To enhance target diversity of PlanarTrack, we only capture a unique target in one sequence, which is different from existing benchmarks. To our best knowledge, PlanarTrack is by far the largest and most diverse and challenging dataset dedicated to planar tracking. To understand performance of existing methods on PlanarTrack and to provide a comparison for future research, we evaluate 10 representative planar trackers with extensive comparison and in-depth analysis. Our evaluation reveals that, unsurprisingly, the top planar trackers heavily degrade on the challenging PlanarTrack, which indicates more efforts are required for improving planar tracking. Our data and results will be released at https://github.com/HengLan/PlanarTrack"
2510.23367,From Rare Events to a Population: Discovering Overlooked Extragalactic Magnetar Giant Flare Candidates in Archival Fermi Gamma-ray Burst Monitor Data,['astro-ph.HE'],"['Aaron C. Trigg', 'Eric Burns', 'Michela Negro', 'Suman Bala', 'P. N. Bhat', 'William H. Cleveland', 'Dmitry D. Frederiks', 'Adam Goldstein', 'Boyan A. Hristov', 'Daniel Kocevski', 'Niccolò Di Lalla', 'Stephen Lesage', 'Bagrat Mailyan', 'Eliza Neights', 'Nicola Omodei', 'Oliver J. Roberts', 'Lorenzo Scotton', 'Dmitry S. Svinkin', 'Joshua Wood']","Magnetar giant flares (MGFs) are rare, extremely bright bursts of gamma-rays from highly magnetized neutron stars. These events are challenging to identify because, at extragalactic distances, they can appear similar to other astrophysical phenomena. Only a handful have been confidently identified to date, limiting our understanding of their origin and physical properties. This study focuses on expanding the sample of known events and enabling a more detailed characterization of their observational features and intrinsic properties, while introducing significant improvements in the methods used to identify and analyze them. When applied to archival data from the Gamma-ray Burst Monitor (GBM) on the \Fermi Gamma-ray Space Telescope, this approach added four previously unidentified events the known sample, expanding the total to 13 MGFs. This demonstrates both the effectiveness of the method and the likelihood that additional MGFs remain hidden in existing gamma-ray burst catalogs. We utilize this expanded sample to gain a deeper understanding of the broader population of MGFs. We develop a statistical modeling framework that combines previously considered data with modern observations from Fermi/GBM. The model accounts for instrumental sensitivity and the expected diversity in event characteristics. We infer a volumetric rate of events above $1.2\times10^{44}\,\rm{erg}$ of $R_{MGF}=5.5^{+4.5}_{-2.7}\times10^5\rm{Gpc^{-3}yr^{-1}}$. The results show that individual magnetars must produce multiple flares throughout their lifetimes, reinforcing the idea that these are recurring phenomena rather than singular explosive events. Expanding the sample of known MGFs improves our understanding of magnetars and their role in other astrophysical phenomena, including possible links to fast radio bursts, gravitational waves, and the creation of heavy elements in extreme astrophysical environments."
2510.23366,Generalized boundary rigidity and minimal surface transform,"['math.AP', 'math-ph', 'math.DG']","['Leonard Busch', 'Tony Liimatainen', 'Mikko Salo', 'Leo Tzou']","We study a generalized boundary rigidity problem, which investigates whether the areas of embedded minimal surfaces can uniquely determine a Riemannian manifold with boundary. We prove that for a conformal perturbation of an analytic metric in dimension $n+1$ ($n \geq 2$), the metric is determined by these volumes under an ampleness condition. Furthermore, we establish Hölder stability for this determination. This result extends earlier works in dimension $2+1$. Instead of relying on reductions to Calderón type problems and complex geometrical optics solutions, we study the linearized forward operator that gives rise to the minimal surface transform, a generalization of the X-ray/Radon transform. We demonstrate that this transform fits into the framework of double fibration transforms and satisfies the Bolker condition in the sense of Guillemin. Under certain assumptions, including a foliation condition, we prove invertibility of this transform on an analytic manifold as well as recovery of the analytic wave front set. The methods developed in this paper offer new tools for addressing the generalized boundary rigidity problem and expand the scope of applications of double fibration transforms. We anticipate that these techniques will also be applicable to other geometric inverse problems. Beyond mathematics, our results have implications for the AdS/CFT correspondence in physics."
2510.23365,Classification of horospherical invariant measures in higher rank,"['math.DS', 'math.GR', 'math.GT']","['Inhyeok Choi', 'Dongryul M. Kim']","Let $G$ be a product of rank-one simple real algebraic groups and let $Γ< G$ be a Zariski dense Anosov subgroup, or relatively Anosov subgroup. In this paper, we prove a complete classification of invariant Radon measures for the maximal horospherical action on $Γ\backslash G$. In particular, when $Γ$ is Anosov, this solves the open problems proposed by Landesberg--Lee--Lindenstrauss--Oh for $\operatorname{rank} G \le 3$, and by Oh in general.
  More generally, we consider the horospherical foliation of a product of $\operatorname{CAT}(-1)$ spaces, and present a classification of Radon measures supported on a recurrent subfoliation that are invariant under the action of transverse subgroups."
2510.23364,ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping,"['cs.LG', 'cs.AI']","['Hyeongkyun Kim', 'Orestis Oikonomou']","Flood susceptibility mapping (FSM) is vital for disaster prevention but remains challenging in data-scarce regions where hydrodynamic models require dense geophysical inputs. This work introduces ZeroFlood, a geospatial foundation model framework for data-efficient FSM. The approach fine-tunes Geospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning, enabling flood prediction from basic Earth observation data such as Sentinel-1 or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich regions, ZeroFlood bridges data availability gaps through cross-modal representation learning. Experiments with TerraMind and Prithvi GFMs show that TiM enhances model robustness, with the TerraMind-Large configuration achieving an F1 score of 67.21. The results demonstrate the feasibility of foundation-model-based FSM as a scalable and data-efficient solution for flood risk management."
2510.23363,Interpretable Tile-Based Classification of Paclitaxel Exposure,['cs.CV'],"['Sean Fletcher', 'Gabby Scott', 'Douglas Currie', 'Xin Zhang', 'Yuqi Song', 'Bruce MacLeod']","Medical image analysis is central to drug discovery and preclinical evaluation, where scalable, objective readouts can accelerate decision-making. We address classification of paclitaxel (Taxol) exposure from phase-contrast microscopy of C6 glioma cells -- a task with subtle dose differences that challenges full-image models. We propose a simple tiling-and-aggregation pipeline that operates on local patches and combines tile outputs into an image label, achieving state-of-the-art accuracy on the benchmark dataset and improving over the published baseline by around 20 percentage points, with trends confirmed by cross-validation. To understand why tiling is effective, we further apply Grad-CAM and Score-CAM and attention analyses, which enhance model interpretability and point toward robustness-oriented directions for future medical image research. Code is released to facilitate reproduction and extension."
2510.23362,Robust Non-negative Proximal Gradient Algorithm for Inverse Problems,"['cs.LG', 'stat.ML']","['Hanzhang Wang', 'Zonglin Liu', 'Jingyi Xu', 'Chenyang Wang', 'Zhiwei Zhong', 'Qiangqiang Shen']","Proximal gradient algorithms (PGA), while foundational for inverse problems like image reconstruction, often yield unstable convergence and suboptimal solutions by violating the critical non-negativity constraint. We identify the gradient descent step as the root cause of this issue, which introduces negative values and induces high sensitivity to hyperparameters. To overcome these limitations, we propose a novel multiplicative update proximal gradient algorithm (SSO-PGA) with convergence guarantees, which is designed for robustness in non-negative inverse problems. Our key innovation lies in superseding the gradient descent step with a learnable sigmoid-based operator, which inherently enforces non-negativity and boundedness by transforming traditional subtractive updates into multiplicative ones. This design, augmented by a sliding parameter for enhanced stability and convergence, not only improves robustness but also boosts expressive capacity and noise immunity. We further formulate a degradation model for multi-modal restoration and derive its SSO-PGA-based optimization algorithm, which is then unfolded into a deep network to marry the interpretability of optimization with the power of deep learning. Extensive numerical and real-world experiments demonstrate that our method significantly surpasses traditional PGA and other state-of-the-art algorithms, ensuring superior performance and stability."
2510.23361,The influence of a stably stratified layer on the hydromagnetic waves in the Earth's core and their electromagnetic torques,"['astro-ph.EP', 'physics.flu-dyn', 'physics.geo-ph']","['Fleur Seuren', 'Santiago A. Triana', 'Jérémy Rekier', 'Véronique Dehant', 'Tim Van Hoolst']","Evidence from seismic studies, mineral physics, thermal evolution models and geomagnetic observations is inconclusive about the presence of a stably stratified layer at the top of the Earth's fluid outer core. Such a convectively stable layer could have a strong influence on the internal fluid waves propagating underneath the core-mantle boundary (CMB) that are used to probe the outermost region of the core through the wave interaction with the geomagnetic field and the rotation of the mantle. Here, we numerically investigate the effect of a top stable layer on the outer core fluid waves by calculating the eigenmodes in a neutrally stratified sphere permeated by a magnetic field with and without a top stable layer. We use a numerical model, assuming a flow with an m-fold azimuthal symmetry, that allows for radial motions across the lower boundary of the stable layer and angular momentum exchanges across the CMB through viscous and electromagnetic coupling. On interannual time-scales, we find torsional Alfvén waves that are only marginally affected by weak to moderate stratification strength in the outer layer. At decadal time-scales similarly weak stable layers promote the appearance of waves that propagate primarily within the stable layer itself and resemble Magneto-Archimedes-Coriolis (MAC) waves, even though they interact with the adiabatic fluid core below. These waves can exert viscous and electromagnetic torques on the mantle that are several orders of magnitude larger than those in the neutrally stratified case."
2510.23360,Effect of intratumor heterogeneity in managing the go-or-grow dichotomy of cancer cells: a game theory modeling to understand metastasis,['q-bio.PE'],"['André Rocha', 'Claudia Manini', 'José I López', 'Annick Laruelle']","We study the effect of intratumor heterogeneity in the likelihood of cancer cells moving from a primary tumor to other sites in the human body, generating a metastatic process. We model different scenarios of competition between tumor cells using a static evolutionary game in which cells compete for nutrients and oxygen and might choose to stay and proliferate in the primary tumor or opt to a motility strategy in order to find resources in a metastatic site. The theoretical results found in the evolutionarily equilibrium in the mathematical model are in line with the empirical results observed in oncology, namely, the coexistence of both primary and metastatic tumors and the conditions that favor a metastatic process. Particularly, the model finds mathematical support for what is empirically observed in punctuated and branching cancers for the specific case of clear cell renal cell carcinomas: motility of cells is larger in punctuated cancers if the proportion of BAP1 mutations remain below a given cell proportion threshold."
2510.23359,T-ESKF: Transformed Error-State Kalman Filter for Consistent Visual-Inertial Navigation,"['cs.RO', 'doi', '10.1109/LRA.2024.3524905']","['Chungeng Tian', 'Ning Hao', 'Fenghua He']","This paper presents a novel approach to address the inconsistency problem caused by observability mismatch in visual-inertial navigation systems (VINS). The key idea involves applying a linear time-varying transformation to the error-state within the Error-State Kalman Filter (ESKF). This transformation ensures that \textrr{the unobservable subspace of the transformed error-state system} becomes independent of the state, thereby preserving the correct observability of the transformed system against variations in linearization points. We introduce the Transformed ESKF (T-ESKF), a consistent VINS estimator that performs state estimation using the transformed error-state system. Furthermore, we develop an efficient propagation technique to accelerate the covariance propagation based on the transformation relationship between the transition and accumulated matrices of T-ESKF and ESKF. We validate the proposed method through extensive simulations and experiments, demonstrating better (or competitive at least) performance compared to state-of-the-art methods. The code is available at github.com/HITCSC/T-ESKF."
2510.23358,How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes,['cs.CL'],"['Sheri Osborn', 'Rohit Valecha', 'H. Raghav Rao', 'Dan Sass', 'Anthony Rios']","Artificial intelligence is reshaping labor markets, yet we lack tools to systematically forecast its effects on employment. This paper introduces a benchmark for evaluating how well large language models (LLMs) can anticipate changes in job demand, especially in occupations affected by AI. Existing research has shown that LLMs can extract sentiment, summarize economic reports, and emulate forecaster behavior, but little work has assessed their use for forward-looking labor prediction. Our benchmark combines two complementary datasets: a high-frequency index of sector-level job postings in the United States, and a global dataset of projected occupational changes due to AI adoption. We format these data into forecasting tasks with clear temporal splits, minimizing the risk of information leakage. We then evaluate LLMs using multiple prompting strategies, comparing task-scaffolded, persona-driven, and hybrid approaches across model families. We assess both quantitative accuracy and qualitative consistency over time. Results show that structured task prompts consistently improve forecast stability, while persona prompts offer advantages on short-term trends. However, performance varies significantly across sectors and horizons, highlighting the need for domain-aware prompting and rigorous evaluation protocols. By releasing our benchmark, we aim to support future research on labor forecasting, prompt design, and LLM-based economic reasoning. This work contributes to a growing body of research on how LLMs interact with real-world economic data, and provides a reproducible testbed for studying the limits and opportunities of AI as a forecasting tool in the context of labor markets."
2510.23357,Large language model-based task planning for service robots: A review,['cs.RO'],"['Shaohan Bian', 'Ying Zhang', 'Guohui Tian', 'Zhiqiang Miao', 'Edmond Q. Wu', 'Simon X. Yang', 'Changchun Hua']","With the rapid advancement of large language models (LLMs) and robotics, service robots are increasingly becoming an integral part of daily life, offering a wide range of services in complex environments. To deliver these services intelligently and efficiently, robust and accurate task planning capabilities are essential. This paper presents a comprehensive overview of the integration of LLMs into service robotics, with a particular focus on their role in enhancing robotic task planning. First, the development and foundational techniques of LLMs, including pre-training, fine-tuning, retrieval-augmented generation (RAG), and prompt engineering, are reviewed. We then explore the application of LLMs as the cognitive core-`brain'-of service robots, discussing how LLMs contribute to improved autonomy and decision-making. Furthermore, recent advancements in LLM-driven task planning across various input modalities are analyzed, including text, visual, audio, and multimodal inputs. Finally, we summarize key challenges and limitations in current research and propose future directions to advance the task planning capabilities of service robots in complex, unstructured domestic environments. This review aims to serve as a valuable reference for researchers and practitioners in the fields of artificial intelligence and robotics."
2510.23356,IoT-Driven Smart Management in Broiler Farming: Simulation of Remote Sensing and Control Systems,"['eess.SY', 'cs.ET']","['Sandra Coello Suarez', 'V. Sanchez Padilla', 'Ronald Ponguillo-Intriago', 'Albert Espinal']","Parameter monitoring and control systems are crucial in the industry as they enable automation processes that improve productivity and resource optimization. These improvements also help to manage environmental factors and the complex interactions between multiple inputs and outputs required for production management. This paper proposes an automation system for broiler management based on a simulation scenario that involves sensor networks and embedded systems. The aim is to create a transmission network for monitoring and controlling broiler temperature and feeding using the Internet of Things (IoT), complemented by a dashboard and a cloud-based service database to track improvements in broiler management. We look forward this work will serve as a guide for stakeholders and entrepreneurs in the animal production industry, fostering sustainable development through simple and cost-effective automation solutions. The goal is for them to scale and integrate these recommendations into their existing operations, leading to more efficient decision-making at the management level."
2510.23355,Uplink SCMA-empowered Uncoordinated Random Access for Future mMTC,['eess.SP'],"['Pengyu Gao', 'Qu Luo', 'Jing Zhu', 'Gaojie Chen', 'Pei Xiao', 'Chuan Heng Foh']","In this paper, a novel uncoordinated random access (URA) protocol is presented to address the pressing demand for massive connectivity with low access latency in future massive machine type communication (mMTC) scenarios. The proposed URA scheme integrates the classical slotted ALOHA (S-ALOHA) protocol with sparse code multiple access (SCMA) technique, referred to as SCMA-empowered URA. Specifically, active users randomly choose an SCMA codebook to access the communication network in an arbitrary time slot whenever they want without scheduling. However, due to the lack of central coordination in the proposed URA scheme, SCMA codebook collisions become inevitable, making decoding challenging and leading to increased access failures. To cope with the decoding issue, an interference-canceling (IC) first decoding strategy is proposed at the access point (AP), which can partially tackles collision problems, contributing to a higher system throughput. Taking the proposed IC-first decoding strategy into account, a closed-form theoretical expression of the throughput is derived. Moreover, to alleviate the throughput degradation under the congested user traffic, a user barring mechanism is introduced to manage the traffic load. Firstly, a closed-form expression of idle codebook probability is developed to help indicate the system state, i.e., congested or not. Then, in addition to the estimated real-time load, the AP adaptively adjusts the access probability and redistributes the actual access load. Finally, simulation results demonstrate that the proposed SCMA-empowered URA scheme enjoys higher maximum throughput, compared to the conventional orthogonal multiple access (OMA) based URA scheme. Moreover, the accuracy of the presented theoretical analysis and the effectiveness of the user barring mechanism are verified."
2510.23354,Quantum versus Classical Descriptions of Spontaneous Emission in Nanophotonic Cavities,['physics.optics'],"['Jian-Hua Liang', 'Yue You', 'Xi-Hua Guan', 'Xiao-Jing Du', 'Jun He', 'Zhong-Jian Yang']","Here, we demonstrate that quantum and classical descriptions generally yield different results for the spontaneous emission in nanophotonic cavities. Starting from the quantized single-mode field in a general context of dispersive and lossy cavities, we derive the expression for emission rate enhancement as well as key relevant parameters such as mode volume and quality factor. For general nanophotonic cavities, this ratio of the quantum to the classical description is typically below unity and varies with the material dispersion properties, scattering-to-absorption ratio and morphology of the cavity. Notably, the two descriptions converge for lossless, non-dispersive dielectric cavities and for noble-metal plasmonic cavities with sufficiently low scattering losses."
2510.23353,Multi-strange and charmed hadrons: A novel probe for the QCD equation of state at high baryon densities,"['nucl-th', 'hep-ph']","['Jan Steinheimer', 'Tom Reichert', 'Marcus Bleicher']","Nuclear experiments near and below the threshold of hyperon production have shown that the production of Kaons is a sensitive probe for the dense QCD equation of state. At beam energies up to 1.5AGeV, strangeness production can probe the equation of state for densities up to approximately twice nuclear saturation. In this paper we will discuss the possibilities of extending this range in density by the study of multi-strange baryons as well as charmed hadrons in the SIS100 beam energy range up to 10AGeV. Here, densities up to five times nuclear saturation can be reached and the production of multi-strange and charmed hadrons shows a strong sensitivity to the equation of state. On the other hand a precise prediction of the effect of the equation of state will require knowledge of the fundamental production cross section near the elementary production threshold in p+p collisions which is yet not measured for the hadrons discussed."
2510.23352,Flexibility aggregation via set projection for distribution grids with multiple interconnections,"['math.OC', 'eess.SY']","['Maísa Beraldo Bandeira', 'Alexander Engelmann', 'Timm Faulwasser']","With the increasing number of flexible energy devices in distribution grids, coordination between Transmission System Operators (TSOs) and Distribution System Operators (DSOs) becomes critical for optimal system operation. One form of coordination is to solve the overall system operation problem in a hierarchical way, computing Feasible Operational Regions (FORs) for the interconnection between TSO/DSO. Most methods for computing FORs rely on the assumption of only one interconnection point between TSO and DSOs, which is often violated in practice. In this work, we propose a method for computing FORs in distribution grids with multiple interconnection points to the transmission grid. We test our method in a grid with two interconnecting points and analyze the properties of the resulting high-dimensional FOR from a power systems perspective."
2510.23351,Pseudodifferential calculus in Schwinger--DeWitt formalism: UV and IR parts,['hep-th'],"['A. O. Barvinsky', 'A. E. Kalugin', 'W. Wachowski']",We consider expansions for the kernels of operator functions of second-order minimal operators on a curved background. We show that the terms of these expansions originate in the ultraviolet or infrared regions. We propose a systematic approach to obtaining ultraviolet terms using term-by-term integration of the DeWitt expansion of the heat kernel. We discuss two methods for regularizing infrared divergences arising at intermediate computational steps -- using analytic continuation and introducing a mass term -- and the relationship between them.
2510.23350,Validating Formal Specifications with LLM-generated Test Cases,['cs.SE'],"['Alcino Cunha', 'Nuno Macedo']","Validation is a central activity when developing formal specifications. Similarly to coding, a possible validation technique is to define upfront test cases or scenarios that a future specification should satisfy or not. Unfortunately, specifying such test cases is burdensome and error prone, which could cause users to skip this validation task. This paper reports the results of an empirical evaluation of using pre-trained large language models (LLMs) to automate the generation of test cases from natural language requirements. In particular, we focus on test cases for structural requirements of simple domain models formalized in the Alloy specification language. Our evaluation focuses on the state-of-art GPT-5 model, but results from other closed- and open-source LLMs are also reported. The results show that, in this context, GPT-5 is already quite effective at generating positive (and negative) test cases that are syntactically correct and that satisfy (or not) the given requirement, and that can detect many wrong specifications written by humans."
2510.23349,The Mathematisation of the World: Uncovering the Socio-Economic Tensions for Ethics in Mathematics Education,['math.HO'],['Dennis Müller'],"The mathematisation of the socio-economic sphere, where mathematics actively constructs social reality, presents a challenge for studies on ethics in mathematics and its education. While existing scholarship on ethics in mathematics offers insights, it often remains philosophically driven and disconnected from other relevant disciplines. This paper addresses this gap by asking how debates on ethics in mathematics and its education can be connected with economic sociology, and what socio-economic tensions become visible through this connection. Drawing from concepts such as imagined futures, varieties of capitalism, and variegated capitalism, we synthesise a new perspective. This analysis reveals six interconnected tensions: a socio-economic valuation gap regarding ethics education; the multifaceted implementation of mathematics across different capitalist systems; its material opaqueness; a growing gap between economic power and social unaccountability; the enclosure of imagination limiting sustainable futures; and the erosion of multilateralism, which challenges critical pedagogy. The paper's contribution is a first step towards a structural socio-economic framework that links the limited literature on ethics in mathematics with these broader sociological perspectives."
2510.23348,Generating Sizable Real and Imaginary $τ$ Electric Dipole Moment,['hep-ph'],"['Zhong-Lv Huang', 'Xin-Yu Du', 'Xiao-Gang He', 'Chia-Wei Liu', 'Zi-Yue Zou']","The CP-violating electric dipole moment~(EDM) of a fermion provides a powerful probe of new physics beyond the Standard Model~(SM). Among the charged leptons, the $τ$ EDM remains the least constrained. When the photon has timelike momentum, the EDM develops an imaginary part. It imposes stronger constraints on new physics~(NP) than the real part. Although the current experimental bounds are several orders of magnitude above the Standard Model prediction, new physics can generate significantly larger values. Our analysis shows that an axion-like coupling of the $τ$ lepton in the two-Higgs-doublet model can induce sizable real and imaginary components of the EDM, despite the stringent constraints imposed by current axion-like particle experiments. The predicted EDM values may approach the present experimental sensitivities, making them accessible to future measurements at Belle II and the Super Tau-Charm Facility."
2510.23347,Macroeconomic Forecasting for the G7 countries under Uncertainty Shocks,"['econ.EM', 'cs.LG', 'stat.AP']","['Shovon Sengupta', 'Sunny Kumar Singh', 'Tanujit Chakraborty']","Accurate macroeconomic forecasting has become harder amid geopolitical disruptions, policy reversals, and volatile financial markets. Conventional vector autoregressions (VARs) overfit in high dimensional settings, while threshold VARs struggle with time varying interdependencies and complex parameter structures. We address these limitations by extending the Sims Zha Bayesian VAR with exogenous variables (SZBVARx) to incorporate domain-informed shrinkage and four newspaper based uncertainty shocks such as economic policy uncertainty, geopolitical risk, US equity market volatility, and US monetary policy uncertainty. The framework improves structural interpretability, mitigates dimensionality, and imposes empirically guided regularization. Using G7 data, we study spillovers from uncertainty shocks to five core variables (unemployment, real broad effective exchange rates, short term rates, oil prices, and CPI inflation), combining wavelet coherence (time frequency dynamics) with nonlinear local projections (state dependent impulse responses). Out-of-sample results at 12 and 24 month horizons show that SZBVARx outperforms 14 benchmarks, including classical VARs and leading machine learning models, as confirmed by Murphy difference diagrams, multivariate Diebold Mariano tests, and Giacomini White predictability tests. Credible Bayesian prediction intervals deliver robust uncertainty quantification for scenario analysis and risk management. The proposed SZBVARx offers G7 policymakers a transparent, well calibrated tool for modern macroeconomic forecasting under pervasive uncertainty."
2510.23346,Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving,['cs.LG'],"['Xinyu Wang', 'Jonas M. Kübler', 'Kailash Budhathoki', 'Yida Wang', 'Matthäus Kleindessner']","When serving a single base LLM with several different LoRA adapters simultaneously, the adapters cannot simply be merged with the base model's weights as the adapter swapping would create overhead and requests using different adapters could not be batched. Rather, the LoRA computations have to be separated from the base LLM computations, and in a multi-device setup the LoRA adapters can be sharded in a way that is well aligned with the base model's tensor parallel execution, as proposed in S-LoRA. However, the S-LoRA sharding strategy encounters some communication overhead, which may be small in theory, but can be large in practice. In this paper, we propose to constrain certain LoRA factors to be block-diagonal, which allows for an alternative way of sharding LoRA adapters that does not require any additional communication for the LoRA computations. We demonstrate in extensive experiments that our block-diagonal LoRA approach is similarly parameter efficient as standard LoRA (i.e., for a similar number of parameters it achieves similar downstream performance) and that it leads to significant end-to-end speed-up over S-LoRA. For example, when serving on eight A100 GPUs, we observe up to 1.79x (1.23x) end-to-end speed-up with 0.87x (1.74x) the number of adapter parameters for Llama-3.1-70B, and up to 1.63x (1.3x) end-to-end speed-up with 0.86x (1.73x) the number of adapter parameters for Llama-3.1-8B."
2510.23345,First detection of ultra-high energy emission from gamma-ray binary LS I +61 303,['astro-ph.HE'],['LHAASO Collaboration'],"We report the first detection of gamma-ray emission up to ultra-high-energy (UHE; $>$100 TeV) emission from the prototypical gamma-ray binary system LS I +61 303 using data from the Large High Altitude Air Shower Observatory (LHAASO). It is detected with significances of 9.2$σ$ in WCDA (1.4--30.5 TeV) and 6.2$σ$ in KM2A (25--267 TeV); in KM2A alone we identify 16 photon-like events above 100 TeV against an estimated 5.1 background events, corresponding to a 3.8$σ$ detection. These results provide compelling evidence of extreme particle acceleration in LS I +61 303. Furthermore, we observe orbital modulation at 4.0$σ$ confidence between 25 and 100 TeV, and a hint of energy-dependent orbital modulation. These features can be understood in a composite scenario in which leptonic and hadronic processes jointly contribute."
2510.23344,"Elastic modeling and total energy calculations of the structural characteristics of ""free-standing"",periodic, pseudomorphic GaN/AlN superlattices","['cond-mat.mtrl-sci', 'physics.comp-ph']","['Th. Karakostas', 'Ph. Komninou', 'V. Pontikis']","The strain states of the components of pseudomorphic superlattices can be accurately modeled analytically through the application of linear elasticity. In this particular case of GaN/AlN 'free-standing' superlattices, the predictions derived from elastic modeling have been compared with total energy calculations of several systems made of components with varying thicknesses. We demonstrate that the elastic predictions for the lattice constants of the components align with the values obtained from their total energy counterparts, within the limits of computational errors and uncertainties. Furthermore, a phenomenological analysis of the elastic energy stored in the superlattices facilitates the evaluation of the excess energies associated with the interfaces present in these systems. The results mentioned above are briefly contrasted with findings reported in previous literature."
2510.23343,"The Cosmic Baryon Cycle in IllustrisTNG: flows of mass, energy, and metals",['astro-ph.GA'],"['Yossi Oren', 'Viraj Pandya', 'Rachel S. Somerville', 'Shy Genel', 'Osase Omoruyi', 'Amiel Sternberg']","We measure and analyze the inflows and outflows of mass, energy, and metals through the interstellar medium (ISM) and circumgalactic medium (CGM) of galaxies in the IllustrisTNG100 simulations. We identify the dominant feedback mechanism in bins of halo virial mass and redshift by computing the integrated energy input from SNe and the ``kinetic'' and ``thermal'' mode of AGN feedback. We measure all quantities in a shell at the virial radius (``halo scale'') and one chosen to be approximately at the interface of the CGM and the interstellar medium (ISM; ``ISM scale''). We find that galaxies have strong net positive inflows on halo scales, and weaker but still net positive inflows on ISM scales, at $z\gtrsim 2$. At later times, partially due to the onset of kinetic AGN feedback in massive halos, inflows and outflows nearly balance one another, leading to the familiar effects of the slow-down of galaxy growth and the onset of quenching. Halos dominated by SN feedback show only weak evidence of preventative feedback on halo scales, and we see excess ISM scale accretion indicative of rapid gas recycling. Wind mass loadings decrease with increasing halo mass, and with increasing redshift, while energy loadings are nearly independent of both mass and redshift. The detailed catalogs of these mass, metal, and energy inflow and outflow rates on galaxy and halo scales can be used to guide empirical and semi-analytic models, and provide deeper insight into how galaxy growth and quenching is regulated in the IllustrisTNG simulations."
2510.23342,Reciprocity Deficits: Observing AI in the street with everyday publics,['cs.HC'],"['Alex S. Taylor', 'Noortje Marres', 'Mercedes Bunz', 'Thao Phan', 'Maya Indira Ganesh', 'Dominique Barron', 'Yasmine Boudiaf', 'Rachel Coldicutt', 'Iain Emsley', 'Beatrice Gobbo', 'Louise Hickman', 'Bettina Nissen', 'Mukul Patel', 'Luis Soares']","The street has emerged as a primary site where everyday publics are confronted with AI as an infrastructural phenomenon, as machine learning-based systems are now commonly deployed in this setting in the form of automated cars, facial recognition, smart billboards and the like. While these deployments of AI in the street have attracted significant media attention and public controversy in recent years, the presence of AI in the street often remains inscrutable, and many everyday publics are unaware of it. In this paper, we explore the challenges and possibilities of everyday public engagement with AI in the situated environment of city streets under these paradoxical conditions. Combining perspectives and approaches from social and cultural studies of AI, Design Research and Science and Technology Studies (STS), we explore the affordances of the street as a site for 'material participation' in AI through design-based interventions: the creation of 'everyday AI observatories.' We narrate and reflect on our participatory observations of AI in five city streets in the UK and Australia and highlight a set of tensions that emerged from them: 1) the framing of the street as a transactional environment, 2) the designed invisibility of AI and its publics in the street 3) the stratification of street environments through statistical governance. Based on this discussion and drawing on Jane Jacobs' notion of ""eyes on the street,"" we put forward the relational notion of ""reciprocity deficits"" between AI infrastructures and everyday publics in the street. The conclusion reflects on the consequences of this form of social invisibility of AI for situated engagement with AI by everyday publics in the street and for public trust in urban governance."
2510.23341,LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data,['cs.CL'],['Teng Lin'],"The scarcity of high-quality knowledge graphs (KGs) remains a critical bottleneck for downstream AI applications, as existing extraction methods rely heavily on error-prone pattern-matching techniques or resource-intensive large language models (LLMs). While recent tools leverage LLMs to generate KGs, their computational demands limit accessibility for low-resource environments. Our paper introduces LightKGG, a novel framework that enables efficient KG extraction from textual data using small-scale language models (SLMs) through two key technical innovations: (1) Context-integrated Graph extraction integrates contextual information with nodes and edges into a unified graph structure, reducing the reliance on complex semantic processing while maintaining more key information; (2) Topology-enhanced relationship inference leverages the inherent topology of the extracted graph to efficiently infer relationships, enabling relationship discovery without relying on complex language understanding capabilities of LLMs. By enabling accurate KG construction with minimal hardware requirements, this work bridges the gap between automated knowledge extraction and practical deployment scenarios while introducing scientifically rigorous methods for optimizing SLM efficiency in structured NLP tasks."
2510.23340,Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps,"['cs.AI', 'cs.CL', 'cs.HC']","['Anwesha Das', 'John Duff', 'Jörg Hoffmann', 'Vera Demberg']","Adaptive agent design offers a way to improve human-AI collaboration on time-sensitive tasks in rapidly changing environments. In such cases, to ensure the human maintains an accurate understanding of critical task elements, an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively, given that human attention represents a zero-sum cognitive resource where focus on one message diminishes awareness of other or upcoming information. We introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication, formalised as Bayesian reference resolution using the Rational Speech Act (RSA) modelling framework, to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment. The agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how prior-guided interpretation of messages will influence attention to the interface and subsequent belief update, across several timesteps out to a fixed horizon. In a comparison to baseline methods, we show that this effectiveness depends crucially on combining multi-step planning with a realistic model of user awareness. As the first application of RSA for communication in a dynamic environment, and for human-AI interaction in general, we establish theoretical foundations for pragmatic communication in human-agent teams, highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents."
2510.23339,The Programmable Liquid-crystal Active Coronagraphic Imager for the 4-m DAG telescope (PLACID) instrument: installation and commissioning update,"['astro-ph.IM', 'doi', '10.1117/12.3063059']","['Jonas G. Kühn', 'Ruben Tandon', 'Lucas Marquis', 'Liurong Lin', 'Derya Öztürk Çetni', 'Iljadin Manurung', 'Axel Potier', 'Laurent Jolissaint', 'Audrey Baur', 'Daniele Piazza', 'Mathias Brändli', 'Martin Rieder']","The Programmable Liquid-crystal Active Coronagraphic Imager for the DAG telescope (PLACID) instrument is a novel high-contrast direct imaging facility that was recently installed on the new Turkish 4-m DAG telescope. In brief, PLACID consists in a fore-optics coronagraphic intermediate stage platform, installed in-between the TROIA XAO system and the DIRAC HAWAII-1RG focal-plane array. The PLACID instrument was delivered to ATASAM campus facilities in March of 2024, and transported to summit in October of 2024. In February of 2025, the PLACID optical breadboard was craned to the DAG observatory floor, and successfully installed on the optical table of the diffraction-limited Nasmyth platform of the 4-m telescope. Following the official DAG Acceptance milestone in the spring of 2025, Assembly, Integration and Validation (AIV) activities have started in July of 2025, when PLACID was cabled up with all active components fully interfaced and tested for functional integrity. When on-sky by early 2026, PLACID will be the world's first active coronagraph system, fielding a customized spatial light modulator (SLM) acting as a dynamically programmable focal-plane phase mask (FPM) coronagraph from H- to Ks-band. This will provide a wealth of novel options to observers, among which software-only abilities to change or re-align the FPM pattern in function of observational conditions or science requirements. Future features will include non-common path aberrations (NCPA) self-calibration, angular differential imaging (ADI) coronagraphy for binary or triple stars, as well as coherent differential imaging (CDI). We hereby present the PLACID AIV activities that have taken place over the last twelve months, and the next steps for commissioning the instrument internally, and on-sky later this year."
2510.23338,Alma Luminous Star catalogue IV: A Massive star census in the Magellanic Clouds,"['astro-ph.GA', 'astro-ph.SR']","['J. A. Molina-Calzada', 'J. Maíz Apellániz']","The fourth part of the Alma Luminous Star catalogue (ALS IV) aims to create the most comprehensive sample of massive stars in the Magellanic Clouds (MCs). By combining Gaia DR3 with Simbad and complementing this information with other photometric and spectroscopic catalogues, we select the massive stars in this region. To achieve this, we apply filters in photometry, combining different bands, as well as in variability and spectral types from the literature. With this approach, we will obtain one of the most complete samples of massive stars in the MCs, which can be used both to study the Clouds and the Magellanic Bridge, as well as the massive stars they contain."
2510.23337,BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning,['cs.CL'],"['Siyuan Zheng', 'Pai Liu', 'Xi Chen', 'Jizheng Dong', 'Sihan Jia']","Human-like virtual characters are crucial for games, storytelling, and virtual reality, yet current methods rely heavily on annotated data or handcrafted persona prompts, making it difficult to scale up and generate realistic, contextually coherent personas. We create the first QA dataset for BaZi-based persona reasoning, where real human experiences categorized into wealth, health, kinship, career, and relationships are represented as life-event questions and answers. Furthermore, we propose the first BaZi-LLM system that integrates symbolic reasoning with large language models to generate temporally dynamic and fine-grained virtual personas. Compared with mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a 30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information is used, our model's accuracy drops by 20%-45%, showing the potential of culturally grounded symbolic-LLM integration for realistic character simulation."
2510.23336,Charge Trap Analysis in a SENSEI Skipper-CCD: Understanding Low-Energy Backgrounds in Rare-Event Searches,"['hep-ex', 'astro-ph.IM']","['Agustin Brusco', 'Bruno Sivilotti', 'Ana M. Botti', 'Brenda Cervantes', 'Ansh Desai', 'Rouven Essig', 'Juan Estrada', 'Erez Etzion', 'Guillermo Fernandez Moroni', 'Stephen E. Holland', 'Ian Lawson', 'Steffon Luoma', 'Santiago E. Perez', 'Dario Rodrigues', 'Javier Tiffenberg', 'Sho Uemura', 'Yikai Wu']","Skipper Charge-Coupled Devices (Skipper-CCDs) are ultra-low-threshold detectors capable of detecting energy deposits in silicon at the eV scale. Increasingly used in rare-event searches, one of the major challenges in these experiments is mitigating low-energy backgrounds. In this work, we present results on trap characterization in a silicon Skipper-CCD produced in the same fabrication run as the SENSEI experiment at SNOLAB. Lattice defects contribute to backgrounds in rare-event searches through single-electron charge trapping. To investigate this, we employ the charge-pumping technique at different temperatures to identify dipoles produced by traps in the CCD channel. We fully characterize a fraction of these traps and use this information to extrapolate their contribution to the single-electron background in SENSEI. We find that this subpopulation of traps does not contribute significantly but more work is needed to assess the impact of the traps that can not be characterized."
2510.23335,Epsilon-Optimal Policies for Average-Cost Separable MDPs with Perturbations,['math.OC'],['Dhairya Kantawala'],"We study a class of infinite-horizon average-cost Markov Decision Processes (MDPs) whose reward and transition structures are nearly separable. For the totally separable baseline (that is, with no perturbation), we derive an explicit stationary decision rule that is exactly average-optimal. We then show that under an epsilon-perturbation of the separable structure, this policy remains epsilon-optimal, meaning that the loss in the average reward is of order O(epsilon)."
2510.23334,Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models,['cs.CL'],"['Mohammad Atif Quamar', 'Mohammad Areeb', 'Nishant Sharma', 'Ananth Shreekumar', 'Jonathan Rosenthal', 'Muslum Ozgur Ozmen', 'Mikhail Kuznetsov', 'Z. Berkay Celik']","LLM alignment remains a critical challenge. Inference-time methods provide a flexible alternative to fine-tuning, but their uniform computational effort often yields suboptimal alignment. We hypothesize that for many alignment tasks, the initial tokens of a response are disproportionately more critical. To leverage this principle, we introduce AdaSearch, a novel blockwise search strategy. It adaptively allocates a fixed computational budget using a sampling schedule, focusing search effort on these critical tokens. We apply AdaSearch to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our comprehensive evaluation across eight LLMs demonstrates that AdaSearch outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates improve by over 10% for harmlessness generation, controlled sentiment generation, and for mathematical reasoning tasks relative to Best-of-N."
2510.23333,"Intersection theory and Siegel-Veech constant for Prym eigenform loci in $Ω\mathcal{M}_3(2,2)^{\rm odd}$","['math.GT', 'math.AG', 'math.DS']",['Duc-Manh Nguyen'],"We compute the Siegel-Veech constants associated to saddle connections with distinct endpoints on Prym eigenforms for real quadratic orders with non-square discriminant in $Ω\mathcal{M}_3(2,2)^{\rm odd}$."
2510.23332,"Quadratic Truncated Random Return in Distributional LQR: Positive Definiteness, Density, and Log-Concavity",['math.OC'],"['Ruyi Teng', 'Dan Wang', 'Wei Chen', 'Yulong Gao']","Distributional linear quadratic regulator (LQR) is a new framework that integrates the distributional reinforcement learning and classical LQR, which offers a new way to study the random return instead of the expected cost. Unlike iterative approximation using dynamic programming in the DRL, a closed-form expression for the random return can be exactly characterized in the distributional LQR, which is defined over infinitely many random variables. In recent work [1, 2], it has been shown that this random return can be well approximated by a finite number of random variables, which we call truncated random return. In this paper, we study the truncated random return in the distributional LQR. We show that the truncated random return can be naturally expressed in the quadratic form. We develop a sufficient condition for the positive definiteness of the block symmetric matrix in the quadratic form and provide the lower and upper bounds on the eigenvalues of this matrix. We further show that in this case, the truncated random return follows a positively weighted non-central chi-square distribution if the random disturbances admits Gaussian, and its cumulative distribution function is log-concave if the probability density function of the random disturbances is log-concave."
2510.23331,Dynamical Phase Transition of Dark Solitons in Spherical Holographic Superfluids,['hep-th'],"['Meng Gao', 'Yu Tian', 'Changxu Yan', 'Hongbao Zhang']","In this paper, we employ, for the first time, the holographic gravity approach to investigate the dynamical stability of solitons in spherical superfluids. Transverse perturbations are applied to the background of spherical soliton configurations, and the collective excitation modes of the solitons are examined within the framework of linear analysis. Our study reveals the existence of two distinct unstable modes in the soliton configurations. Through fully nonlinear evolution schemes, the dynamical evolution and final states of the solitons are elucidated. The results demonstrate that the solitons exhibit both self-acceleration instability and snake instability at different temperatures, respectively. And we explore the corresponding temperature-dependent dynamical phase transitions. It is noteworthy that the dynamical behavior of spherical solitons is distinct from the planar case due to the presence of spherical curvature."
2510.23330,The First Star-by-star $N$-body/Hydrodynamics Simulation of Our Galaxy Coupling with a Surrogate Model,"['astro-ph.GA', 'cs.DC', 'cs.LG', 'physics.comp-ph', 'doi', '10.1145/3712285.3759866']","['Keiya Hirashima', 'Michiko S. Fujii', 'Takayuki R. Saitoh', 'Naoto Harada', 'Kentaro Nomura', 'Kohji Yoshikawa', 'Yutaka Hirai', 'Tetsuro Asano', 'Kana Moriwaki', 'Masaki Iwasawa', 'Takashi Okamoto', 'Junichiro Makino']","A major goal of computational astrophysics is to simulate the Milky Way Galaxy with sufficient resolution down to individual stars. However, the scaling fails due to some small-scale, short-timescale phenomena, such as supernova explosions. We have developed a novel integration scheme of $N$-body/hydrodynamics simulations working with machine learning. This approach bypasses the short timesteps caused by supernova explosions using a surrogate model, thereby improving scalability. With this method, we reached 300 billion particles using 148,900 nodes, equivalent to 7,147,200 CPU cores, breaking through the billion-particle barrier currently faced by state-of-the-art simulations. This resolution allows us to perform the first star-by-star galaxy simulation, which resolves individual stars in the Milky Way Galaxy. The performance scales over $10^4$ CPU cores, an upper limit in the current state-of-the-art simulations using both A64FX and X86-64 processors and NVIDIA CUDA GPUs."
2510.23329,Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon,['cs.RO'],"['Shreya Santra', 'Thomas Robbins', 'Kazuya Yoshida']","Autonomous navigation in unstructured environments is essential for field and planetary robotics, where robots must efficiently reach goals while avoiding obstacles under uncertain conditions. Conventional algorithmic approaches often require extensive environment-specific tuning, limiting scalability to new domains. Deep Reinforcement Learning (DRL) provides a data-driven alternative, allowing robots to acquire navigation strategies through direct interactions with their environment. This work investigates the feasibility of DRL policy generalization across visually and topographically distinct simulated domains, where policies are trained in terrestrial settings and validated in a zero-shot manner in extraterrestrial environments. A 3D simulation of an agricultural rover is developed and trained using Proximal Policy Optimization (PPO) to achieve goal-directed navigation and obstacle avoidance in farmland settings. The learned policy is then evaluated in a lunar-like simulated environment to assess transfer performance. The results indicate that policies trained under terrestrial conditions retain a high level of effectiveness, achieving close to 50\% success in lunar simulations without the need for additional training and fine-tuning. This underscores the potential of cross-domain DRL-based policy transfer as a promising approach to developing adaptable and efficient autonomous navigation for future planetary exploration missions, with the added benefit of minimizing retraining costs."
2510.23328,User-defined Electrostatic Potentials in DFT Supercell Calculations: Implementation and Application to Electrified Interfaces,['cond-mat.mtrl-sci'],"['Samuel Mattoso', 'Jing Yang', 'Florian Deißenbeck', 'Ahmed Abdelkawy', 'Christoph Freysoldt', 'Stefan Wipperman', 'Mira Todorova', 'Jörg Neugebauer']","Introducing electric fields into density functional theory (DFT) calculations is essential for understanding electrochemical processes, interfacial phenomena, and the behavior of materials under applied bias. However, applying user-defined electrostatic potentials in DFT is nontrivial and often requires direct modification to the specific DFT code. In this work, we present an implementation for supercell DFT calculations under arbitrary electric fields and discuss the required corrections to the energies and forces. The implementation is realized through the recently released VASP-Python interface, enabling the application of user-defined fields directly within the standard VASP software and providing great flexibility and control. We demonstrate the application of this approach with diverse case studies, including molecular adsorption on electrified surfaces, field ion microscopy, electrochemical solid-water interfaces, and implicit solvent models."
2510.23327,GRAD: Real-Time Gated Recurrent Anomaly Detection in Autonomous Vehicle Sensors Using Reinforced EMA and Multi-Stage Sliding Window Techniques,['cs.LG'],"['Mohammad Hossein Jafari Naeimi', 'Ali Norouzi', 'Athena Abdi']","This paper introduces GRAD, a real-time anomaly detection method for autonomous vehicle sensors that integrates statistical analysis and deep learning to ensure the reliability of sensor data. The proposed approach combines the Reinforced Exponential Moving Average (REMA), which adapts smoothing factors and thresholding for outlier detection, with the Multi-Stage Sliding Window (MS-SW) technique for capturing both short- and long-term patterns. These features are processed using a lightweight Gated Recurrent Unit (GRU) model, which detects and classifies anomalies based on bias types, while a recovery module restores damaged sensor data to ensure continuous system operation. GRAD has a lightweight architecture consisting of two layers of GRU with a limited number of neurons that make it appropriate for real-time applications while maintaining high detection accuracy. The GRAD framework achieved remarkable performance in anomaly detection and classification. The model demonstrated an overall F1-score of 97.6% for abnormal data and 99.4% for normal data, signifying its high accuracy in distinguishing between normal and anomalous sensor data. Regarding the anomaly classification, GRAD successfully categorized different anomaly types with high precision, enabling the recovery module to accurately restore damaged sensor data. Relative to analogous studies, GRAD surpasses current models by attaining a balance between elevated detection accuracy and diminished computational expense. These results demonstrate GRAD's potential as a reliable and efficient solution for real-time anomaly detection in autonomous vehicle systems, guaranteeing safe vehicle operation with minimal computational overhead."
2510.23326,Binary gravitational waves as probes of quantum graviton states,"['gr-qc', 'quant-ph']","['Sugumi Kanno', 'Jiro Soda', 'Akira Taniguchi']","It is well known that the most reliable way to reveal the quantum nature of light is through photon number statistics, since photons exhibiting sub-Poissonian statistics unambiguously demonstrate their quantum behavior. In this paper, we show that gravitons emitted by binary systems can, in principle, exhibit analogous sub-Poissonian statistics. The key idea is that the vacuum state of gravitons may not be the standard Minkowski vacuum but rather a nonclassical state imprinted with the physics of the early Universe, such as inflation. Accordingly, gravitational waves from binary systems provide a means to probe the graviton states generated in the early Universe. As a concrete example, we show that squeezed graviton states originating from inflation may be detected through the observation of gravitational waves from binary systems."
2510.23325,Multitask Multimodal Self-Supervised Learning for Medical Images,"['cs.CV', 'cs.AI', 'cs.LG']",['Cristian Simionescu'],"This thesis works to address a pivotal challenge in medical image analysis: the reliance on extensive labeled datasets, which are often limited due to the need for expert annotation and constrained by privacy and legal issues. By focusing on the development of self-supervised learning techniques and domain adaptation methods, this research aims to circumvent these limitations, presenting a novel approach to enhance the utility and efficacy of deep learning in medical imaging.
  Central to this thesis is the development of the Medformer, an innovative neural network architecture designed for multitask learning and deep domain adaptation. This model is adept at pre-training on diverse medical image datasets, handling varying sizes and modalities, and is equipped with a dynamic input-output adaptation mechanism. This enables efficient processing and integration of a wide range of medical image types, from 2D X-rays to complex 3D MRIs, thus mitigating the dependency on large labeled datasets.
  Further, the thesis explores the current state of self-supervised learning in medical imaging. It introduces novel pretext tasks that are capable of extracting meaningful information from unlabeled data, significantly advancing the model's interpretative abilities. This approach is validated through rigorous experimentation, including the use of the MedMNIST dataset, demonstrating the model's proficiency in learning generalized features applicable to various downstream tasks.
  In summary, this thesis contributes to the advancement of medical image analysis by offering a scalable, adaptable framework that reduces reliance on labeled data. It paves the way for more accurate, efficient diagnostic tools in healthcare, signifying a major step forward in the application of deep learning in medical imaging."
2510.23324,Partnering with Generative AI: Experimental Evaluation of Human-Led and Model-Led Interaction in Human-AI Co-Creation,['cs.HC'],"['Sebastian Maier', 'Manuel Schneider', 'Stefan Feuerriegel']","Large language models (LLMs) show strong potential to support creative tasks, but the role of the interface design is poorly understood. In particular, the effect of different modes of collaboration between humans and LLMs on co-creation outcomes is unclear. To test this, we conducted a randomized controlled experiment ($N = 486$) comparing: (a) two variants of reflective, human-led modes in which the LLM elicits elaboration through suggestions or questions, against (b) a proactive, model-led mode in which the LLM independently rewrites ideas. By assessing the effects on idea quality, diversity, and perceived ownership, we found that the model-led mode substantially improved idea quality but reduced idea diversity and users' perceived idea ownership. The reflective, human-led mode also improved idea quality, yet while preserving diversity and ownership. Our findings highlight the importance of designing interactions with generative AI systems as reflective thought partners that complement human strengths and augment creative processes."
2510.23323,Towards Scaling Deep Neural Networks with Predictive Coding: Theory and Practice,"['cs.LG', 'cs.NE']",['Francesco Innocenti'],"Backpropagation (BP) is the standard algorithm for training the deep neural networks that power modern artificial intelligence including large language models. However, BP is energy inefficient and unlikely to be implemented by the brain. This thesis studies an alternative, potentially more efficient brain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks (PCNs) perform inference by iterative equilibration of neuron activities before learning or weight updates. Recent work has suggested that this iterative inference procedure provides a range of benefits over BP, such as faster training. However, these advantages have not been consistently observed, the inference and learning dynamics of PCNs are still poorly understood, and deep PCNs remain practically untrainable. Here, we make significant progress towards scaling PCNs by taking a theoretical approach grounded in optimisation theory. First, we show that the learning dynamics of PC can be understood as an approximate trust-region method using second-order information, despite explicitly using only first-order local updates. Second, going beyond this approximation, we show that PC can in principle make use of arbitrarily higher-order information, such that for feedforward networks the effective landscape on which PC learns is far more benign and robust to vanishing gradients than the (mean squared error) loss landscape. Third, motivated by a study of the inference dynamics of PCNs, we propose a new parameterisation called ``$μ$PC'', which for the first time allows stable training of 100+ layer networks with little tuning and competitive performance on simple tasks. Overall, this thesis significantly advances our fundamental understanding of the inference and learning dynamics of PCNs, while highlighting the need for future research to focus on hardware co-design if PC is to compete with BP at scale."
2510.23322,Decoupled Solution for Composite Sparse-plus-Smooth Inverse Problems,['math.OC'],"['Adrian Jarret', 'Julien Fageot']","We consider composite linear inverse problems where the signal to recover is modeled as a sum of two functions. We study a variational framework formulated as an optimization problem over the pairs of components using two regularization terms, each of them acting on a different part of the solution. The specificity of our work is to study the case where one component is regularized with an atomic norm over a Banach space, which is known to promote sparse reconstruction, while the other is regularized with a quadratic norm over a Hilbert space, which promotes smooth solution.
  We show how this composite optimization problem can be reduced to an optimization problem over the Banach space component only up to a linear problem. This reveals a decoupling between the two components, allowing for a new composite representer theorem. It naturally induces a decoupled numerical procedure to solve the composite optimization problem.
  We exemplify our main result with a composite deconvolution problem of Dirac recovery over a smooth background. In this setting, we illustrate the relevance of a composite model and show a significant temporal gain on signal reconstruction, which results from our decoupled algorithmic approach."
2510.23321,Model-Behavior Alignment under Flexible Evaluation: When the Best-Fitting Model Isn't the Right One,"['q-bio.NC', 'stat.ME']","['Itamar Avitan', 'Tal Golan']","Linearly transforming stimulus representations of deep neural networks yields high-performing models of behavioral and neural responses to complex stimuli. But does the test accuracy of such predictions identify genuine representational alignment? We addressed this question through a large-scale model-recovery study. Twenty diverse vision models were linearly aligned to 4.5 million behavioral judgments from the THINGS odd-one-out dataset and calibrated to reproduce human response variability. For each model in turn, we sampled synthetic responses from its probabilistic predictions, fitted all candidate models to the synthetic data, and tested whether the data-generating model would re-emerge as the best predictor of the simulated data. Model recovery accuracy improved with training-set size but plateaued below 80%, even at millions of simulated trials. Regression analyses linked misidentification primarily to shifts in representational geometry induced by the linear transformation, as well as to the effective dimensionality of the transformed features. These findings demonstrate that, even with massive behavioral data, overly flexible alignment metrics may fail to guide us toward artificial representations that are genuinely more human-aligned. Model comparison experiments must be designed to balance the trade-off between predictive accuracy and identifiability-ensuring that the best-fitting model is also the right one."
2510.23320,LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization,"['eess.AS', 'cs.CL', 'cs.SD']","['Máté Gedeon', 'Péter Mihajlik']","We introduce LibriConvo, a simulated multi-speaker conversational dataset based on speaker-aware conversation simulation (SASC), designed to support training and evaluation of speaker diarization and automatic speech recognition (ASR) systems. Unlike prior resources that mostly rely on semantically disconnected utterances and implausible temporal gaps, LibriConvo ensures semantic coherence and realistic conversational timing. Our pipeline leverages CallHome with external VAD for reliable boundaries, applies compression to reduce unnaturally long silences, and organizes LibriTTS utterances by book to maintain contextual consistency. Acoustic realism is enhanced via a novel room impulse response selection procedure that ranks speaker-microphone configurations by spatial plausibility, balancing realism and diversity. The dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers, split in a speaker-disjoint manner for robust evaluation. Baselines show that the sortformer model outperforms the pyannote pipeline in diarization, while a fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves 7.29\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides a valuable resource for advancing multi-speaker speech processing research with realistic conversational dynamics and controlled experimental conditions."
2510.23319,Arabic Little STT: Arabic Children Speech Recognition Dataset,"['cs.CL', 'cs.AI', 'cs.HC', 'cs.LG', 'cs.SD']","['Mouhand Alkadri', 'Dania Desouki', 'Khloud Al Jallad']","The performance of Artificial Intelligence (AI) systems fundamentally depends on high-quality training data. However, low-resource languages like Arabic suffer from severe data scarcity. Moreover, the absence of child-specific speech corpora is an essential gap that poses significant challenges. To address this gap, we present our created dataset, Arabic Little STT, a dataset of Levantine Arabic child speech recorded in classrooms, containing 355 utterances from 288 children (ages 6 - 13). We further conduct a systematic assessment of Whisper, a state-of-the-art automatic speech recognition (ASR) model, on this dataset and compare its performance with adult Arabic benchmarks. Our evaluation across eight Whisper variants reveals that even the best-performing model (Large_v3) struggles significantly, achieving a 0.66 word error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on adult datasets. These results align with other research on English speech. Results highlight the critical need for dedicated child speech benchmarks and inclusive training data in ASR development. Emphasizing that such data must be governed by strict ethical and privacy frameworks to protect sensitive child information. We hope that this study provides an initial step for future work on equitable speech technologies for Arabic-speaking children. We hope that our publicly available dataset enrich the children's demographic representation in ASR datasets."
2510.23318,Semifree Isovariant Poincaré Spaces and the Gap Condition,"['math.AT', 'math.GT']","['Dominik Kirstein', 'Christian Kremer']","We introduce the notion of a semifree isovariant $G$-Poincaré space, a homotopical notion interpolating between semifree closed smooth $G$-manifolds and the equivariant Poincaré spaces of [HKK24b]. It carries the additional structure of an equivariant Poincaré embedding of the fixed points of a semifree $G$-Poincaré space. Under suitable gap conditions on the codimension, we show that the space of isovariant structures on a semifree $G$-Poincaré space for a periodic finite group $G$ is highly connected, giving a useful construction tool for manifold structures on equivariant Poincaré spaces."
2510.23317,"Equivariance2Inverse: A Practical Self-Supervised CT Reconstruction Method Benchmarked on Real, Limited-Angle, and Blurred Data",['eess.IV'],"['Dirk Elias Schut', 'Adriaan Graas', 'Robert van Liere', 'Tristan van Leeuwen']","Deep learning has shown impressive results in reducing noise and artifacts in X-ray computed tomography (CT) reconstruction. Self-supervised CT reconstruction methods are especially appealing for real-world applications because they require no ground truth training examples. However, these methods involve a simplified X-ray physics model during training, which may make inaccurate assumptions, for example, about scintillator blurring, the scanning geometry, or the distribution of the noise. As a result, they can be less robust to real-world imaging circumstances. In this paper, we review the model assumptions of six recent self-supervised CT reconstruction methods. Moreover, we benchmark these methods on the real-world 2DeteCT dataset and on synthetic data with and without scintillator blurring and a limited-angle scanning geometry. The results of our benchmark show that methods that assume that the noise is pixel-wise independent do not perform well on data with scintillator blurring, and that assuming rotation invariance improves results on limited-angle reconstructions. Based on these findings, we combined successful concepts of the Robust Equivariant Imaging and Sparse2Inverse methods in a new self-supervised CT reconstruction method called Equivariance2Inverse."
2510.23316,"Efficient Repair of (k+2, k) Degraded Read Friendly MDS Array Codes With Sub-packetization 2",['cs.IT'],"['Jie Li', 'Xiaohu Tang']","In this paper, we present two constructions of degraded read friendly (DRF) MDS array codes with two parity nodes and a sub-packetization level of 2 over small finite fields, applicable for any arbitrary code length. The first construction achieves the smallest repair bandwidth among all existing constructions with the same parameters, and is asymptotically optimal with respect to the lower bound on the average repair bandwidth characterized by Zhang et al. The second construction supports two repair mechanisms, depending on whether computation within the helper nodes is permitted or not during the node repair process, thereby optimizing either the repair bandwidth or the rebuilding access."
2510.23315,"Pinching-antenna-enabled Federated Learning: Tail Latency, Participation, and Convergence Analysis",['cs.IT'],"['Yushen Lin', 'Zihan Chen', 'Zhiguo Ding']","Federated learning (FL) in wireless networks is limited by straggler delays from unpredictable channel conditions. In this paper, we investigate the pinching-antenna system (PASS), which dynamically 'pinches' the radiator along a dielectric waveguide to shorten the worst links. In synchronous FL (SFL), we prove that PASS shortens the worst-link distance, and it increases the on-time completion probability in asynchronous FL (AFL). Accordingly, SFL exhibits stochastic dominance on round time, while AFL yields explicit latency and participation gains. We then pair physical-layer (PHY)-aware sampling with error-feedback compression and prove that pinching raises the minimum inclusion probability, thus shrinking both the sampling variability and compression-induced floors in a Lyapunov analysis. Simulations demonstrate consistent wall clock speedups and markedly shorter latency tails. By addressing stragglers at their PHY root, PASS complements higher-layer scheduling and accelerates wireless FL in both SFL and AFL."
2510.23314,Norm of the Hilbert matrix operator on logarithmically weighted Bloch and Hardy spaces,"['math.FA', 'math.CV']","['Shanli Ye', 'Qisong Zheng']","In this paper, we compute the exact value of the norm of the Hilbert matrix operator $\mathcal{H}$ acting from the classical Bloch space $\mathcal{B}$ into the logarithmically weighted Bloch space $\mathcal{B}_{\log}$, and show that it equals $\frac{3}{2}$; we also find that the norm from the space of bounded analytic functions $H^\infty$ into the logarithmically weighted Hardy space $H^{\infty}_{\log}$ is $1$. Furthermore, we establish both lower and upper bounds for the norm of $\mathcal{H}$ when it maps from the $α$-Bloch space $\mathcal{B}^α$ into the logarithmically weighted $\mathcal{B}^α_{\log}$ with $1 <α< 2$, and from the Hardy space $H^{1}$ into the logarithmically weighted Hardy space $H^{1}_{\log}$."
2510.23313,Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks,['cs.CR'],"['Yaokai Feng', 'Kouichi Sakurai']","This survey systematizes the evolution of network intrusion detection systems (NIDS), from conventional methods such as signature-based and neural network (NN)-based approaches to recent integrations with large language models (LLMs). It clearly and concisely summarizes the current status, strengths, and limitations of conventional techniques, and explores the practical benefits of integrating LLMs into NIDS. Recent research on the application of LLMs to NIDS in diverse environments is reviewed, including conventional network infrastructures, autonomous vehicle environments and IoT environments.
  From this survey, readers will learn that: 1) the earliest methods, signature-based IDSs, continue to make significant contributions to modern systems, despite their well-known weaknesses; 2) NN-based detection, although considered promising and under development for more than two decades, and despite numerous related approaches, still faces significant challenges in practical deployment; 3) LLMs are useful for NIDS in many cases, and a number of related approaches have been proposed; however, they still face significant challenges in practical applications. Moreover, they can even be exploited as offensive tools, such as for generating malware, crafting phishing messages, or launching cyberattacks. Recently, several studies have been proposed to address these challenges, which are also reviewed in this survey; and 4) strategies for constructing domain-specific LLMs have been proposed and are outlined in this survey, as it is nearly impossible to train a NIDS-specific LLM from scratch."
2510.23312,Low-Resource Audio Codec (LRAC): 2025 Challenge Description,"['cs.SD', 'eess.AS']","['Kamil Wojcicki', 'Yusuf Ziya Isik', 'Laura Lechler', 'Mansur Yesilbursa', 'Ivana Balić', 'Wolfgang Mack', 'Rafał Łaganowski', 'Guoqing Zhang', 'Yossi Adi', 'Minje Kim', 'Shinji Watanabe']","While recent neural audio codecs deliver superior speech quality at ultralow bitrates over traditional methods, their practical adoption is hindered by obstacles related to low-resource operation and robustness to acoustic distortions. Edge deployment scenarios demand codecs that operate under stringent compute constraints while maintaining low latency and bitrate. The presence of background noise and reverberation further necessitates designs that are resilient to such degradations. The performance of neural codecs under these constraints and their integration with speech enhancement remain largely unaddressed. To catalyze progress in this area, we introduce the 2025 Low-Resource Audio Codec Challenge, which targets the development of neural and hybrid codecs for resource-constrained applications. Participants are supported with a standardized training dataset, two baseline systems, and a comprehensive evaluation framework. The challenge is expected to yield valuable insights applicable to both codec design and related downstream audio tasks."
2510.23311,The role of supercluster filaments in shaping galaxy clusters,"['astro-ph.CO', 'astro-ph.GA', 'doi', '10.1051/0004-6361/202556957']","['Raúl Baier-Soto', 'Yara Jaffé', 'Alexis Finoguenov', 'P. Christopher Haines', 'Paola Merluzzi', 'Hugo Méndez-Hernández', 'Antonela Monachesi', 'Ulrike Kuchner', 'Rory Smith', 'Nicolas Tejos', 'Cristóbal Sifón', 'Maria Argudo-Fernández', 'C. R. Bom', 'Johan Comparat', 'Ricardo Demarco', 'F. Rodrigo Haack', 'Ivan Lacerna', 'E. V. R. Lima', 'Ciria Lima-Dias', 'Elismar Lösch', 'C. Mendes de Oliveira', 'Diego Pallero', 'Laerte Sodré Jr', 'S. M. Gabriel Teixeira', 'O. Alghamdi']","In a hierarchical $Λ$CDM Universe, cosmic filaments serve as the primary channels for matter accretion into galaxy clusters, influencing the shape of their dark matter halos. We investigate whether the elongation of galaxy clusters correlates with the orientation of surrounding filaments, providing the first observational test of this relationship in large supercluster regions. We identified and characterized cosmic filaments in two dimensions within the two superclusters that are part of the low-redshift sub-survey of the Chilean Cluster Galaxy Evolution Survey (CHANCES): the Shapley supercluster and the Horologium-Reticulum supercluster. We analyzed the alignment between filament directions --traced by galaxy distributions- and the triaxiality of cluster gravitational potentials --traced by X-ray emission- using publicly available optical and X-ray data. We have found that most (82%) of the X-ray clusters are associated with and interconnected by the optically detected filaments. The clusters-filaments alignment analysis shows that the elongation of most clusters is well aligned with nearby filaments, providing observational confirmation of theoretical predictions, with the alignment progressively reducing at larger cluster-centric distances ($> 1.6 r_{200}$). Overall, our results support the notion that filaments are the main source of galaxy accretion at redshift below 0.1 and additionally provide evidence that matter accretion through filaments shapes the gravitational potential of galaxy clusters. We propose this measurement as a simple observational proxy to determine the direction of accretion in clusters, which is key to understanding both galaxy evolution and the merger history of galaxy clusters."
2510.23310,Effectiveness of cardinality-return weighted maximum independent set approach for financial portfolio optimization,['cond-mat.stat-mech'],"['Keita Takahashi', 'Tetsuro Abe', 'Yasuhito Nakamura', 'Ryo Hidaka', 'Shuta Kikuchi', 'Shu Tanaka']","The portfolio optimization problem is a critical issue in asset management and has long been studied. Markowitz's mean-variance model has fundamental limitations, such as the assumption of a normal distribution for returns and sensitivity to estimation errors in input parameters. In this research, we propose a novel graph theory-based approach, the cardinality-return weighted maximum independent set (CR-WMIS) model, to overcome these limitations. The CR-WMIS model pursues the optimization of both return and risk characteristics. It integrates the risk diversification effect by selecting the largest number of weakly correlated stocks, a feature of the maximum independent set (MIS) model, with the weighting effect based on expected returns from the weighted maximum independent set (WMIS) model. We validated the effectiveness of the proposed method through a five-year backtesting simulation (April 2019 - March 2024) using real market data from the S&P 500. For this task, we employed a simulated-bifurcation-based solver for finding high-quality solutions to large-scale combinatorial optimization problems. In our evaluation, we conducted a comprehensive risk assessment, which has not been sufficiently explored in previous MIS and WMIS studies. The results demonstrate that the CR-WMIS model exhibits superiority in both return and risk characteristics compared to the conventional MIS and WMIS models, as well as the market index (S&P 500). This study provides a practical portfolio optimization method that overcomes the theoretical limitations of the mean-variance model, contributing to both the advancement of academic theory and the support of practical investment decision-making."
2510.23309,Stochastic fractional evolution equations of order $1<α<2$ with generalized operators,"['math.PR', 'math.DS']","['Miloš Japundžić', 'Danijela Rajter-Ćirić']","We consider the Cauchy problem for stochastic fractional evolution equations with Caputo time fractional derivative of order $1<α<2$ and space variable coefficients on an unbounded domain. The space derivatives that appear in the equations are of integer or fractional order such as the left and the right Liouville fractional derivative as well as the Riesz fractional derivative. To solve the problem we use generalized uniformly continuous solution operators. We obtain the unique solution within a certain Colombeau generalized stochastic process space. In our solving procedure, instead of the originate problem we solve a certain approximate problem, where operators of the original and the approximate problem are $L^2$-associated. Finally, application of the theory in solving stochastic time and time-space fractional wave equation is shown."
2510.23308,Probabilistic Proof of Conditional Limit Theorem for Critical Galton--Waston Process,['math.PR'],"['Jiayan Guo', 'Wenming Hong']","Let $\{Z_{n}\}_{n\geq0}$ be a critical Galton--Waston branching process with finite variance $σ^{2}$. Spitzer (unpublished), Lamperti and Ney (1968) proved that for any fixed $0<t<1$, $$\mathscr{L}\left(\frac{Z_{nt}}{n}\Big|Z_{n}>0\right)\overset{\text{d}}{\rightarrow}U_{t}+V_{t}$$ as $n\rightarrow\infty$, where $U_{t}$ and $V_{t}$ are independent random variables having exponential distributions with parameters $2/(t(1-t)σ^{2})$ and $2/(tσ^{2})$ respectively. The proof is short and elegent based on the Laplace transform.
  In this paper, we will specify where the two exponential random variables come from explicitly, in terms of the Geiger's conditioned tree. Actually, $U_{t}$ and $V_{t}$ are resulted from the ``left'' and ``right'' parts of the ``spine'' of the Geiger's tree at generation $[nt]$. To this end, more details and intrinsic properties about the Geiger's conditioned tree will be investigated, which are interesting in its own right as well."
2510.23307,Genetic interfaces at the frontier of expanding microbial colonies,"['physics.bio-ph', 'cond-mat.soft']","['Jonathan Bauermann', 'David R. Nelson']","We study the genetic interfaces between two species of an expanding colony that consists of individual microorganisms that reproduce and undergo diffusion, both at the frontier and in the interior. Within the bulk of the colony, the genetic interface is controlled in a simple way via interspecies interactions. However, at the frontier of the colony, the genetic interface width saturates at finite values for long times, both for neutral strains and interspecies interactions such as antagonism. This finite width arises from geometric effects: genetic interfaces drift toward local minima at an undulating colony frontier, where a focusing mechanism induced by curvature impedes diffusive mixing. Numerical simulations support a logarithmic dependence of the genetic interface width on the strength of the number fluctuations."
2510.23306,ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation,"['cs.CV', 'cs.AI']","['Jiahao Chang', 'Chongjie Ye', 'Yushuang Wu', 'Yuantao Chen', 'Yidan Zhang', 'Zhongjin Luo', 'Chenghong Li', 'Yihao Zhi', 'Xiaoguang Han']","Existing multi-view 3D object reconstruction methods heavily rely on sufficient overlap between input views, where occlusions and sparse coverage in practice frequently yield severe reconstruction incompleteness. Recent advancements in diffusion-based 3D generative techniques offer the potential to address these limitations by leveraging learned generative priors to hallucinate invisible parts of objects, thereby generating plausible 3D structures. However, the stochastic nature of the inference process limits the accuracy and reliability of generation results, preventing existing reconstruction frameworks from integrating such 3D generative priors. In this work, we comprehensively analyze the reasons why diffusion-based 3D generative methods fail to achieve high consistency, including (a) the insufficiency in constructing and leveraging cross-view connections when extracting multi-view image features as conditions, and (b) the poor controllability of iterative denoising during local detail generation, which easily leads to plausible but inconsistent fine geometric and texture details with inputs. Accordingly, we propose ReconViaGen to innovatively integrate reconstruction priors into the generative framework and devise several strategies that effectively address these issues. Extensive experiments demonstrate that our ReconViaGen can reconstruct complete and accurate 3D models consistent with input views in both global structure and local details.Project page: https://jiahao620.github.io/reconviagen."
2510.23305,The AMBRE Project: Line-broadening and stellar rotation of ESO/FEROS archived spectra,['astro-ph.SR'],"['F. Bado', 'P. de Laverny', 'Z. Kam', 'A. Recio-Blanco', 'P. A. Palicio', 'J. Koulidiati']","Stellar rotation is a fundamental parameter in stellar studies. However, large homogeneous catalogues of rotational velocities derived from high-resolution stellar spectra are still lacking. The main objective of this work is to determine the line-broadening parameter (Vbroad), a proxy for the stellar rotational velocity, in a large sample of FGKM stars based on their ESO/FEROS spectra, previously parameterised by the AMBRE Project. Vbroad was estimated by cross-correlating the FEROS spectra with AMBRE binary masks. This methodology also relies on a specific calibration of a coupling constant between the rotational velocity and the width of the cross-correlation function. This fundamental step was performed by adopting the AMBRE grid of synthetic spectra. The derived Vbroad were then validated using data from the literature, ground-based spectroscopic surveys, and Gaia/RVS. After analysing more than 5,000 FEROS spectra (including repeated spectra for several stars), we obtained the line-broadening coefficients for 2,584 stars covering the FGKM spectral types, any stellar gravity, and metallicities between the metal-poor up to sub-solar regimes. The mean Vbroad relative uncertainty of this sample was found to be smaller than 8%. As expected, most stars were found to be slow rotators (below a few km/s), in particular, cool dwarfs and giants. However, several hot dwarfs and high-luminosity stars with high-Vbroad rates were identified, most of them not previously classified as fast rotators and/or affected by large macro-turbulent effects. The measured rotational broadening values are of high-quality and verified on the basis of literature comparisons. We publicly provide this catalogue of Vbroad parameters, including stellar atmospheric and quality parameters, for the analysed AMBRE/FEROS sources."
2510.23304,CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach,['cs.AI'],"['Riccardo Romanello', 'Daniele Lizzio Bosco', 'Jacopo Cossio', 'Dusan Sutulovic', 'Giuseppe Serra', 'Carla Piazza', 'Paolo Burelli']","CNOT gates are fundamental to quantum computing, as they facilitate entanglement, a crucial resource for quantum algorithms. Certain classes of quantum circuits are constructed exclusively from CNOT gates. Given their widespread use, it is imperative to minimise the number of CNOT gates employed. This problem, known as CNOT minimisation, remains an open challenge, with its computational complexity yet to be fully characterised. In this work, we introduce a novel reinforcement learning approach to address this task. Instead of training multiple reinforcement learning agents for different circuit sizes, we use a single agent up to a fixed size $m$. Matrices of sizes different from m are preprocessed using either embedding or Gaussian striping. To assess the efficacy of our approach, we trained an agent with m = 8, and evaluated it on matrices of size n that range from 3 to 15. The results we obtained show that our method overperforms the state-of-the-art algorithm as the value of n increases."
2510.23303,Ground and excited potential energy surfaces for CaF+Ca interactions and isotope exchange reactions,"['physics.atom-ph', 'cond-mat.quant-gas', 'physics.chem-ph']","['Dibyendu Sardar', 'John L. Bohn']","We investigate the intermolecular interactions between laser-cooled CaF and Ca, in their ground and excited electronic states, aiming to understand atom-exchange reaction pathways. Using state-of-the-art \textit{ab initio} quantum chemistry methods, we compute potential energy surfaces for nine electronic states arising from the lowest three asymptotes of Ca$_2$F trimer, within the rigid rotor approximation applied to CaF. Two-dimensional potential energy surfaces are computed for the ground state and one of the excited states. We use a combination of the coupled cluster method restricted to single, double, and perturbative triple excitations, and the multireference configuration interaction method with single and double excitations. The ground (X)~$^2\mathrm{A}'$ electronic state of the trimer is significantly deep and highly anisotropic. The excited electronic states are also strongly bound. Notably, the potential energy surface of one of the excited states, (2)~$^2\mathrm{A}'$, lies below the ground-state asymptote of the trimer. By analyzing the potential energy surfaces, we discuss atom-exchange reaction pathways involving both the ground-state interaction between CaF and Ca and the excited metastable state of Ca."
2510.23302,Estimating applied potentials in cold atom lattice simulators,"['cond-mat.quant-gas', 'quant-ph']","['Bhavik Kumar', 'Daniel Malz']","Cold atoms in optical lattices are a versatile and highly controllable platform for quantum simulation, capable of realizing a broad family of Hubbard models, and allowing site-resolved readout via quantum gas microscopes. In principle, arbitrary site-dependent potentials can also be implemented; however, since lattice spacings are typically below the diffraction limit, precisely applying and calibrating these potentials remains challenging. Here, we propose a simple and efficient experimental protocol that can be used to measure any potential with high precision. The key ingredient in our protocol is the ability in some atomic species to turn off interactions using a Feshbach resonance, which makes the evolution easy to compute. Given this, we demonstrate that collecting snapshots from the time evolution of a known, easily prepared initial state is sufficient to accurately estimate the potential. Our protocol is robust to state preparation errors and uncertainty in the hopping rate. This paves the way toward precision quantum simulation with arbitrary potentials."
2510.23301,MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification,['cs.CV'],"['Yingying Feng', 'Jie Li', 'Jie Hu', 'Yukang Zhang', 'Lei Tan', 'Jiayi Ji']","Real-world object re-identification (ReID) systems often face modality inconsistencies, where query and gallery images come from different sensors (e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched conditions, which limits their robustness and scalability in practical applications. To address this challenge, we propose MDReID, a flexible any-to-any image-level ReID framework designed to operate under both modality-matched and modality-mismatched scenarios. MDReID builds on the insight that modality information can be decomposed into two components: modality-shared features that are predictable and transferable, and modality-specific features that capture unique, modality-dependent characteristics. To effectively leverage this, MDReID introduces two key components: the Modality Decoupling Learning (MDL) and Modality-aware Metric Learning (MML). Specifically, MDL explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enforces orthogonality and complementarity between the two components to enhance discriminative power across modalities. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDReID. Notably, MDReID achieves significant mAP improvements of 9.8\%, 3.0\%, and 11.5\% in general modality-matched scenarios, and average gains of 3.4\%, 11.8\%, and 10.9\% in modality-mismatched scenarios, respectively. The code is available at: \textcolor{magenta}{https://github.com/stone96123/MDReID}."
2510.23300,A least squares finite element method for backward parabolic problems,['math.NA'],['Harald Monsuur'],"Backward parabolic equations, such as the backward heat equation, are classical examples of ill-posed problems where solutions may not exist or depend continuously on the data. In this work, we study a least squares finite element method to numerically approximate solutions to such problems. We derive conditional stability estimates for the weak formulation of inhomogeneous backward parabolic equations, assuming minimal regularity of the solution. These stability results are then used to establish \emph{a priori} error bounds for our proposed method. We address key computational aspects, including the treatment of dual norms through the construction of suitable test spaces, and iterative solutions. Numerical experiments are used to validate our theoretical findings."
2510.23299,MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm Detection,"['cs.CV', 'cs.MM']","['Haochen Zhao', 'Yuyao Kong', 'Yongxiu Xu', 'Gaopeng Gou', 'Hongbo Xu', 'Yubin Wang', 'Haoliang Zhang']","Despite progress in multimodal sarcasm detection, existing datasets and methods predominantly focus on single-image scenarios, overlooking potential semantic and affective relations across multiple images. This leaves a gap in modeling cases where sarcasm is triggered by multi-image cues in real-world settings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed entirely of multi-image samples curated from tweets and Amazon reviews. We further propose the Cross-Image Reasoning Model (CIRM), which performs targeted cross-image sequence modeling to capture latent inter-image connections. In addition, we introduce a relevance-guided, fine-grained cross-modal fusion mechanism based on text-image correspondence to reduce information loss during integration. We establish a comprehensive suite of strong and representative baselines and conduct extensive experiments, showing that MMSD3.0 is an effective and reliable benchmark that better reflects real-world conditions. Moreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0 and MMSD3.0, validating its effectiveness in both single-image and multi-image scenarios."
2510.23298,Galois Groups of Apéry-like Series Modulo Primes,['math.NT'],"['Xavier Caruso', 'Florian Fürnsinn', 'Daniel Vargas-Montoya', 'Wadim Zudilin']","We compute the Galois groups of the reductions modulo the prime numbers $p$ of the generating series of Apéry numbers, Domb numbers and Almkvist--Zudilin numbers. We observe in particular that their behavior is governed by congruence conditions on p."
2510.23297,Drivers of Variation in the Optimal Spatial Structure of Collective Information Gatherers,['q-bio.PE'],"['R. S. Walker', 'G. Ramos-Fernandez', 'D. Boyer', 'S. E. Smith-Aguilar', ""X. O'Neill"", 'M. J. Silk']","Collective systems that self-organise to maximise the group's ability to collect and distribute information can be successful in environments with high spatial and temporal variation. Such organisations are abundant in nature, as sharing information is a key benefit of many biological collective systems, and have been influential in the design of many artificial collectives such as swarm robotics. Understanding how these systems may be spatially distributed to optimise their collective potential is therefore of importance in both ecology and in collective systems design. Here, we develop a mathematical model which uses an optimisation framework to determine the higher-order spatial structure of a collective that optimises group-level knowledge transfer. The domain of the objective function is a set of weighted simplicial sets, which can fully represent the spatial structure from a topological perspective. By varying the parameters within the objective function and the constraints, we determine how the optimal spatial structure may vary when individuals differ in their information gathering ability and how this variation differs in the context of resource constraints. Our key findings are that the amount of resources in the environment can lead to specific subgroup sizes being optimal for the group as a whole when individuals are homogeneous in their information gathering abilities. Further, when there is variation in information gathering abilities, our model implies that the sharing of space between smaller subgroups of the population, rather than the whole population, is optimal for collective knowledge sharing. Our results have applications across diverse contexts from behavioural ecology to bio-inspired collective systems design."
2510.23296,Payload trajectory tracking control for aerial transportation systems with cable length online optimization,"['eess.SY', 'cs.RO']","['Hai Yu', 'Zhichao Yang', 'Wei He', 'Jianda Han', 'Yongchun Fang', 'Xiao Liang']","Cable-suspended aerial transportation systems are employed extensively across various industries. The capability to flexibly adjust the relative position between the multirotor and the payload has spurred growing interest in the system equipped with variable-length cable, promising broader application potential. Compared to systems with fixed-length cables, introducing the variable-length cable adds a new degree of freedom. However, it also results in increased nonlinearity and more complex dynamic coupling among the multirotor, the cable and the payload, posing significant challenges in control design. This paper introduces a backstepping control strategy tailored for aerial transportation systems with variable-length cable, designed to precisely track the payload trajectory while dynamically adjusting cable length. Then, a cable length generator has been developed that achieves online optimization of the cable length while satisfying state constraints, thus balancing the multirotor's motion and cable length changes without the need for manual trajectory planning. The asymptotic stability of the closed-loop system is guaranteed through Lyapunov techniques and the growth restriction condition. Finally, simulation results confirm the efficacy of the proposed method in managing trajectory tracking and cable length adjustments effectively."
2510.23295,Predicting symbolic ODEs from multiple trajectories,['cs.LG'],"['Yakup Emre Şahin', 'Niki Kilbertus', 'Sören Becker']","We introduce MIO, a transformer-based model for inferring symbolic ordinary differential equations (ODEs) from multiple observed trajectories of a dynamical system. By combining multiple instance learning with transformer-based symbolic regression, the model effectively leverages repeated observations of the same system to learn more generalizable representations of the underlying dynamics. We investigate different instance aggregation strategies and show that even simple mean aggregation can substantially boost performance. MIO is evaluated on systems ranging from one to four dimensions and under varying noise levels, consistently outperforming existing baselines."
2510.23294,General Strategy for Large Nernst Coefficient,['cond-mat.mtrl-sci'],"['Junya Endo', 'Hiroyasu Matsuura', 'Manfred Sigrist', 'Masao Ogata']",We propose a general strategy for enhancing the anomalous Nernst coefficient based on the Sommerfeld-Bethe relation. This approach provides a systematic framework for understanding the small anomalous Nernst coefficients typically observed in ferromagnets and identifies conditions under which substantial enhancements can be realized. We further introduce simplified models that exhibit large Nernst coefficients as offering illustrative examples.
2510.23293,Characterization of generalized quasi-Einstein manifolds and modified gravity,"['gr-qc', 'math-ph']","['Uday Chand De', 'Hülya Bağdatli Yilmaz']","In this work, a detailed examination of a specific case of a generalized quasi-Einstein manifold (GQE)n is provided. It begins by exploring generalized quasi-Einstein spacetimes under certain conditions. The analysis then focuses on cases that admit a parallel time-like vector field. Among the findings, it is demonstrated that such spacetimes can be categorized as generalized Robertson-Walker spacetimes, Robertson-Walker spacetimes, and quasi-constant curvature spacetimes. Additionally, the physical implications of these results are discussed. It is also investigated (GQE)4 spacetimes, which accept F(R)-gravity and feature a parallel unit time-like vector field. Finally, various energy conditions are analyzed based on the results related to F(R)-gravity."
2510.23292,Energy extraction from a rotating black hole via magnetic reconnection: Bumblebee gravity,['gr-qc'],"['Ho-Yun YuChih', 'Ye Shen']","Many efforts were made in order to better understand the energy extraction via magnetic reconnection from a rotating black hole, following the work of Comisso and Asenjo in 2021. We also tried to make some progress in our previous works, in which we discussed differences between bulk plasma with different streamlines and also defined the covering factor as an internal property of an accretion system to quantify its capability on extracting energy via magnetic reconnection from its central black hole. In this study, we aim to explore this topic within the framework of a Kerr-Sen-like spacetime induced from Bumblebee gravity, which, among various alternative theories of gravity beyond pure Einstein gravity, stands out as a promising candidate for explaining certain high energy astrophysical phenomena. More specifically, we would like to analyze the influence of the rate of Lorentz symmetry breaking and the Bumblebee charge, the two additional parameters in Bumblebee gravity except for the black hole mass and spin, on the energy extraction via magnetic reconnection. By analyzing the allowed regions for energy extraction and the variations of covering factor, we find that energy extraction becomes more likely to succeed and tends to occur closer to the central region when the spacetime carries bigger rate of Lorentz symmetry breaking and Bumblebee charge. Furthermore, our results indicate that the most favorable spacetime configuration for energy extraction via magnetic reconnection, when the extractable energy of the central black hole is determined, corresponds to the scenario in which the cosmic censorship hypothesis is marginally not violated."
2510.23291,Selmer ranks under quadratic twists satisfying the Heegner hypothesis,['math.NT'],['Alexandros Konstantinou'],"We investigate variations of Selmer ranks under quadratic twists satisfying the Heegner hypothesis. In particular, starting with an elliptic curve $E/\mathbb{Q}$ with partial $2$-torsion and a common relaxed Selmer group, we derive explicit formulae describing the effect of twisting on Selmer ranks in terms of matrices over $\mathbb{F}_{2}$. As an application, we show that these formulae are compatible with predictions made by the parity conjecture."
2510.23290,Group-Level and Personalized Optimization for the Insula and Hippocampus Focal Electric Field in Transcranial Temporal Interferential Stimulation: A Computational Study,['physics.med-ph'],"['Taiga Inoue', 'Naofumi Otsuru', 'Akimasa Hirata']","This study evaluated transcranial temporal interference stimulation (tTIS) for focal targeting of the insula and hippocampus, which are clinically relevant yet anatomically difficult to stimulate. Individualized and group-level electrode optimizations were compared to determine whether generalized montages can provide reliable targeting with reduced modeling demands. Sixty high-resolution head models (30 individuals and their mirrored counterparts) were constructed from T1- and T2-weighted MRI. Electric fields (EFs) were computed using the scalar-potential finite-difference method. Electrode montages and current ratios were optimized to minimize the root-mean-square error between simulated and target EF envelope (EFE) distributions, with a threshold of 0.3 V/m. Subsampling analysis was performed to estimate the number of models required for stable group-level outcomes. For the insula, a montage combining T7-P7 and Fp1-Fp2 achieved the highest focality, comparable to individualized results with reduced variability. For the hippocampus, the F7-T7 and T8-P8 montage gave the best group-level focality, though individualized optimization improved off-target suppression. Stable group-level patterns were obtained using 20 models for the insula and 9 for the hippocampus. Optimal tTIS montages depend on target depth. Group-level optimization suffices for cortical regions like the insula, whereas individualized tuning remains preferable for deeper targets such as the hippocampus."
2510.23289,An Energy-Stable Discontinuous Galerkin Method for the Compressible Navier--Stokes--Allen--Cahn System,['math.NA'],"['Lukas Ostrowski', 'Christian Rohde']","We consider a Navier--Stokes--Allen--Cahn (NSAC) system that governs the compressible motion of a viscous, immiscible two-phase fluid at constant temperature. Weak solutions of the NSAC system dissipate an appropriate energy functional. Based on an equivalent re-formulation of the NSAC system we propose a fully-discrete discontinuous Galerkin (dG) discretization that is mass-conservative, energy-stable, and provides higher-order accuracy in space and second-order accuracy in time. The approach relies on the approach in \cite{Giesselmann2015a} and a special splitting discretization of the derivatives of the free energy function within the Crank-Nicolson time-stepping. Numerical experiments confirm the analytical statements and show the applicability of the approach."
2510.23288,Learning from Frustration: Torsor CNNs on Graphs,"['cs.LG', 'math.AT']","['Daiyuan Li', 'Shreya Arya', 'Robert Ghrist']","Most equivariant neural networks rely on a single global symmetry, limiting their use in domains where symmetries are instead local. We introduce Torsor CNNs, a framework for learning on graphs with local symmetries encoded as edge potentials-- group-valued transformations between neighboring coordinate frames. We establish that this geometric construction is fundamentally equivalent to the classical group synchronization problem, yielding: (1) a Torsor Convolutional Layer that is provably equivariant to local changes in coordinate frames, and (2) the frustration loss--a standalone geometric regularizer that encourages locally equivariant representations when added to any NN's training objective. The Torsor CNN framework unifies and generalizes several architectures--including classical CNNs and Gauge CNNs on manifolds-- by operating on arbitrary graphs without requiring a global coordinate system or smooth manifold structure. We establish the mathematical foundations of this framework and demonstrate its applicability to multi-view 3D recognition, where relative camera poses naturally define the required edge potentials."
2510.23287,Can KM3-230213A be compatible with a cosmogenic origin?,['astro-ph.HE'],['The KM3NeT collaboration'],"On the 13th February 2023 the KM3NeT/ARCA telescope observed a track-like event compatible with a ultra-high-energy muon with an estimated energy of 120 PeV, produced by a neutrino with an even higher energy, making it the most energetic neutrino event ever detected. The reported equivalent flux suggest the possible existence of a new diffuse component. A diffuse cosmogenic flux is expected to originate from the interactions of ultra-high-energy cosmic rays with ambient photon and matter fields. Here we show that this component can be compatible with the reported flux level only integrating the cosmogenic emission, at least up to redshift ~$z = 6 $ and assuming a subdominant fraction of protons in the ultra-high-energy cosmic-ray flux, thus placing constraints on known cosmic accelerators. These conditions impose constraints on known cosmic accelerators and open a window into an unexplored region of the Universe at this energy scale."
2510.23286,Precise Time Delay Measurement and Compensation for Tightly Coupled Underwater SINS/piUSBL Navigation,['cs.RO'],"['Jin Huang', 'Yingqiang Wang', 'Haoda Li', 'Zichen Liu', 'Zhikun Wang', 'Ying Chen']","In multi-sensor systems, time synchronization between sensors is a significant challenge, and this issue is particularly pronounced in underwater integrated navigation systems incorporating acoustic positioning. Such systems are highly susceptible to time delay, which can significantly degrade accuracy when measurement and fusion moments are misaligned. To address this challenge, this paper introduces a tightly coupled navigation framework that integrates a passive inverted ultra-short baseline (piUSBL) acoustic positioning system, a strapdown inertial navigation system (SINS), and a depth gauge under precise time synchronization. The framework fuses azimuth and slant range from the piUSBL with depth data, thereby avoiding poor vertical-angle observability in planar arrays. A novel delay measurement strategy is introduced, combining synchronized timing with acoustic signal processing, which redefines delay-traditionally an unobservable error-into a quantifiable parameter, enabling explicit estimation of both acoustic propagation and system processing delays. Simulations and field experiments confirm the feasibility of the proposed method, with delay-compensated navigation reducing RMSE by 40.45% and maximum error by 32.55%. These findings show that precise delay measurement and compensation not only enhance underwater navigation accuracy but also establish a generalizable framework for acoustic positioning integration, offering valuable insights into time alignment and data fusion in latency-sensitive multi-sensor systems."
2510.23285,Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling,['cs.CV'],"['Ruoyu Wang', 'Beier Zhu', 'Junzhi Li', 'Liangyu Yuan', 'Chi Zhang']","Diffusion-based generative processes, formulated as differential equation solving, frequently balance computational speed with sample quality. Our theoretical investigation of ODE- and SDE-based solvers reveals complementary weaknesses: ODE solvers accumulate irreducible gradient error along deterministic trajectories, while SDE methods suffer from amplified discretization errors when the step budget is limited. Building upon this insight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify the efficiency of ODEs with the error resilience of SDEs. Specifically, we introduce a single per-step learnable coefficient, estimated via lightweight distillation, which dynamically regulates the error correction strength to accelerate diffusion sampling. Notably, our framework can be integrated with existing solvers to enhance their capabilities. Extensive experiments demonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores of 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available in https://github.com/WLU-wry02/AdaSDE."
2510.23284,DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model,['cs.CL'],"['Yuanzhen Xie', 'Liu Ye', 'Jiqun Chu', 'Mochi Gao', 'Hehuan Liu', 'Yunzhi Tan', 'Bo Hu', 'Zang Li']","Text-to-SQL tasks have gained attractive improvements since the release of ChatGPT. Among them, agent-based frameworks have been widely used in this field. However, the impact of data-centric strategies on text-to-SQL tasks has rarely been explored. In this paper, we systemically design a fully automated data-centric pipeline for text-to-SQL tasks, including \emph{adaptive data repair}, which can automatically find and fix errors in the training dataset; and \emph{error data augmentation}, where we specifically diffuse and enhance erroneous data predicted by the initially trained models. Meanwhile, we propose a Multi-Model collaboration training schema, aiming to train multiple models with different augmented data, enabling them to possess distinct capabilities and work together to complement each other, because it has been found that the capability of a single fine-tuned model is very limited. Furthermore, we utilize an ensemble strategy to integrate the capabilities of multiple models to solve a multiple-choice question, aiming to further improve the accuracy of text-to-SQL tasks. The experiment results and ablation study have demonstrated the effectiveness of data-centric pipeline and Multi-Model(MM) interactive iterative strategies, achieving first place in lightweight text-to-SQL models (within 70B)."
2510.23283,Generalized Strichartz estimates for the massive Dirac equation with critical potentials,['math.AP'],"['Federico Cacciafesta', 'Elena Danesi', 'Eric Séré']","In this paper we prove generalized Strichartz estimates for the massive Dirac equation in the case of two critical potential perturbations, namely the $2d$ Aharonov-Bohm magnetic potential and the $3d$ Coulomb potential. The proof makes use of the relativistic Hankel transform introduced in previous works of Cacciafesta, Séré and Cacciafesta, Fanelli for the massless systems, and here adapted to the massive case: this allows for an explicit representation of the solutions, which reduces the analysis to the proof of suitable estimates on the generalized eigenfunctions of the operators. To the best of our knowledge, these are the first dispersive estimates for the massive Dirac equation with critical potentials."
2510.23282,The effects of stellar activity cycles on planetary atmospheric escape and the HeI 1083nm transit signature,"['astro-ph.EP', 'doi', '10.1093/mnras/staf1855']","['Andrew P. Allan', 'Aline A. Vidotto', 'Jorge Sanz-Forcada', ""Carolina Villarreal D'Angelo""]","The HeI 1083nm transit signature is commonly used in tracing escaping planetary atmospheres. However, it can be affected by stellar activity, complicating detections and interpretations of atmospheric escape. We model how stellar activity cycles affect the atmospheric escape and HeI 1083nm signatures of four types of highly irradiated exoplanets, at 0.025 and 0.05 au, during minimum and maximum cycle phases. We consider two stars, exhibiting different cycle behaviours: the Sun and the more active star iota Hor, for which we reconstruct its spectral energy distributions at minimum and maximum phases using X-ray observations and photospheric models. We show that over a modulated activity cycle, the release of extreme ultraviolet photons, responsible for atmospheric escape, varies substantially more than that of mid-UV photons, capable of photoionising HeI (23S). This leads to consistently stronger helium signatures during maximum phases. We show that planets at the largest orbit are more affected by cycles, showing larger variations in escape rates and absorptions between minimum and maximum. We also confirm the counter-intuitive behaviour that, despite the fall-off in escape rate with orbital distance, the HeI 1083nm absorption is not significantly weaker at further orbits, even strengthening with orbital distance for some iota Hor planets. We partially explain this behaviour with the lower mid-UV fluxes at more distant orbits, leading to less HeI (23S) photoionisations. Finally, we propose that stellar cycles could explain some of the conflicting HeI 1083nm observations of the same planet, with detections more likely during a phase of activity maximum."
2510.23281,New Hypernuclei Measurements from STAR,"['nucl-ex', 'hep-ex']",['Yingjie Zhou'],"Hypernuclei are bound states of hyperons (Y) and nucleons (N). Measurements on their yields can help us investigate their production mechanisms. In particular, the ${}^5_Λ$He and $^{4}_Λ$H(e) are bounded substantially tighter compared to the $^{3}_Λ$H. The large radius of the $^{3}_Λ$H leads to suppression in coalescence models, but not in the thermal model where the size of the nucleus does not play a role. As such, studying the $A=3$--5 hypernuclei yields allow us to extract information on the effects of hypernuclear binding on hypernuclei production in heavy-ion collisions.
  In these proceedings, we present measurements of ${}^5_Λ$He yields in Au+Au collisions at $\sqrt{s_{NN}}=3.0$ GeV, $^{4}_Λ$H(e) yields in Au+Au collisions at $\sqrt{s_{NN}}=3.0$--$4.5$ GeV, and $^{3}_Λ$H yields in Au+Au collisions at $\sqrt{s_{NN}}=3$--$27$ GeV. Results on the directed flow of hypernuclei are also reported. The physics implications of these measurements are discussed."
2510.23280,A geometric model for the non-homogeneous tubes of the cluster category of affine type D,"['math.CO', 'math.RT']",['Amandine Favre'],"In this article, we give a geometric model for non-homogeneous tubes of the cluster category of the affine type $D$. This model is given in terms of homotopy classes of unoriented arcs in the twice punctured disk. In particular, we extend the geometric model for the tube of rank $n-2$ given in arXiv:2407.11232 to the two tubes of rank $2$."
2510.23279,Probing Warm Inflation via Correlated Gravitational Waves from First Order Phase Transitions,['gr-qc'],"['Xiao-Bin Sui', 'Jing Liu', 'Rong-Gen Cai']","We investigate the properties of gravitational waves generated by heating induced phase transitions in warm inflation. In this scenario, the heating phase of inflation followed by subsequent cosmological cooling can trigger two associated first-order phase transitions and generate characteristic gravitational waves. The correlated gravitational wave spectral features amplitude, peak frequencies, and oscillatory behavior originate from a unified model governing both phase transitions. These signatures allow discrimination between warm and cold inflation models, and give constraint on the key parameters including the dissipative coupling strength and the inflationary energy scale, collectively illuminating early-Universe dissipative dynamics. Future gravitational wave observatories such as BBO, Ultimate DECIGO, $μ$Ares, resonant cavities, and Pulsar Timing Array experiments, will play a important role in testing these theoretical predictions."
2510.23278,hYOLO Model: Enhancing Object Classification with Hierarchical Context in YOLOv8,['cs.CV'],"['Veska Tsenkova', 'Peter Stanchev', 'Daniel Petrov', 'Deyan Lazarov']","Current convolution neural network (CNN) classification methods are predominantly focused on flat classification which aims solely to identify a specified object within an image. However, real-world objects often possess a natural hierarchical organization that can significantly help classification tasks. Capturing the presence of relations between objects enables better contextual understanding as well as control over the severity of mistakes. Considering these aspects, this paper proposes an end-to-end hierarchical model for image detection and classification built upon the YOLO model family. A novel hierarchical architecture, a modified loss function, and a performance metric tailored to the hierarchical nature of the model are introduced. The proposed model is trained and evaluated on two different hierarchical categorizations of the same dataset: a systematic categorization that disregards visual similarities between objects and a categorization accounting for common visual characteristics across classes. The results illustrate how the suggested methodology addresses the inherent hierarchical structure present in real-world objects, which conventional flat classification algorithms often overlook."
2510.23277,Energetics of star-planet magnetic interactions: Novel insights from 3D modelling,['astro-ph.SR'],"['Arghyadeep Paul', 'Antoine Strugarek']","Star-planet magnetic interactions (SPMI) occurring in the sub-Alfvenic regime can, in principle, induce stellar chromospheric hotspots. Currently, estimates of the power generated by SPMI primarily rely on analytical scaling laws that relate stellar and planetary parameters to the interaction energetics. The existing scaling laws published in the literature so far do not agree with each other by at least an order of magnitude. Our aim is to quantify an absolute upper limit on the power that a planet can channel back to its host star during such interactions, which in turn lead to the formation of stellar hotspots. By performing a series of 3D MHD simulations with varied parameters known to influence the energetics of SPMI, we derive a numerically supported scaling law that can be used to reliably estimate the energy channeled from the planet back to the star. Our results suggest that existing analytical scaling laws may not fully capture the power transferred from the planet to the star through SPMI. The scaling law derived from our numerical simulations appears to provide a more comprehensive estimate, reflecting dependencies on common stellar and planetary parameters also considered in earlier models. Moreover, our findings indicate that power generation involves not only the planetary obstacle itself but also the extended magnetic structure of the Alfven wings interacting with the streaming stellar wind. This study suggests that care should be taken when applying analogies directly from jovian sub-Alfvenic interactions to SPMI, as the underlying physical conditions (specifically the value of the Alfvenic Mach number) may not be directly comparable. Our numerically derived scaling law offers a potentially improved approach for estimating SPMI power, capturing some of the interaction's complexities exclusive to SPMI."
2510.23276,A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results,['cs.CL'],"['Thai-Binh Nguyen', 'Katerina Zmolikova', 'Pingchuan Ma', 'Ngoc Quan Pham', 'Christian Fuegen', 'Alexander Waibel']","We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in the ninth CHiME Challenge, which addresses the cocktail-party problem of overlapping conversations in a single-room setting using audio, visual, and contextual cues. MCoRec captures natural multi-party conversations where the recordings focus on unscripted, casual group chats, leading to extreme speech overlap of up to 100% and highly fragmented conversational turns. The task requires systems to answer the question ""Who speaks when, what, and with whom?"" by jointly transcribing each speaker's speech and clustering them into their respective conversations from audio-visual recordings. Audio-only baselines exceed 100% word error rate, whereas incorporating visual cues yields substantial 50% improvements, highlighting the importance of multi-modality. In this manuscript, we present the motivation behind the task, outline the data collection process, and report the baseline systems developed for the MCoRec."
2510.23275,Analytic $G_0W_0$ gradients based on a double-similarity transformation equation-of-motion coupled-cluster treatment,"['physics.chem-ph', 'cond-mat.mtrl-sci', 'cond-mat.str-el', 'nucl-th']","['Marios-Petros Kitsaras', 'Johannes Tölle', 'Pierre-François Loos']","The accurate prediction of ionization potentials (IPs) is central to understanding molecular reactivity, redox behavior, and spectroscopic properties. While vertical IPs can be accessed directly from electronic excitations at fixed nuclear geometries, the computation of adiabatic IPs requires nuclear gradients of the ionized states, posing a major theoretical and computational challenge, especially within correlated frameworks. Among the most promising approaches for IP calculations is the many-body Green's function $GW$ method, which provides a balanced compromise between accuracy and computational efficiency. Furthermore, it is applicable to both finite and extended systems. Recent work has established formal connections between $GW$ and coupled-cluster doubles (CCD) theory, leading to the first derivation of analytic $GW$ nuclear gradients via a unitary CCD framework. In this work, we present an alternative, fully analytic formulation of $GW$ nuclear gradients based on a modified version of the traditional equation-of-motion CCD formalism, enabling the inclusion of missing correlation effects in the traditional CCD methods."
2510.23274,Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy,"['cs.CR', 'eess.IV']","['Weixuan Chen', 'Qianqian Yang', 'Shuo Shao', 'Shunpu Tang', 'Zhiguo Shi', 'Shui Yu']","While semantic communication (SemCom) improves transmission efficiency by focusing on task-relevant information, it also raises critical privacy concerns. Many existing secure SemCom approaches rely on restrictive or impractical assumptions, such as favorable channel conditions for the legitimate user or prior knowledge of the eavesdropper's model. To address these limitations, this paper proposes a novel secure SemCom framework for image transmission over wiretap channels, leveraging differential privacy (DP) to provide approximate privacy guarantees. Specifically, our approach first extracts disentangled semantic representations from source images using generative adversarial network (GAN) inversion method, and then selectively perturbs private semantic representations with approximate DP noise. Distinct from conventional DP-based protection methods, we introduce DP noise with learnable pattern, instead of traditional white Gaussian or Laplace noise, achieved through adversarial training of neural networks (NNs). This design mitigates the inherent non-invertibility of DP while effectively protecting private information. Moreover, it enables explicitly controllable security levels by adjusting the privacy budget according to specific security requirements, which is not achieved in most existing secure SemCom approaches. Experimental results demonstrate that, compared with the previous DP-based method and direct transmission, the proposed method significantly degrades the reconstruction quality for the eavesdropper, while introducing only slight degradation in task performance. Under comparable security levels, our approach achieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86 for the legitimate user compared with the previous DP-based method."
2510.23273,A Novel Framework for Multi-Modal Protein Representation Learning,"['cs.LG', 'cs.AI', 'q-bio.QM']","['Runjie Zheng', 'Zhen Wang', 'Anjie Qiao', 'Jiancong Xie', 'Jiahua Rao', 'Yuedong Yang']","Accurate protein function prediction requires integrating heterogeneous intrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts (e.g., protein-protein interactions and GO term annotations). However, two key challenges hinder effective fusion: (i) cross-modal distributional mismatch among embeddings produced by pre-trained intrinsic encoders, and (ii) noisy relational graphs of extrinsic data that degrade GNN-based information aggregation. We propose Diffused and Aligned Multi-modal Protein Embedding (DAMPE), a unified framework that addresses these through two core mechanisms. First, we propose Optimal Transport (OT)-based representation alignment that establishes correspondence between intrinsic embedding spaces of different modalities, effectively mitigating cross-modal heterogeneity. Second, we develop a Conditional Graph Generation (CGG)-based information fusion method, where a condition encoder fuses the aligned intrinsic embeddings to provide informative cues for graph reconstruction. Meanwhile, our theoretical analysis implies that the CGG objective drives this condition encoder to absorb graph-aware knowledge into its produced protein representations. Empirically, DAMPE outperforms or matches state-of-the-art methods such as DPFunc on standard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains 0.004-0.007 pp. Ablation studies further show that OT-based alignment contributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 pp Fmax. Overall, DAMPE offers a scalable and theoretically grounded approach for robust multi-modal protein representation learning, substantially enhancing protein function prediction."
2510.23272,Code Aesthetics with Agentic Reward Feedback,['cs.CL'],"['Bang Xiao', 'Lingjie Jiang', 'Shaohan Huang', 'Tengchao Lv', 'Yupan Huang', 'Xun Wu', 'Lei Cui', 'Furu Wei']","Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach."
2510.23271,Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding,['cs.CL'],"['Mohammed Aljafari', 'Ismail Alturki', 'Ahmed Mori', 'Yehya Kadumi']","Mubeen is a proprietary Arabic language model developed by MASARAT SA, optimized for deep understanding of Arabic linguistics, Islamic studies, and cultural heritage. Trained on an extensive collection of authentic Arabic sources significantly expanded by digitizing historical manuscripts via a proprietary Arabic OCR engine, the model incorporates seminal scholarly works in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside thousands of academic theses and peer-reviewed research papers. Conditioned through a deep linguistic engineering framework, Mubeen masters not just the meaning but the eloquence of Arabic, enabling precise understanding across classical texts, contemporary writing, and regional dialects with focus on comprehending user intent and delivering accurate, contextually relevant responses. Unlike other Arabic models relying on translated English data that often fail in intent detection or retrieval-augmented generation (RAG), Mubeen uses native Arabic sources to ensure cultural authenticity and accuracy. Its core innovation is the Practical Closure Architecture, designed to solve the ""Utility Gap Crisis"" where factually correct answers fail to resolve users' core needs, forcing them into frustrating cycles of re-prompting. By prioritizing clarity and decisive guidance, Mubeen transforms from an information repository into a decisive guide, aligning with Saudi Vision 2030. The model's architecture combines deep heritage specialization with multi-disciplinary expert modules, enabling robust performance across both cultural preservation and general knowledge domains."
2510.23270,Analysis of Hematocrit-Plasma Separation in a Trifurcated Microchannel by a Diffusive Flux Model,['physics.flu-dyn'],"['Rishi Kumar', 'Indranil Saha Dalal', 'K. Muralidhar']","Platelet-enriched plasma and red blood cells (RBC) are needed in the treatment of blood-related diseases, including anaemia and blood cancer. These essential components must be separated from blood in well-designed experimental setups. If active techniques are used, the blood components are likely to be damaged or contaminated while handling. Passive techniques for component separation are preferred, and their design for effectiveness before manufacturing is the subject of this article. Specifically, the performance of a design consisting of a trifurcated microchannel is examined in the framework of 3D numerical simulation, following similar design ideas in recent experimental studies. The influence of geometrical parameters of the channel, such as width and separation arm angle, inlet extension, flow constriction, and flow parameters, including flow rates, hematocrit concentration, and temperature, is studied. The present study utilizes the diffusive flux model (DFM) to model the shear-driven migration of red blood cells (RBC) in a microchannel along with an appropriate rheology model. The physical mechanism driving separation is the formation of the cell-free layer near the walls, using which the separation efficiency and device effectiveness are quantified. It is found that a microchannel with a smaller width and an extended inlet, along with diluted blood samples of lower hematocrit, is effective for greater separation, while the device performance is less sensitive to the flow rates, flow constriction, and the separator angle."
2510.23269,All-Altermagnetic Tunnel Junction of RuO2/NiF2/RuO2,"['cond-mat.mtrl-sci', 'physics.app-ph', 'physics.comp-ph']","['Long Zhang', 'Guoying Gao']","Emerging altermagnets offer a promising avenue for spintronics, yet their integration into magnetic tunnel junctions has been hindered by reliance on ferromagnetic electrodes (introducing stray fields) or limited functionality (non-tunable magnetoresistance without spin filtering). Here, we propose an all-altermagnetic tunnel junction (AAMTJ) paradigm composed exclusively of altermagnets-exemplified by experiment-feasible RuO2/NiF2/RuO2. Giant tunneling magnetoresistance of 11704%, and high spin-filtering of ~90% in both spin channels are achieved. This architecture unlocks tunable multistate magnetoresistance and spin filtering via magnetization control of electrode and barrier, stemming from their synergistic and antagonistic alignments of momentum-dependent altermagnetic spin-splitting. Our AAMTJ inherently exhibits low consumption and no stray field, with nonrelativistic spin splitting and zero magnetic moment, combining advantages of both ferromagnetic and antiferromagnetic tunnel junctions. This AAMTJ paradigm provides a realistically versatile platform to develop revolutionarily potential of altermagnets for reconfigurable magnetic memory devices."
2510.23268,Memory-controlled random bit generator,"['cond-mat.stat-mech', 'cond-mat.mes-hall', 'cond-mat.soft']","['Mateusz Wiśniewski', 'Jakub Spiechowicz']","Nowadays a bit is no longer a mere abstraction but a physical quantity whose manipulation governs both operation of modern technologies and theoretical frontiers of fundamental science. In this work we propose a setup in which the memory time can be utilized to control the generation and storage of binary information. In particular, we consider a nonequilibrium Brownian particle immersed in a viscoelastic environment and dwelling in a spatially periodic potential. We interpret its average velocity as a bit and show that depending on the memory time characterizing the viscoelastic bath the particle can be either in one of two stable states representing the bit values or in a chaotic state in which the information is erased and a new bit can be generated. We analyze randomness of the so obtained bit sequence and assess the stability of the produced values. Our study provides a blueprint for storing and processing information in a microscopic system using its memory."
2510.23267,The derived $\infty$-category of Frobenius modules,"['math.AG', 'math.AC']","['Klaus Mattis', 'Timo Weiß']","We prove that for $X$ a quasi-compact $\mathbb{F}_p$-scheme with affine diagonal (e.g.\ $X$ quasi-compact
  and separated) there is a t-exact equivalence
  $\mathcal D(\mathrm{Frob}(\mathrm{QCoh}(X),F_*)) \to \mathrm{Frob}(\mathcal D(\mathrm{QCoh}(X)),\mathcal D(F_*))$
  of stable $\infty$-categories.
  Here, $\mathrm{Frob}(-,-)$ denotes the $\infty$-category of generalized Frobenius modules
  as introduced in arXiv:2410.17102.
  This generalizes our result from arXiv:2410.17102,
  where we proved the above for regular Noetherian $\mathbb{F}_p$-schemes.
  As a byproduct we prove that the derived $\infty$-category of Frobenius (and Cartier) modules
  satisfies Zariski descent."
2510.23266,Existence and multiplicity results for the zero mass Schrödinger-Bopp-Podolsky system with critical growth,['math.AP'],"['Wentao Huang', 'Li Wang']","In this paper we study the following zero mass Schrödinger-Bopp-Podolsky system with critical growth \[ \begin{cases} -Δu +q^2φu=μ|u|^{p-2}u+|u|^4u\\ -Δφ+a^2Δ^2φ=4πu^2, \end{cases} \] where $a>0$, $q\neq0$, $μ>0$ is a parameter and $p\in(3,6)$. By introducing a new functional framework developed by Caponio et al. \cite{Cd}, we first establish the existence of positive ground state solutions for the case of $p\in(3,6)$. Moreover, for the case of $p\in(4,6)$, multiplicity results are obtained by applying an abstract critical point theorem due to Perera \cite{Pe}."
2510.23265,Minimal depth $K$-types for wild double covers and Shimura correspondences,['math.RT'],"['Edmund Karasiewicz', 'Shuichiro Takeda']","We construct some Iwahori types, in the sense of Bushnell-Kutzko, for the double cover of an almost simple simply-laced simply-connected Chevalley group $\Gt$ over any $2$-adic field. These types capture the covering group analog of the Bernstein block of unramified principal series.
  We also prove that the associated Hecke algebra essentially admits an Iwahori-Matsumoto (IM) presentation. The complete presentation is obtained for types $A_{r}$, $D_{2r+1}$, $E_{6}$, $E_{7}$; for the other types, some technical obstacles remain. Those Hecke algebras with the complete IM presentation are isomorphic to Iwahori-Hecke algebras of explicit linear Chevalley groups, giving rise to Shimura correspondences.
  Along the way, we show that the Iwahori type extends to a hyperspecial maximal compact subgroup $\Kt\subseteq \Gt$. This extension has minimal depth among the genuine $\Kt$-representations and allows us to construct a finite Shimura correspondence, generalizing a result of Savin."
2510.23264,PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization,"['cs.LG', 'cs.AI']","['Xinhai Wang', 'Shu Yang', 'Liangyu Wang', 'Lin Zhang', 'Huanyi Xie', 'Lijie Hu', 'Di Wang']","Circuit discovery, which involves identifying sparse and task-relevant subnetworks in pre-trained language models, is a cornerstone of mechanistic interpretability. Automated Circuit Discovery (ACDC) has emerged as a pivotal methodology in circuit discovery, but its application to large language models is severely limited by computational inefficiency and prohibitively high memory requirements. Although several accelerated approaches have been proposed, they primarily rely on linear approximations to ACDC, which significantly compromises analytical faithfulness. Our proposed method for accelerating automated circuit discovery, Per Attention Head Quantization (PAHQ), takes a fundamentally different approach by optimizing the efficiency of each individual patching operation. PAHQ leverages a fundamental alignment between activation patching and mixed-precision quantization (MPQ): interpretability analysis through patching essentially performs targeted ablation studies. Therefore, we can maintain high precision exclusively for investigated components while safely reducing precision elsewhere in the network. PAHQ-accelerated ACDC reduces runtime by up to 80\% and memory consumption by up to 30\% compared to unaccelerated ACDC while maintaining faithfulness. Importantly, our method readily integrates with existing edge-based circuit discovery techniques by modifying the attention computation mechanism. This training-free approach provides a practical and novel pathway for accelerating mechanistic interpretability methods. Our code is available at https://github.com/626619403/PAHQ."
2510.23263,Non-compact inaudibility of Naturally Reductive property,"['math.DG', 'math.SP']","['Teresa Arias-Marco', 'José-Manuel Fernández-Barroso']","Naturally reductive manifolds are an important class of Riemannian manifolds because they provide examples that generalize the locally symmetric ones. A property is said to be inaudible if there exists a unitary operator which intertwines the Laplace-Beltrami operator of two Riemannian manifolds such that one of them satisfies the property and the other does not.
  In this paper, we study the relation between 2-step nilpotent Lie groups and the naturally reductive property to prove that this property is inaudible, using a pair of non-compact 11-dimensional generalized Heisenberg groups."
2510.23262,Moderating Role of Presence in EEG Responses to Visuo-haptic Prediction Error in Virtual Reality,['cs.HC'],"['Lukas Gehrke', 'Leonie Terfurth', 'Klaus Gramann']","Virtual reality (VR) can create compelling experiences that evoke presence, the sense of ``being there.'' However, problems in rendering can create sensorimotor disruptions that undermine presence and task performance. Presence is typically assessed with post-hoc questionnaires, but their coarse temporal resolution limits insight into how sensorimotor disruptions shape user experience. Here, we combined questionnaires with electroencephalography (EEG) to identify neural markers of presence-affecting prediction error in immersive VR. Twenty-five participants performed a grasp-and-place task under two levels of immersion (visual-only vs.~visuo-haptic). Occasional oddball-like sensorimotor disruptions introduced premature feedback to elicit prediction errors. Overall, higher immersion enhanced self-presence but not physical presence, while accuracy and speed improved over time irrespective of immersion. At the neural level, sensorimotor disruptions elicited robust event-related potential effects at FCz and Pz, accompanied by increases in frontal midline $θ$ and posterior $α$ suppression. Through source analyses localized to anterior-- and posterior cingulate cortex (ACC/PCC) we found that PCC $α$ activity showed heightened sensitivity to disruptions exclusively in visuo-haptic immersion. Exploratory moderation analyses by presence scores revealed no consistent patterns. Together, these results suggest that higher immersion amplifies both the benefits and costs of sensorimotor coherence."
2510.23261,Toward Interpretable Evaluation Measures for Time Series Segmentation,['cs.LG'],"['Félix Chavelli', 'Paul Boniol', 'Michaël Thomazo']","Time series segmentation is a fundamental task in analyzing temporal data across various domains, from human activity recognition to energy monitoring. While numerous state-of-the-art methods have been developed to tackle this problem, the evaluation of their performance remains critically limited. Existing measures predominantly focus on change point accuracy or rely on point-based measures such as Adjusted Rand Index (ARI), which fail to capture the quality of the detected segments, ignore the nature of errors, and offer limited interpretability. In this paper, we address these shortcomings by introducing two novel evaluation measures: WARI (Weighted Adjusted Rand Index), that accounts for the position of segmentation errors, and SMS (State Matching Score), a fine-grained measure that identifies and scores four fundamental types of segmentation errors while allowing error-specific weighting. We empirically validate WARI and SMS on synthetic and real-world benchmarks, showing that they not only provide a more accurate assessment of segmentation quality but also uncover insights, such as error provenance and type, that are inaccessible with traditional measures."
2510.23260,The spatial distribution of dwarf and giant galaxies in and around Virgo cluster,['astro-ph.GA'],"['Nelvy Choque-Challapa', 'Rory Smith', 'Iván Lacerna', 'J. Alfonso L. Aguerri', 'Daniela Palma']","The Virgo cluster is one of the closest clusters to us where we can further study the evolution of galaxies, with several infalling substructures and several filaments around it have been reported. Therefore, it makes this cluster and its surrounding an interesting place to study the spatial distribution of the population of dwarf and bright giant galaxies. We analyse the dwarf fraction (DF) in different regions of the cluster, inside the virial radius, in its surrounding area, and in the filamentary structure surrounding it using available catalogues with the aim of measuring whether the DF changes in different environments. Although the total dwarf fraction within the cluster is $\sim$ 0.8, significant local variations are measured throughout the cluster; there are regions with a relatively higher concentration of giant or dwarf galaxies. The fact that Virgo is embedded in a rich environment surrounded by several filaments that feed the cluster with new substructures could imply changes in the DF locally. When we analyse the DF variation at further distances from the cluster we observe some regions with few or no giant galaxies at all, with a locally DF ranging from 0.8 - 1.0. Additionally, when comparing the dwarf fraction in different environments, overall, the DF is larger in regions further away from denser regions such as the Virgo cluster and its filamentary structure surrounding it. When comparing the filament and the cluster area, the dwarf fraction is slightly higher in the filaments, but from filament to filament, the DF changes depending on the presence of groups."
2510.23259,GCAO: Group-driven Clustering via Gravitational Attraction and Optimization,"['cs.LG', 'stat.ML']","['Qi Li', 'Jun Wang']","Traditional clustering algorithms often struggle with high-dimensional and non-uniformly distributed data, where low-density boundary samples are easily disturbed by neighboring clusters, leading to unstable and distorted clustering results. To address this issue, we propose a Group-driven Clustering via Gravitational Attraction and Optimization (GCAO) algorithm. GCAO introduces a group-level optimization mechanism that aggregates low-density boundary points into collaboratively moving groups, replacing the traditional point-based contraction process. By combining local density estimation with neighborhood topology, GCAO constructs effective gravitational interactions between groups and their surroundings, enhancing boundary clarity and structural consistency. Using groups as basic motion units, a gravitational contraction strategy ensures globally stable and directionally consistent convergence. Experiments on multiple high-dimensional datasets demonstrate that GCAO outperforms 11 representative clustering methods, achieving average improvements of 37.13%, 52.08%, 44.98%, and 38.81% in NMI, ARI, Homogeneity, and ACC, respectively, while maintaining competitive efficiency and scalability. These results highlight GCAO's superiority in preserving cluster integrity, enhancing boundary separability, and ensuring robust performance on complex data distributions."
2510.23258,Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation,"['cs.RO', 'cs.AI', 'cs.LG']","['Riko Yokozawa', 'Kentaro Fujii', 'Yuta Nomura', 'Shingo Murata']","Autonomous robotic navigation in real-world environments requires exploration to acquire environmental information as well as goal-directed navigation in order to reach specified targets. Active inference (AIF) based on the free-energy principle provides a unified framework for these behaviors by minimizing the expected free energy (EFE), thereby combining epistemic and extrinsic values. To realize this practically, we propose a deep AIF framework that integrates a diffusion policy as the policy model and a multiple timescale recurrent state-space model (MTRSSM) as the world model. The diffusion policy generates diverse candidate actions while the MTRSSM predicts their long-horizon consequences through latent imagination, enabling action selection that minimizes EFE. Real-world navigation experiments demonstrated that our framework achieved higher success rates and fewer collisions compared with the baselines, particularly in exploration-demanding scenarios. These results highlight how AIF based on EFE minimization can unify exploration and goal-directed navigation in real-world robotic settings."
2510.23257,Probing CP Violation through Vector Boson Fusion at High-Energy Muon Colliders,['hep-ph'],"['Qing-Hong Cao', 'Jian-Nan Ding', 'Yandong Liu', 'Jin-Long Yuan']","We investigate CP-violating effects in electroweak interactions at future high-energy muon colliders within the Standard Model Effective Field Theory (SMEFT) framework. Focusing on four dimension-six CP-odd operators -- $ \mathcal{O}_{\widetilde{W}}, \mathcal{O}_{H\widetilde{W}}, \mathcal{O}_{H\widetilde{W}B}, \mathcal{O}_{H\widetilde{B}}$ -- we analyze vector boson fusion production of $W$ and Higgs bosons using CP-odd observables and their asymmetries. With detailed simulations including parton showering, hadronization, and detector effects, we derive exclusion sensitivities through a binned likelihood analysis. For example, at $\sqrt{s} = 3$ TeV with 2 ab$^{-1}$, the coefficient $C_{\widetilde{W}}$ can be constrained at the $\mathcal{O}(0.02)$ level, improving to $\mathcal{O}(0.008)$ at 10 TeV with 2 ab$^{-1}$, and $\mathcal{O}(0.003)$ with 10 ab$^{-1}$. These results significantly surpass current LHC and projected ILC sensitivities, demonstrating the unique potential of high-energy muon colliders to provide direct and model-independent probes of CP violation in the electroweak sector."
2510.23256,Invariant measures on the space of measured laminations for subgroups of mapping class group,"['math.DS', 'math.GR', 'math.GT']","['Inhyeok Choi', 'Dongryul M. Kim']","For a non-elementary subgroup of the mapping class group of a surface, we study its invariant Radon measures on the space of measured laminations, by classifying them on the recurrent measured laminations. In particular, given a divergence-type subgroup, we show the uniquely ergodic by explicitly constructing the ergodic measure. This generalizes Lindenstrauss--Mirzakhani's result and Hamenstädt's result for the full mapping class group, in which case the ergodic measure is the Thurston measure. As a special case, we deduce that for a convex cocompact subgroup, every invariant ergodic Radon measure on the space of all measured laminations is either the unique measure on recurrent measured laminations, or a counting measure on the orbit of a non-recurrent measured lamination.
  Our method is geometric and does not rely on continuous or homogeneous flows on the ambient space or a dynamical system associated with a finite measure space. This leads to a unifying approach for various metric spaces, including Teichmüller spaces and partially $\operatorname{CAT}(-1)$ spaces."
2510.23255,On the topology of the limit set of non-autonomous IFS,"['math.DS', 'math.GT', 'math.PR']","['Yuto Nakajima', 'Takayuki Watanabe']","Fractals are ubiquitous in nature, and since Mandelbrot's seminal insight into their structure, there has been growing interest in them. While the topological properties of the limit sets of IFSs have been studied -- notably in the pioneering work of Hata -- many aspects remain poorly understood, especially in the non-autonomous setting. In this paper, we present a homological framework which captures the structure of the limit set. We apply our novel abstract theory to the concrete analysis of the so-called fractal square, and provide an answer to a variant of Mandelbrot's percolation problem. This work offers new insights into the topology of fractals."
2510.23254,Provable test-time adaptivity and distributional robustness of in-context learning,"['stat.ML', 'cs.LG', 'math.ST']","['Tianyi Ma', 'Tengyao Wang', 'Richard J. Samworth']","We study in-context learning problems where a Transformer is pretrained on tasks drawn from a mixture distribution $π=\sum_{α\in\mathcal{A}} λ_α π_α$, called the pretraining prior, in which each mixture component $π_α$ is a distribution on tasks of a specific difficulty level indexed by $α$. Our goal is to understand the performance of the pretrained Transformer when evaluated on a different test distribution $μ$, consisting of tasks of fixed difficulty $β\in\mathcal{A}$, and with potential distribution shift relative to $π_β$, subject to the chi-squared divergence $χ^2(μ,π_β)$ being at most $κ$. In particular, we consider nonparametric regression problems with random smoothness, and multi-index models with random smoothness as well as random effective dimension. We prove that a large Transformer pretrained on sufficient data achieves the optimal rate of convergence corresponding to the difficulty level $β$, uniformly over test distributions $μ$ in the chi-squared divergence ball. Thus, the pretrained Transformer is able to achieve faster rates of convergence on easier tasks and is robust to distribution shift at test time. Finally, we prove that even if an estimator had access to the test distribution $μ$, the convergence rate of its expected risk over $μ$ could not be faster than that of our pretrained Transformers, thereby providing a more appropriate optimality guarantee than minimax lower bounds."
2510.23253,A Video Is Not Worth a Thousand Words,['cs.CV'],"['Sam Pollard', 'Michael Wray']","As we become increasingly dependent on vision language models (VLMs) to answer questions about the world around us, there is a significant amount of research devoted to increasing both the difficulty of video question answering (VQA) datasets, and the context lengths of the models that they evaluate. The reliance on large language models as backbones has lead to concerns about potential text dominance, and the exploration of interactions between modalities is underdeveloped. How do we measure whether we're heading in the right direction, with the complexity that multi-modal models introduce? We propose a joint method of computing both feature attributions and modality scores based on Shapley values, where both the features and modalities are arbitrarily definable. Using these metrics, we compare $6$ VLM models of varying context lengths on $4$ representative datasets, focusing on multiple-choice VQA. In particular, we consider video frames and whole textual elements as equal features in the hierarchy, and the multiple-choice VQA task as an interaction between three modalities: video, question and answer. Our results demonstrate a dependence on text and show that the multiple-choice VQA task devolves into a model's ability to ignore distractors. Code available at https://github.com/sjpollard/a-video-is-not-worth-a-thousand-words."
2510.23252,Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?,['cs.CL'],"['Tawsif Tashwar Dipto', 'Azmol Hossain', 'Rubayet Sabbir Faruque', 'Md. Rezuwan Hassan', 'Kanij Fatema', 'Tanmoy Shome', 'Ruwad Naswan', 'Md. Foriduzzaman Zihad', 'Mohaymen Ul Anam', 'Nazia Tasnim', 'Hasan Mahmud', 'Md Kamrul Hasan', 'Md. Mehedi Hasan Shawon', 'Farig Sadeque', 'Tahsin Reasat']","Conventional research on speech recognition modeling relies on the canonical form for most low-resource languages while automatic speech recognition (ASR) for regional dialects is treated as a fine-tuning task. To investigate the effects of dialectal variations on ASR we develop a 78-hour annotated Bengali Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and data-driven perspectives shows that speech foundation models struggle heavily in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe that all deep learning methods struggle to model speech data under dialectal variations but dialect specific model training alleviates the issue. Our dataset also serves as a out of-distribution (OOD) resource for ASR modeling under constrained resources in ASR algorithms. The dataset and code developed for this project are publicly available"
2510.23251,Design principles for amorphous solid-state electrolytes,['cond-mat.mtrl-sci'],"['Qifan Yang', 'Xiao Fu', 'Xuhe Gong', 'Jingchen Lian', 'Liqi Wang', 'Ruijuan Xiao', 'Yong-Sheng Hu', 'Hong Li']","Amorphous solid-state electrolytes (SSEs) offer unique advantages for next-generation batteries, but their rational design is hindered by an unclear structure-property relationship. This study establishes universal design principles through atomistic simulations of 32 amorphous Li-M-X systems (M = B, Al, Si, P; X = F, Cl, Br, I, O, S, Se, N). We identify four structure types governed by a rule that saturated M-X groups with more negative charges preferentially form M-X-M chains, identify paddle-wheel and cooperative migration as two favorable transport mechanisms that are significantly enhanced in amorphous structures. We also pinpoint Oxides and fluorides as optimal for electrochemical and hydrolytic stability, and reveal bulk modulus as a simple predictor for $Li^+$ mobility. These insights are integrated into a practical design diagram, providing a novel and valuable framework for advancing high-performance amorphous SSEs."
2510.23250,Modelling Fluid--Structure Interaction in an Initially Elliptical Elastic-Walled tube: Improved Onset Criterion for Self-Excited Oscillations,"['physics.flu-dyn', 'physics.app-ph']","['Daniel J. Netherwood', 'Robert J. Whittaker']","We present a theoretical description of the fluid--structure interaction observed within a Starling resistor. The typical setup consists of a pre-stretched finite length thin-walled elastic tube mounted between two rigid tubes. The collapsible section is enclosed within a pressure chamber and a viscous fluid is driven through the system by imposing an axial volume flux at the downstream end. Valid within a long-wavelength thin-walled regime, we use our own results to model the wall mechanics. These results arise from the solution of a generalised eigenvalue problem, and avoid the need to invoke the ad-hoc approximations made in previous studies. The wall mechanics are then coupled to the fluid mechanics using the Navier--Stokes equations, under the assumption that the oscillations in the tube wall are of small amplitude, long wavelength and high frequency. We derive problems governing the leading-order steady and oscillatory fluid-structure interaction. At leading order, the system permits normal-mode oscillations of constant frequency and amplitude, which are obtained in the form of series solutions. Higher-order corrections govern the slow growth or decay of the oscillations, however (as in previous work) these growth rates can be determined by analysing the system's global energy budget without needing to compute the higher-order terms explicitly. Our results permit the first formal analysis of the errors incurred by neglecting contributions from higher-order azimuthal modes, and enable the determination of improved criterion for the onset of self-excited oscillations in the tube wall."
2510.23249,On Conjectures concerning the Labeled Coupon Collector Problem,"['math.PR', 'math.CO']","['Dina Barak-Pelleg', 'Daniel Berend']","We study a labeled variant of the classical Coupon Collector Problem (CCP), recently introduced by Tan et al., where coupons arrive in groups and only the set of labels is revealed. The goal is to determine the expected number of group drawings required to uniquely identify the labeling of all coupons. We focus on the case where groups consist of pairs ($k=2$), and provide rigorous proofs for two conjectures posed by Tan et al."
2510.23248,The Generalized Second Law and the Spatial Curvature Index,['gr-qc'],['Diego Pavon'],"By applying the generalized second law to the apparent horizon of a homogeneous and isotropic universe and imposing that the equation of state is no less than $-1$, it is seen that universes with either flat or closed spatial sections are consistent with the joint consideration of the aforesaid law and the dominant energy condition, but not so universes with hyperbolic spatial sections"
2510.23247,Chaos in Systems with Quantum Group Symmetry,"['hep-th', 'cond-mat.stat-mech', 'cond-mat.str-el', 'quant-ph']","['Victor Gorbenko', 'Aleksandr Zhabin']","Quantum groups have a long and fruitful history of applications in integrable systems. Can quantum group symmetries exist in the absence of integrability? We provide an explicit example of a system with quantum group global symmetry which is chaotic. The example is a spin chain with next-to-nearest interaction term. We show the chaotic behavior of the system by studying the Eigenvalue statistics. The spin chain is non-unitary but PT-symmetric and, in addition to chaos, exhibits an interesting transition after which the eigenvalues become complex."
2510.23246,Fine details in solar flare ribbons: Statistical insights from observations with the Swedish 1-m Solar Telescope,['astro-ph.SR'],"['Jonas Thoen Faber', 'Reetika Joshi', 'Luc Rouppe van der Voort', 'Sven Wedemeyer', 'Eilif Sommer Øyre', 'Ignasi J. Soler Poquet', 'Aline Rangøy Brunvoll']","Flare ribbons serve as chromospheric footprints of energy deposition resulting from particle acceleration during magnetic reconnection. Their fine-scale structure provides a valuable tool for probing the dynamics of the flare reconnection process. Our goal is to investigate the fine-scale structure of flare ribbons through multiple observations of flares, utilising data obtained from the Atmospheric Imaging Assembly (AIA) and the Swedish 1-m Solar Telescope (SST). The aligned AIA and SST datasets for the three solar flares were used to examine their overall morphology. The SST datasets were specifically used to identify fine-scale structures within the flare ribbons. For spectroscopic analysis of these fine structures, we applied machine-learning methods (k-means clustering) and Gaussian fitting. Using k-means, we identified elongated features in the flare ribbons, termed as ""riblets"", which are short-lived and jet-like small-scale structures that extend as plasma columns from the flare ribbons. Riblets are more prominent near the solar limb and represent the ribbon front. Riblet widths are consistent across observations, ranging from 110-310 km (0"".15-0"".41), while vertical lengths span 620-1220 km (0"".83-1"".66), with a potential maximum of 2000 km (2"".67), after accounting for projection effects. Detailed H-beta spectral analysis reveals that riblets exhibit a single, redshifted emission component, with velocities of 16-21 km s^1, independent of viewing angle. Our high-resolution observations of the three flare ribbons show that they are not continuous structures, but are composed of vertically extended, fine-scale substructures. These irregular features indicate that the reconnection region is not a smooth, laminar current sheet, but rather a fragmented zone filled with magnetic islands, consistent with the theory of patchy reconnection within the coronal current sheet."
2510.23245,Multi-Stakeholder Alignment in LLM-Powered Collaborative AI Systems: A Multi-Agent Framework for Intelligent Tutoring,"['cs.HC', 'cs.MA']","['Alexandre P Uchoa', 'Carlo E T Oliveira', 'Claudia L R Motta', 'Daniel Schneider']","The integration of Large Language Models into Intelligent Tutoring Systems pre-sents significant challenges in aligning with diverse and often conflicting values from students, parents, teachers, and institutions. Existing architectures lack for-mal mechanisms for negotiating these multi-stakeholder tensions, creating risks in accountability and bias. This paper introduces the Advisory Governance Layer (AGL), a non-intrusive, multi-agent framework designed to enable distributed stakeholder participation in AI governance. The AGL employs specialized agents representing stakeholder groups to evaluate pedagogical actions against their spe-cific policies in a privacy-preserving manner, anticipating future advances in per-sonal assistant technology that will enhance stakeholder value expression. Through a novel policy taxonomy and conflict-resolution protocols, the frame-work provides structured, auditable governance advice to the ITS without altering its core pedagogical decision-making. This work contributes a reference architec-ture and technical specifications for aligning educational AI with multi-stakeholder values, bridging the gap between high-level ethical principles and practical implementation."
2510.23244,Over four minutes relaxation of pyruvate using chemically and physically induced deceleration of relaxation,"['physics.chem-ph', 'physics.med-ph']","['Josh P. Peters', 'Florin Teleanu', 'Huijing Zou', 'Ehtisham Rasool', 'Jan-Bernd Hövener', 'Alexej Jerschow', 'Andrey N. Pravdivtsev']","[1-13C]pyruvate is the most widely used tracer for hyperpolarized metabolic magnetic resonance imaging, with profound applications in tumor and inflammation diagnosis as well as treatment monitoring. The most fundamental hurdle to broader application, however, remains the rapid polarization relaxation and the associated signal loss. Here, we report a method to address this challenge. Studying the nuclear spin relaxation dispersion of [1-13C]pyruvate across magnetic fields from 8 μT to 9.4 T, as a function of additives, solvents, and preparation methods, allowed us to achieve relaxation times of up to four minutes. Such a long time could enable reliable quality control and nearly polarization loss-free transport, further boosting the power of hyperpolarized metabolic MRI."
2510.23243,Classification results for bounded positive solutions to the critical $p$-Laplace equation,['math.AP'],"['Giulio Ciraolo', 'Michele Gatti']","By providing optimal or nearly optimal integral estimates, we show that every positive, bounded or moderately growing, local weak solution to the critical $p$-Laplace equation in $\mathbb{R}^n$, with $n\geq 3$, must be a bubble."
2510.23242,Automated Exploration of Radical-Molecule Chemistry: The Case of Oxirane + CH in the ISM,"['astro-ph.GA', 'astro-ph.IM', 'physics.chem-ph', 'physics.comp-ph']","['Moritz Bensberg', 'Silvia Alessandrini', 'Mattia Melosso', 'Cristina Puzzarini', 'Markus Reiher']","Quantum chemistry provides accurate and reliable methods to investigate reaction pathways of reactive molecular systems relevant to the interstellar medium. However, the exhaustive exploration of a reactive network is often a daunting task, resulting in unexplored reactive channels that affect kinetic outcomes and branching ratios. Here, an automated workflow for exploring reactive potential energy surfaces (PESs) is employed for the first time to study the oxirane (C$_2$H$_4$O) plus methylidyne ($^.$CH) reaction. The ultimate goal is to comprehensively map its PES and, subsequently, derive rate constants for the most important reaction channels. In addition to its astrochemical relevance, this reaction has been considered because it is a challenging test case, its network being very extended, with 60 exothermic bimolecular products lying below the reactant's energy. Kinetic simulations indicate that the main product of the reaction is the HCO radical plus ethene (C$_2$H$_4$), while formation of s-trans-propenal (acrolein) and 2H-oxene is also possible, but to a lesser extent. Based on the present study and other references in the literature, we suggest that the slightly higher relative abundance of s-trans-propenal compared to methyl ketene in the interstellar medium is a gas-phase kinetic effect, s-trans-propenal being a more easily accessible product on the C$_3$H$_5$O$^.$ PES."
2510.23241,Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation,"['cs.CV', 'cs.AI', 'cs.LG']","['Stefan M. Fischer', 'Johannes Kiechle', 'Laura Daza', 'Lina Felsner', 'Richard Osuala', 'Daniel M. Lang', 'Karim Lekadir', 'Jan C. Peeken', 'Julia A. Schnabel']","In this work, we introduce Progressive Growing of Patch Size, an automatic curriculum learning approach for 3D medical image segmentation. Our approach progressively increases the patch size during model training, resulting in an improved class balance for smaller patch sizes and accelerated convergence of the training process. We evaluate our curriculum approach in two settings: a resource-efficient mode and a performance mode, both regarding Dice score performance and computational costs across 15 diverse and popular 3D medical image segmentation tasks. The resource-efficient mode matches the Dice score performance of the conventional constant patch size sampling baseline with a notable reduction in training time to only 44%. The performance mode improves upon constant patch size segmentation results, achieving a statistically significant relative mean performance gain of 1.28% in Dice Score. Remarkably, across all 15 tasks, our proposed performance mode manages to surpass the constant patch size baseline in Dice Score performance, while simultaneously reducing training time to only 89%. The benefits are particularly pronounced for highly imbalanced tasks such as lesion segmentation tasks. Rigorous experiments demonstrate that our performance mode not only improves mean segmentation performance but also reduces performance variance, yielding more trustworthy model comparison. Furthermore, our findings reveal that the proposed curriculum sampling is not tied to a specific architecture but represents a broadly applicable strategy that consistently boosts performance across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In summary, we show that this simple yet elegant transformation on input data substantially improves both Dice Score performance and training runtime, while being compatible across diverse segmentation backbones."
2510.23240,"Autoregressive Styled Text Image Generation, but Make it Reliable",['cs.CV'],"['Carmine Zaccagnino', 'Fabio Quattrini', 'Vittorio Pippi', 'Silvia Cascianelli', 'Alessio Tonioni', 'Rita Cucchiara']","Generating faithful and readable styled text images (especially for Styled Handwritten Text generation - HTG) is an open problem with several possible applications across graphic design, document understanding, and image editing. A lot of research effort in this task is dedicated to developing strategies that reproduce the stylistic characteristics of a given writer, with promising results in terms of style fidelity and generalization achieved by the recently proposed Autoregressive Transformer paradigm for HTG. However, this method requires additional inputs, lacks a proper stop mechanism, and might end up in repetition loops, generating visual artifacts. In this work, we rethink the autoregressive formulation by framing HTG as a multimodal prompt-conditioned generation task, and tackle the content controllability issues by introducing special textual input tokens for better alignment with the visual ones. Moreover, we devise a Classifier-Free-Guidance-based strategy for our autoregressive model. Through extensive experimental validation, we demonstrate that our approach, dubbed Eruku, compared to previous solutions requires fewer inputs, generalizes better to unseen styles, and follows more faithfully the textual prompt, improving content adherence."
2510.23239,Mean curvature flow into an ambient Riemannian manifold evolving by Ricci flow coupled with harmonic map heat flow,['math.DG'],"['José N. V. Gomes', 'Matheus Hudson', 'Carlos M. de Sousa']","The main objective of this article is to study the mean curvature flow into an ambient compact smooth manifold M with boundary and with a Riemannian metric that evolves by a self-similar solution of the Ricci flow coupled with the harmonic map heat flow of a map from M to a Riemannian manifold N. In this context, we address a functional associated with this flow and calculate its variation along parameters that preserve the weighted volume measure. An extension of Hamilton's differential Harnack expression appears by considering the boundary of M evolving by mean curvature flow, which must vanish on the gradient steady soliton case. Next, we obtain a Huisken monotonicity-type formula for the mean curvature flow in the proposed background. We also show how to construct a family of mean curvature solitons and establish a characterization of such a family."
2510.23238,"Lensing, not luck! Detection prospects of strongly lensed gravitational waves","['gr-qc', 'astro-ph.CO']","['A. Barsode', 'K. N. Maity', 'P. Ajith']","A small fraction of gravitational-wave (GW) signals detected by ground-based observatories will be strongly lensed by intervening galaxies or clusters. This may produce multiple copies of the signals (i.e., lensed images) arriving at different times at the detector. These, if observed, could offer new probes of astrophysics and cosmology. However, identification of lensed image pairs among a large number of unrelated GW events is challenging. Though the number of lensed events increases with improved detector sensitivity, the false alarms increase quadratically faster. While this ""lensing or luck"" problem would appear to be insurmountable, we show that the expected increase in measurement precision of source parameters will efficiently weed out false alarms. Based on current astrophysical models and anticipated sensitivities, we predict that the first confident detection could occur in the fifth observing run of LIGO, Virgo, and KAGRA. We expect computational costs to be a major hurdle in achieving such a detection, and show that the Posterior Overlap 2.0 method may offer a near-optimal solution to this challenge."
2510.23237,Robust Iterative Learning Hidden Quantum Markov Models,"['cs.LG', 'quant-ph', 'stat.CO', 'stat.ME', 'stat.ML']",['Ning Ning'],"Hidden Quantum Markov Models (HQMMs) extend classical Hidden Markov Models to the quantum domain, offering a powerful probabilistic framework for modeling sequential data with quantum coherence. However, existing HQMM learning algorithms are highly sensitive to data corruption and lack mechanisms to ensure robustness under adversarial perturbations. In this work, we introduce the Adversarially Corrupted HQMM (AC-HQMM), which formalizes robustness analysis by allowing a controlled fraction of observation sequences to be adversarially corrupted. To learn AC-HQMMs, we propose the Robust Iterative Learning Algorithm (RILA), a derivative-free method that integrates a Remove Corrupted Rows by Entropy Filtering (RCR-EF) module with an iterative stochastic resampling procedure for physically valid Kraus operator updates. RILA incorporates L1-penalized likelihood objectives to enhance stability, resist overfitting, and remain effective under non-differentiable conditions. Across multiple HQMM and HMM benchmarks, RILA demonstrates superior convergence stability, corruption resilience, and preservation of physical validity compared to existing algorithms, establishing a principled and efficient approach for robust quantum sequential learning."
2510.23236,A platform for zero-field isolated skyrmions: 4$d$/Co atomic bilayers on Re(0001),['cond-mat.mtrl-sci'],"['Moinak Ghosh', 'Stefan Heinze', 'Souvik Paul']","Using first-principles density functional theory (DFT) combined with atomistic spin simulations, we explore the possibility of realizing zero-field isolated skyrmions in three 4$d$/Co atomic bilayers -- Rh/Co, Pd/Co, and Ru/Co -- grown on the Re(0001) surface. Our investigation employs an extended atomistic spin model, which goes beyond the standard model by including the multi-spin higher-order exchange interactions (HOI) in addition to the Heisenberg pairwise exchange interaction, Dzyaloshinskii-Moriya interaction (DMI), and magnetocrystalline anisotropy energy (MAE). All magnetic interactions of the extended spin model are calculated using DFT. The phase diagram obtained from atomistic spin simulations based on this spin model for Rh/Co and Pd/Co on Re(0001) reveals that isolated skyrmions emerge spontaneously on the ferromagnetic background even in the absence of an external magnetic field. The radius of zero-field isolated skyrmions in Rh/Co/Re(0001) is around 6 nm, whereas the radius of those skyrmions in Pd/Co/Re(0001) is about 12 nm. Transition-state theory calculations show that the skyrmions are protected by substantial energy barriers, approximately 150 meV, which predominantly arise from DMI, with a small contribution from the HOI interactions. The height of the barriers suggests that skyrmions could be observed in low-temperature experiments. Based on this work, we propose 4$d$/Co bilayers on Re(0001) as a new platform to realize nanoscale zero-field isolated skyrmions."
2510.23235,Grassmanian Interpolation of Low-Pass Graph Filters: Theory and Applications,"['cs.LG', 'cs.SI', 'eess.SP', 'math.NA', 'math.SP']","['Anton Savostianov', 'Michael T. Schaub', 'Benjamin Stamm']","Low-pass graph filters are fundamental for signal processing on graphs and other non-Euclidean domains. However, the computation of such filters for parametric graph families can be prohibitively expensive as computation of the corresponding low-frequency subspaces, requires the repeated solution of an eigenvalue problem. We suggest a novel algorithm of low-pass graph filter interpolation based on Riemannian interpolation in normal coordinates on the Grassmann manifold. We derive an error bound estimate for the subspace interpolation and suggest two possible applications for induced parametric graph families. First, we argue that the temporal evolution of the node features may be translated to the evolving graph topology via a similarity correction to adjust the homophily degree of the network. Second, we suggest a dot product graph family induced by a given static graph which allows to infer improved message passing scheme for node classification facilitated by the filter interpolation."
2510.23234,Optimal Dimensioning of Elastic-Link Manipulators regarding Lifetime Estimation,['cs.RO'],"['Klaus Zauner', 'Hubert Gattringer', 'Andreas Mueller']","Resourceful operation and design of robots is key for sustainable industrial automation. This will be enabled by lightweight design along with time and energy optimal control of robotic manipulators. Design and control of such systems is intertwined as the control must take into account inherent mechanical compliance while the design must accommodate the dynamic requirements demanded by the control. As basis for such design optimization, a method for estimating the lifetime of elastic link robotic manipulators is presented. This is applied to the geometry optimization of flexible serial manipulators performing pick-and-place operations, where the optimization objective is a combination of overall weight and vibration amplitudes. The lifetime estimation draws from a fatigue analysis combining the rainflow counting algorithm and the method of critical cutting plane. Tresca hypothesis is used to formulate an equivalent stress, and linear damage accumulation is assumed. The final robot geometry is selected from a Pareto front as a tradeoff of lifetime and vibration characteristic. The method is illustrated for a three degrees of freedom articulated robotic manipulator."
2510.23233,Partition analysis and the little Göllnitz identites,"['math.CO', 'math.NT']",['Runqiao Li'],"This work follows the spirit of Andrews' series of papers on Partition Analysis. In $2011$, Savage and Sills found new sum sides for the little Göllnitz identities and provided their partition interpretations. It turns out that similar companions exist for a mod $8$ partition identity due to Andrews. In this work, we use MacMahon's Partition Analysis to study partitions related to these identities. We find refined generating functions for them, where we keep track of the size of each part. Finally, by considering the alternating sum and Schmidt weight, we show the application of these refined functions in the study of partition statistics."
2510.23232,"Fluctuations, Clustering, and Interaction-Driven Dynamics in Sedimenting Particles at Low Galileo Numbers: A Neural Network Approach",['physics.flu-dyn'],"['Nejc Vovk', 'Jana Wedel', 'Paul Steinmann', 'Jure Ravnik']","In this study, we investigate the behaviour of sedimenting solid particles and the influence of microscopic particle dynamics on the collective motion of a sedimenting cloud. Departing from conventional direct numerical simulations (DNS), we introduce a novel machine learning framework, the Interaction-Decomposed Neural Network (IDNN), to model hydrodynamic particle interactions. The IDNN acts as a black-box module within a Lagrangian solver, predicting the particle drag force based on the relative positions of the nearest neighbours. This enables the recovery of force fluctuations, capturing effects previously accessible only through DNS. Our results show an increase in collective settling velocity in the dilute regime, consistent with earlier experimental and numerical studies, which we attribute to (i) fluctuations in the streamwise particle force around a value that is lower than the Stokes limit and (ii) the formation of particle clusters sedimenting at enhanced velocities. These fluctuations originate from persistent entrainment and ejection of particles in and out of the long, diffusive wakes generated by upstream particles at low Galileo numbers. Energy spectra of particle velocity fluctuations reveal a scale-dependent transfer of fluctuation energy, analogous to a turbulent-like cascade, with pronounced large-scale fluctuations at higher volume fractions. At very low volume fractions, fluctuation intensity and energy spectrum amplitudes diminish, though hydrodynamic interactions still remain appreciable."
2510.23231,Stability analysis of discontinuous Galerkin with a high order embedded boundary treatment for linear hyperbolic equations,['math.NA'],['Mirco Ciallella'],"Embedded, or immersed, approaches have the goal of reducing to the minimum the computational costs associated with the generation of body-fitted meshes by only employing fixed, possibly Cartesian, meshes over which complex boundaries can move freely. However, this boundary treatment introduces a geometrical error of the order of the mesh size that, if not treated properly, can spoil the global accuracy of a high order discretization, herein based on discontinuous Galerkin. The shifted boundary polynomial correction was proposed as a simplified version of the shifted boundary method, which is an embedded boundary treatment based on Taylor expansions to deal with unfitted boundaries. It is used to accordingly correct the boundary conditions imposed on a non-meshed boundary to compensate the aforementioned geometrical error, and reach high order accuracy. In this paper, the stability analysis of discontinuous Galerkin methods coupled with the shifted boundary polynomial correction is conducted in depth for the linear advection equation, by visualizing the eigenvalue spectrum of the high order discretized operators. The analysis considers a simplified one-dimensional setting by varying the degree of the polynomials and the distance between the real boundary and the closest mesh interface. The main result of the analysis shows that the considered high order embedded boundary treatment introduces a limitation to the stability region of high order discontinuous Galerkin methods with explicit time integration, which becomes more and more important when using higher order methods. The implicit time integration is also studied, showing that the implicit treatment of the boundary condition allows one to overcome such limitation and achieve an unconditionally stable high order embedded boundary treatment."
2510.23230,"On the use of information fusion techniques to improve information quality: Taxonomy, opportunities and challenges","['cs.IT', 'doi', '10.1016/j.inffus.2021.09.017']","['Raúl Gutiérrez', 'Víctor Rampérez', 'Horacio Paggi', 'Juan A. Lara', 'Javier Soriano']","The information fusion field has recently been attracting a lot of interest within the scientific community, as it provides, through the combination of different sources of heterogeneous information, a fuller and/or more precise understanding of the real world than can be gained considering the above sources separately. One of the fundamental aims of computer systems, and especially decision support systems, is to assure that the quality of the information they process is high. There are many different approaches for this purpose, including information fusion. Information fusion is currently one of the most promising methods. It is particularly useful under circumstances where quality might be compromised, for example, either intrinsically due to imperfect information (vagueness, uncertainty) or because of limited resources (energy, time). In response to this goal, a wide range of research has been undertaken over recent years. To date, the literature reviews in this field have focused on problem-specific issues and have been circumscribed to certain system types. Therefore, there is no holistic and systematic knowledge of the state of the art to help establish the steps to be taken in the future. In particular, aspects like what impact different information fusion methods have on information quality, how information quality is characterised, measured and evaluated in different application domains depending on the problem data type or whether fusion is designed as a flexible process capable of adapting to changing system circumstances and their intrinsically limited resources have not been addressed. This paper aims precisely to review the literature on research into the use of information fusion techniques specifically to improve information quality, analysing the above issues in order to identify a series of challenges and research directions, which are presented in this paper."
2510.23229,Validation of field cage and cathode for low radioactivity operation with the CYGNO experiment,"['physics.ins-det', 'astro-ph.IM', 'hep-ex']","['F. D. Amaro', 'R. Antonietti', 'E. Baracchini', 'L. Benussi', 'S. Bianco', 'A. Biondi', 'C. Capoccia', 'M. Caponero', 'L. G. M. de Carvalho', 'G. Cavoto', 'I. A. Costa', 'A. Croce', ""M. D'Astolfo"", ""G. D'Imperio"", 'E. Danè', 'G. Dho', 'E. Di Marco', 'J. M. F. dos Santos', 'D. Fiorina', 'F. Iacoangeli', 'Z. Islam', 'E. Kemp', 'H. P. Lima Jr', 'G. Maccarrone', 'R. D. P. Mano']","Dark matter, which is considered to account for approximately the 27% of the Universe's energy-mass content, remains an open issue in modern particle physics along with its composition. The CYGNO Experiment aims to exploit an innovative approach applied to the direct detection search of low energy nuclear recoils possibly induced by cold particle-like dark matter candidates. CYGNO employs a directional detector based on a Time Projection Chamber (TPC) filled with a He:CF$_{4}$ gas mixture and equipped with an optical readout. Currently, the CYGNO Collaboration is constructing the detector demonstrator, CYGNO-04, in Hall F at Laboratori Nazionali del Gran Sasso (LNGS). This 0.4 m$^3$ detector has the goal of proving the scalability of the technology and assessing the physics and radiopurity capabilities. Given the low radioactivity requirements, especially in internal components such as field cage and cathode, the reduction of material while keeping the correct electrical behavior is paramount. In this paper, we present the validation of several internal components, mainly focusing on the field cage material and support structure. The tests included geometrical asymmetries in the electric field response, collection efficiency as well as measurement of known physical quantities. A preferred configuration is found with a structure based on Nylon material which supports a PET or Kapton sheet with copper strips deposited on."
2510.23228,Spoofing resilience for simple-detection quantum illumination LIDAR,"['quant-ph', 'physics.optics']","['Richard J. Murchie', 'John Jeffers']","Object detection and range finding using a weak light source is vulnerable to jamming and spoofing attacks by an intruder. Quantum illumination with nonsimultaneous, phase-insensitive coincidence measurements can provide jamming resilience compared to identical measurements for classical illumination. We extend an experimentally-feasible object detection and range finding quantum illumination-based protocol to include spoofing resilience. This approach allows the system to be characterised by its experimental parameters and quantum states, rather than just its detector data. Therefore we can scope the parameter-space which provides some spoofing resilience without relying upon the prohibitive method of acquiring detector data for all combinations of the experimental parameters. We demonstrate that in certain regimes the intruder has an optimal relative detection basis angle to minimise the induced error. We also show that there are spoofing-vulnerable regimes where excessive background noise prevents any induced error, while it is still possible to perform object detection, i.e. our detectors have not been fully blinded. The sensing protocol which we describe can allow for the recognition of intrusion and the possible detection of our trustworthy return signal. Our results reinforce that quantum illumination is advantageous for spoofing resilience compared to a classical illumination-based protocol."
2510.23227,Workspace Registration and Collision Detection for Industrial Robotics Applications,['cs.RO'],"['Klaus Zauner', 'Josef El Dib', 'Hubert Gattringer', 'Andreas Mueller']","Motion planning for robotic manipulators relies on precise knowledge of the environment in order to be able to define restricted areas and to take collision objects into account. To capture the workspace, point clouds of the environment are acquired using various sensors. The collision objects are identified by region growing segmentation and VCCS algorithm. Subsequently the point clusters are approximated. The aim of the present paper is to compare different sensors, to illustrate the process from detection to the finished collision environment and to detect collisions between the robot and this environment."
2510.23226,Inertia Partitioning Modular Control Framework for Reconfigurable Multibody Systems,['eess.SY'],"['Mohammad Dastranj', 'Jouni Mattila']","A novel modular control framework for reconfigurable rigid multibody systems is proposed, motivated by the challenges of modular control of systems with closed kinematic chains. In the framework, modularity is defined in the sense of degrees of freedom, and the inertial properties of each body are partitioned with respect to how they are reflected in the kinetic energy of the system through the motion induced by each degree of freedom. This approach inherently handles closed chains in the same manner as tree-like structures, eliminating the need for explicit constraint force calculations or formulations based on differential-algebraic equations. The proposed framework is implemented via simulation on a three-degree-of-freedom series-parallel manipulator, with the results being consistent with the expected stability and tracking performance, and indicating the framework's potential for scalability in trajectory-tracking control of multibody systems."
2510.23225,Through the Lens: Benchmarking Deepfake Detectors Against Moiré-Induced Distortions,['cs.CV'],"['Razaib Tariq', 'Minji Heo', 'Simon S. Woo', 'Shahroz Tariq']","Deepfake detection remains a pressing challenge, particularly in real-world settings where smartphone-captured media from digital screens often introduces Moiré artifacts that can distort detection outcomes. This study systematically evaluates state-of-the-art (SOTA) deepfake detectors on Moiré-affected videos, an issue that has received little attention. We collected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF, DFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world conditions, including varying screens, smartphones, lighting setups, and camera angles. To further examine the influence of Moiré patterns on deepfake detection, we conducted additional experiments using our DeepMoiréFake, referred to as (DMF) dataset and two synthetic Moiré generation techniques. Across 15 top-performing detectors, our results show that Moiré artifacts degrade performance by as much as 25.4%, while synthetically generated Moiré patterns lead to a 21.4% drop in accuracy. Surprisingly, demoiréing methods, intended as a mitigation approach, instead worsened the problem, reducing accuracy by up to 17.2%. These findings underscore the urgent need for detection models that can robustly handle Moiré distortions alongside other realworld challenges, such as compression, sharpening, and blurring. By introducing the DMF dataset, we aim to drive future research toward closing the gap between controlled experiments and practical deepfake detection."
2510.23224,Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment,"['cs.CV', 'cs.IR']","['Hongyi Wang', 'Zhengjie Zhu', 'Jiabo Ma', 'Fang Wang', 'Yue Shi', 'Bo Luo', 'Jili Wang', 'Qiuyu Cai', 'Xiuming Zhang', 'Yen-Wei Chen', 'Lanfen Lin', 'Hao Chen']","The rapid digitization of histopathology slides has opened up new possibilities for computational tools in clinical and research workflows. Among these, content-based slide retrieval stands out, enabling pathologists to identify morphologically and semantically similar cases, thereby supporting precise diagnoses, enhancing consistency across observers, and assisting example-based education. However, effective retrieval of whole slide images (WSIs) remains challenging due to their gigapixel scale and the difficulty of capturing subtle semantic differences amid abundant irrelevant content. To overcome these challenges, we present PathSearch, a retrieval framework that unifies fine-grained attentive mosaic representations with global-wise slide embeddings aligned through vision-language contrastive learning. Trained on a corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained morphological cues and high-level semantic patterns to enable accurate and flexible retrieval. The framework supports two key functionalities: (1) mosaic-based image-to-image retrieval, ensuring accurate and efficient slide research; and (2) multi-modal retrieval, where text queries can directly retrieve relevant slides. PathSearch was rigorously evaluated on four public pathology datasets and three in-house cohorts, covering tasks including anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination, and grading across diverse organs such as breast, lung, kidney, liver, and stomach. External results show that PathSearch outperforms traditional image-to-image retrieval frameworks. A multi-center reader study further demonstrates that PathSearch improves diagnostic accuracy, boosts confidence, and enhances inter-observer agreement among pathologists in real clinical scenarios. These results establish PathSearch as a scalable and generalizable retrieval solution for digital pathology."
2510.23223,Black hole echos reflect the phase transition and fluctuations in Hawking radiation,['gr-qc'],"['Tianqi Yue', 'Jin Wang']","Black holes are thermal objects. They can form thermodynamic phases and exhibit phase transitions. Furthermore, black holes can also radiate, termed as Hawking radiation. However, the signatures of these behaviors are challenging to observe. In this work,we consider Hawking radiation in black hole phase transitions. We uncovered that an echo can emerge from the correlations between individual single event and joint two events. This provides possible signature of black hole phase transition and fluctuations in Hawking radiation."
2510.23222,Rabi oscillations of a monolayer quantum emitter driven through its excited state,"['cond-mat.mes-hall', 'quant-ph']","['Victor N. Mitryakhin', 'Ivan A. Solovev', 'Alexander Steinhoff', 'Jaewon Lee', 'Martin Esmann', 'Ana Predojević', 'Christopher Gies', 'Christian Schneider']","The interaction of a quantum two-level system with a resonant driving field results in the emergence of Rabi oscillations, which are the hallmark of a controlled manipulation of a quantum state on the Bloch sphere. This all-optical coherent control of solid-state two-level systems is crucial for quantum applications. In this work we study Rabi oscillations emerging in a WSe2 monolayer-based quantum dot. The emitter is driven coherently using picosecond laser pulses to a higher-energy state, while photoluminescence is probed from the ground state. The theoretical treatment based on a three-level exciton model reveals the population transfer between the exciton ground and excited states coupled by Coulomb interaction. Our calculations demonstrate that the resulting exciton ground state population can be controlled by varying driving pulse area and detuning which is evidenced by the experimental data. Our results pave the way towards the coherent control of quantum emitters in atomically thin semiconductors, a crucial ingredient for monolayer-based high-performance, on-demand single photon sources."
2510.23221,Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action,"['cs.AI', 'physics.comp-ph']","['Hong Wang', 'Wenkai Yang', 'Jie Wang', 'Huanshuo Dong', 'Zijie Geng', 'Zhen Huang', 'Depeng Xie', 'Zhezheng Hao', 'Hande Dong']","Recent advances in data-driven approaches, such as neural operators (NOs), have shown substantial efficacy in reducing the solution time for integrated circuit (IC) thermal simulations. However, a limitation of these approaches is requiring a large amount of high-fidelity training data, such as chip parameters and temperature distributions, thereby incurring significant computational costs. To address this challenge, we propose a novel algorithm for the generation of IC thermal simulation data, named block Krylov and operator action (BlocKOA), which simultaneously accelerates the data generation process and enhances the precision of generated data. BlocKOA is specifically designed for IC applications. Initially, we use the block Krylov algorithm based on the structure of the heat equation to quickly obtain a few basic solutions. Then we combine them to get numerous temperature distributions that satisfy the physical constraints. Finally, we apply heat operators on these functions to determine the heat source distributions, efficiently generating precise data points. Theoretical analysis shows that the time complexity of BlocKOA is one order lower than the existing method. Experimental results further validate its efficiency, showing that BlocKOA achieves a 420-fold speedup in generating thermal simulation data for 5000 chips with varying physical parameters and IC structures. Even with just 4% of the generation time, data-driven approaches trained on the data generated by BlocKOA exhibits comparable performance to that using the existing method."
2510.23220,Using Type-II Cepheids as Extragalactic Standard Candles: Distances to M31,"['astro-ph.GA', 'astro-ph.SR']","['V. D. Pipwala', 'H. N. Lala', 'B. Lemasle', 'E. K. Grebel', 'G. Bono', 'G. Fiorentino']","Several standard candles have been tested and used to measure accurate extragalactic distances over the past decades. There have been discussions regarding the possibility of using Type-II Cepheids (T2Cs) as an alternative tool, but rarely was this ever implemented. The aim of this project is to assert the use of T2Cs as a new avenue for calibrating the extragalactic distance scale, by using M31 as a benchmark galaxy. Since Ordinary Least Squares regression methods are not immune to outliers and offer an incomplete treatment of the uncertainties, we favor a Bayesian robust regression model to compute new Period--Luminosity (PL) and Period--Wesenheit (PW) relations calibrated using $\sim$100 T2Cs, $\sim$1000 fundamental-mode and $\sim$750 first-overtone classical Cepheids (CCs) in the LMC. Using these relations, we employ a classification routine based on Bhattacharyya distances to filter out any contaminants from the M31 sample. We validate our method by verifying that we retrieve an accurate distance for the LMC. We find a distance to M31 of $24.487\pm0.001$ (statistical) $\pm0.052$ (systematic) mag using CCs and of $24.409\pm0.025$ (statistical) $\pm0.156$ (systematic) mag using T2Cs. Both values are in excellent agreement with literature values derived from meta-analyses, from Hubble Space Telescope (HST) observations of CCs, from the Tip of the Red Giant Branch method, and from HST observations of RR Lyrae. In almost all cases, we reach a relative accuracy better than 98\%, although the archival ground-based data we use cannot compare with HST photometry. We demonstrate that T2Cs can also be used as accurate tracers for determining extragalactic distances, thereby making them excellent candidates for JWST, LSST, and ELT observations. These stars allow us to probe galaxies deprived of young populations and are beyond the reach of the fainter RR Lyrae."
2510.23219,Development of the Reconstruction Procedure of the Fluorescence detector Array of Single-pixel Telescopes for measuring Ultra-High Energy Cosmic Rays,"['astro-ph.IM', 'astro-ph.HE']",['Fraser Bradfield'],"The Fluorescence detector Array of Single-pixel Telescopes aims to deploy an array of simplified, autonomous fluorescence telescopes over an area of $\sim60,000$ km$^{2}$ to observe ultra-high energy cosmic rays. The unprecedented size of such an array will enable measurements of cosmic rays with energies above 10$^{20}$ eV with large statistics, providing new insights into UHECR sources. With a single FAST telescope consisting of just four photomultiplier tubes, traditional techniques to reconstruct observed extensive air showers are not applicable. Instead, FAST utilises a top-down approach where simulations are directly compared to data and the best match chosen via a maximum likelihood estimation. This method, known as the ""top-down reconstruction (TDR)"", requires an accurate ""first guess"" of the shower parameters to be successful. In this work, improvements to the efficiency and precision of the TDR are made and two different first guess estimation methods are investigated. The combined performance of a machine-learning-based first guess and improved TDR is shown to achieve resolutions in the shower arrival direction, depth of shower maximum and shower energy of $\sim2^\circ$, $\sim30$ g cm$^{-2}$ and $\sim7\%$ respectively for simulated events observed from two or more locations. The improved reconstruction is then applied to data from the current FAST prototype installations at the Pierre Auger Observatory and Telescope Array experiment. Using these results, the first measurements of the UHECR energy spectrum and composition by FAST are presented."
2510.23218,Fundamental effective temperature measurements for eclipsing binary stars - VI. Improved methodology and application to the circumbinary planet host star BEBOP-3,"['astro-ph.SR', 'astro-ph.EP']","['P. F. L. Maxted', 'N. J. Miller', 'T. A. Baycroft', 'D. Sebastian', 'A. H. M. J. Triaud', 'D. V. Martin']","BEBOP-3 is detached eclipsing binary star that shows total eclipses of a faint M~dwarf every 13.2 days by a 9$^{\rm th}$-magnitude F9V star. High precision radial velocity measurements have recently shown that this binary star is orbited by a planet with an orbital period $\approx 550$ days. The extensive spectroscopy used to detect this circumbinary planet has also been used to directly measure the masses of the stars in the eclipsing binary. We have used light curves from the TESS mission combined with these mass measurements to directly measure the following radii and surface gravities for the stars in this system: $R_1 = 1.386 \pm 0.010\,R_{\odot}$, $\log g_1 = 4.190 \pm 0.004$, $R_2 = 0.274 \pm 0.002\,R_{\odot}$, $\log g_2 = 4.979 \pm 0.002$. We describe an improved version of our method to measure the effective temperatures (T$_{\rm eff}$) of stars in binary systems directly from their angular diameters and bolometric fluxes. We measure T$_{\rm eff,1} = 6065{\rm\,K} \pm 44\,{\rm K}$ and T$_{\rm eff,2} = 3191{\rm\,K} \pm 40\,{\rm K}$ for the stars in BEBOP-3 using this method. BEBOP-3 can be added to our growing sample of stars that can be used test the accuracy of spectroscopic and photometric methods to estimate T$_{\rm eff}$ and $\log g$ for solar-type stars."
2510.23217,Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports,"['cs.CL', 'cs.AI']","['Alois Thomas', 'Maya Varma', 'Jean-Benoit Delbrouck', 'Curtis P. Langlotz']","Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations"
2510.23216,Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach,"['cs.AI', 'cs.LG']","['Alessandro Sestini', 'Joakim Bergdahl', 'Jean-Philippe Barrette-LaPierre', 'Florian Fuchs', 'Brady Chen', 'Micheal Jones', 'Linus Gisslén']","While several high profile video games have served as testbeds for Deep Reinforcement Learning (DRL), this technique has rarely been employed by the game industry for crafting authentic AI behaviors. Previous research focuses on training super-human agents with large models, which is impractical for game studios with limited resources aiming for human-like agents. This paper proposes a sample-efficient DRL method tailored for training and fine-tuning agents in industrial settings such as the video game industry. Our method improves sample efficiency of value-based DRL by leveraging pre-collected data and increasing network plasticity. We evaluate our method training a goalkeeper agent in EA SPORTS FC 25, one of the best-selling football simulations today. Our agent outperforms the game's built-in AI by 10% in ball saving rate. Ablation studies show that our method trains agents 50% faster compared to standard DRL methods. Finally, qualitative evaluation from domain experts indicates that our approach creates more human-like gameplay compared to hand-crafted agents. As a testimony of the impact of the approach, the method is intended to replace the hand-crafted counterpart in next iterations of the series."
2510.23215,Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter,"['cs.LG', 'cs.AI', 'math.NA']","['Hong Wang', 'Jie Wang', 'Jian Luo', 'huanshuo dong', 'Yeqiu Chen', 'Runmin Jiang', 'Zhen huang']","Eigenvalue problems are among the most important topics in many scientific disciplines. With the recent surge and development of machine learning, neural eigenvalue methods have attracted significant attention as a forward pass of inference requires only a tiny fraction of the computation time compared to traditional solvers. However, a key limitation is the requirement for large amounts of labeled data in training, including operators and their eigenvalues. To tackle this limitation, we propose a novel method, named Sorting Chebyshev Subspace Filter (SCSF), which significantly accelerates eigenvalue data generation by leveraging similarities between operators -- a factor overlooked by existing methods. Specifically, SCSF employs truncated fast Fourier transform sorting to group operators with similar eigenvalue distributions and constructs a Chebyshev subspace filter that leverages eigenpairs from previously solved problems to assist in solving subsequent ones, reducing redundant computations. To the best of our knowledge, SCSF is the first method to accelerate eigenvalue data generation. Experimental results show that SCSF achieves up to a $3.5\times$ speedup compared to various numerical solvers."
2510.23214,AUPO - Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm,['cs.AI'],"['Robin Schmöcker', 'Alexander Dockhorn', 'Bodo Rosenhahn']","We introduce a novel, drop-in modification to Monte Carlo Tree Search's (MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an automatic action abstraction algorithm that solely relies on reward distribution statistics acquired during the MCTS. Thus, unlike other automatic abstraction algorithms, AUPO requires neither access to transition probabilities nor does AUPO require a directed acyclic search graph to build its abstraction, allowing AUPO to detect symmetric actions that state-of-the-art frameworks like ASAP struggle with when the resulting symmetric states are far apart in state space. Furthermore, as AUPO only affects the decision policy, it is not mutually exclusive with other abstraction techniques that only affect the tree search."
2510.23213,Noisy nonlinear information and entropy numbers,"['math.NA', 'cs.CC', 'cs.IT', 'math.FA']","['David Krieg', 'Erich Novak', 'Leszek Plaskota', 'Mario Ullrich']","It is impossible to recover a vector from $\mathbb{R}^m$ with less than $m$ linear measurements, even if the measurements are chosen adaptively. Recently, it has been shown that one can recover vectors from $\mathbb{R}^m$ with arbitrary precision using only $O(\log m)$ continuous (even Lipschitz) adaptive measurements, resulting in an exponential speed-up of continuous information compared to linear information for various approximation problems. In this note, we characterize the quality of optimal (dis-)continuous information that is disturbed by deterministic noise in terms of entropy numbers. This shows that in the presence of noise the potential gain of continuous over linear measurements is limited, but significant in some cases."
2510.23212,Resource analysis of Shor's elliptic curve algorithm with an improved quantum adder on a two-dimensional lattice,['quant-ph'],"['Quan Gu', 'Han Ye', 'Junjie Chen', 'Xiongfeng Ma']","Quantum computers have the potential to break classical cryptographic systems by efficiently solving problems such as the elliptic curve discrete logarithm problem using Shor's algorithm. While resource estimates for factoring-based cryptanalysis are well established, comparable evaluations for Shor's elliptic curve algorithm under realistic architectural constraints remain limited. In this work, we propose a carry-lookahead quantum adder that achieves Toffoli depth $\log n + \log\log n + O(1)$ with only $O(n)$ ancillas, matching state-of-the-art performance in depth while avoiding the prohibitive $O(n\log n)$ space overhead of existing approaches. Importantly, our design is naturally compatible with the two-dimensional nearest-neighbor architectures and introduce only a constant-factor overhead. Further, we perform a comprehensive resource analysis of Shor's elliptic curve algorithm on two-dimensional lattices using the improved adder. By leveraging dynamic circuit techniques with mid-circuit measurements and classically controlled operations, our construction incorporates the windowed method, Montgomery representation, and quantum tables, and substantially reduces the overhead of long-range gates. For cryptographically relevant parameters, we provide precise resource estimates. In particular, breaking the NIST P-256 curve, which underlies most modern public-key infrastructures and the security of Bitcoin, requires about $4300$ logical qubits and logical Toffoli fidelity about $10^{-9}$. These results establish new benchmarks for efficient quantum arithmetic and provide concrete guidance toward the experimental realization of Shor's elliptic curve algorithm."
2510.23211,Proceedings of the Combined 32nd International Workshop on Expressiveness in Concurrency and 22nd Workshop on Structural Operational Semantics,"['cs.LO', 'cs.FL', 'doi', '10.4204/EPTCS.433']","['Cinzia Di Giusto', 'Giorgio Bacci']","This volume contains the proceedings of EXPRESS/SOS 2025: the Combined 32nd International Workshop on Expressiveness in Concurrency and the 22nd Workshop on Structural Operational Semantics, which was held in Aarhus, Denmark, as an affiliated workshop of CONFEST 2025. The EXPRESS/SOS workshop series aims at bringing together researchers interested in the formal semantics of systems and programming concepts, and in the expressiveness of computational models."
2510.23210,Higher order numerical schemes for SPDEs with additive Noise,['math.NA'],"['Abhishek Chaudhary', 'Andreas Prohl']","We present high-order numerical schemes for linear stochastic heat and wave equations with Dirichlet boundary conditions, driven by additive noise. Standard Euler schemes for SPDEs are limited to an order convergence between 1/2 and 1 due to the low temporal regularity of noise. For the stochastic heat equation, a modified Crank-Nicolson scheme with proper numerical quadrature rule for the noise term in its reformulation as random PDE achieves a strong convergence rate of 3/2. For the stochastic wave equation with additive noise a corresponding approach leads to a scheme which is of order 2."
2510.23209,Computing Binary Integer Programming via A New Exact Penalty Function,['math.OC'],"['Shuai Li', 'Shenglong Zhou']","Unconstrained binary integer programming (UBIP) poses significant computational challenges due to its discrete nature. We introduce a novel reformulation approach using a piecewise cubic function that transforms binary constraints into continuous equality constraints. Instead of solving the resulting constrained problem directly, we develop an exact penalty framework with a key theoretical advantage: the penalty parameter threshold ensuring exact equivalence is independent of the unknown solution set, unlike classical exact penalty theory. To facilitate the analysis of the penalty model, we introduce the concept of P-stationary points and systematically characterize their optimality properties and relationships with local and global minimizers. The P-stationary point enables the development of an efficient algorithm called APPA, which is guaranteed to converge to a P-stationary point within a finite number of iterations under a single mild assumption, namely, strong smoothness of the objective function over the unit box. Comprehensive numerical experiments demonstrate that APPA outperforms established solvers in both accuracy and efficiency across diverse problem instances."
2510.23208,Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks,"['cs.LG', 'cs.AI']","['Amal Abed', 'Ivan Lukic', 'Jörg K. H. Franke', 'Frank Hutter']","Large language models (LLMs) have shown impressive promise in code generation, yet their progress remains limited by the shortage of large-scale datasets that are both diverse and well-aligned with human reasoning. Most existing resources pair problems with solutions, but omit the intermediate thought process that guides coding. To close this gap, we present a scalable synthetic data generation pipeline that produces nearly 800k instruction-reasoning-code-test quadruplets. Each sample combines a task, a step-by-step reasoning trace, a working solution, and executable tests, enabling models to learn not just the what but also the how of problem solving. Our pipeline combines four key components: curated contest problems, web-mined content filtered by relevance classifiers, data expansion guided by reasoning patterns, and multi-stage execution-based validation. A genetic mutation algorithm further increases task diversity while maintaining consistency between reasoning traces and code implementations. Our key finding is that fine-tuning LLMs on this dataset yields consistent improvements on coding benchmarks. Beyond raw accuracy, reasoning-aware data can substitute for model scaling, generalize across architectures, and outperform leading open-source alternatives under identical sample budgets. Our work establishes reasoning-centered synthetic data generation as an efficient approach for advancing coding capabilities in LLMs. We publish our dataset and generation pipeline to facilitate further research."
2510.23207,The Shimurian BT stack is a gerbe over truncated displays,"['math.AG', 'math.NT']",['Eike Lau'],"We show that the mod $p$ fiber of the Shimurian stack $BT_n^{G,μ}$ constructed by Gardner--Madapusi is a gerbe over the corresponding stack of truncated displays. This confirms a conjecture of Drinfeld."
2510.23206,QUEST (Quasar Unsupervised Encoder and Synthesis Tool): A machine learning framework to generate quasar spectra,"['astro-ph.GA', 'astro-ph.IM']","['F. Guarneri', 'J. T. Schindler', 'R. A. Meyer', 'D. Yang', 'J. F. Hennawi', 'L. Lucie-Smith', 'S. E. I. Bosman', 'F. B. Davies']","Quasars at the redshift frontier (z > 7.0) are fundamental probes of black hole (BH) growth and evolution but notoriously difficult to identify. At these redshifts, machine learning-based selection methods have proven to be efficient, but require appropriate training sets to express their full potential. Here, we present QUEST, a Variational Auto-Encoder capable of generating realistic quasar spectra that can be post-processed for generating synthetic photometry and for spectral imputation. We start from the SDSS DR16Q catalogue, pre-process the spectra, and vet the sample to obtain a clean data set. After training the model, we investigate the properties of its latent space to understand whether it has learnt relevant physics. We provide a pipeline to generate photometry from the sampled spectra, compare it with actual quasar photometry, and showcase the capabilities of the model in reconstructing and extending quasar spectra. The trained network faithfully reproduces the input spectrum, both in terms of sample median and variance. By examining the latent space, we find correlations with continuum and bolometric luminosity, BH mass, redshift, continuum slope, and emission line properties. When used to generate photometry, we find results in excellent agreement with the control sample. The model provides satisfactory results in reconstructing emission lines: estimates of the BH mass from the reconstructed spectra are in good agreement with those from the original spectra. Furthermore, when spectra with broad absorption line features are reconstructed, the model successfully interpolates over the absorption systems. Compared with previous work, we find excellent agreement between the spectra sampled from our model and the output of their results. However, QUEST does not require any ad-hoc tuning, and is capable of reproducing the full variety of spectra available in the training set."
2510.23205,VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting,['cs.CV'],"['Hoonhee Cho', 'Jae-Young Kang', 'Giwon Lee', 'Hyemin Yang', 'Heejun Park', 'Seokwoo Jung', 'Kuk-Jin Yoon']","End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm that unifies perception, prediction, and planning into a holistic, data-driven framework. However, achieving robustness to varying camera viewpoints, a common real-world challenge due to diverse vehicle configurations, remains an open problem. In this work, we propose VR-Drive, a novel E2E-AD framework that addresses viewpoint generalization by jointly learning 3D scene reconstruction as an auxiliary task to enable planning-aware view synthesis. Unlike prior scene-specific synthesis approaches, VR-Drive adopts a feed-forward inference strategy that supports online training-time augmentation from sparse views without additional annotations. To further improve viewpoint consistency, we introduce a viewpoint-mixed memory bank that facilitates temporal interaction across multiple viewpoints and a viewpoint-consistent distillation strategy that transfers knowledge from original to synthesized views. Trained in a fully end-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and improves planning under viewpoint shifts. In addition, we release a new benchmark dataset to evaluate E2E-AD performance under novel camera viewpoints, enabling comprehensive analysis. Our results demonstrate that VR-Drive is a scalable and robust solution for the real-world deployment of end-to-end autonomous driving systems."
2510.23204,"If They Disagree, Will You Conform? Exploring the Role of Robots' Value Awareness in a Decision-Making Task","['cs.RO', 'cs.HC']","['Giulia Pusceddu', 'Giulio Antonio Abbo', 'Francesco Rea', 'Tony Belpaeme', 'Alessandra Sciutti']","This study investigates whether the opinions of robotic agents are more likely to influence human decision-making when the robots are perceived as value-aware (i.e., when they display an understanding of human principles). We designed an experiment in which participants interacted with two Furhat robots - one programmed to be Value-Aware and the other Non-Value-Aware - during a labeling task for images representing human values. Results indicate that participants distinguished the Value-Aware robot from the Non-Value-Aware one. Although their explicit choices did not indicate a clear preference for one robot over the other, participants directed their gaze more toward the Value-Aware robot. Additionally, the Value-Aware robot was perceived as more loyal, suggesting that value awareness in a social robot may enhance its perceived commitment to the group. Finally, when both robots disagreed with the participant, conformity occurred in about one out of four trials, and participants took longer to confirm their responses, suggesting that two robots expressing dissent may introduce hesitation in decision-making. On one hand, this highlights the potential risk that robots, if misused, could manipulate users for unethical purposes. On the other hand, it reinforces the idea that social robots might encourage reflection in ambiguous situations and help users avoid scams."
2510.23203,DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification,['cs.CV'],"['Lukas Bierling', 'Davide Pasero', 'Fleur Dolmans', 'Helia Ghasemi', 'Angelo Broere']","Accurate vertex-level contact prediction between humans and surrounding objects is a prerequisite for high fidelity human object interaction models used in robotics, AR/VR, and behavioral simulation. DECO was the first in the wild estimator for this task but is limited to binary contact maps and struggles with soft surfaces, occlusions, children, and false-positive foot contacts. We address these issues and introduce DecoDINO, a three-branch network based on DECO's framework. It uses two DINOv2 ViT-g/14 encoders, class-balanced loss weighting to reduce bias, and patch-level cross-attention for improved local reasoning. Vertex features are finally passed through a lightweight MLP with a softmax to assign semantic contact labels. We also tested a vision-language model (VLM) to integrate text features, but the simpler architecture performed better and was used instead. On the DAMON benchmark, DecoDINO (i) raises the binary-contact F1 score by 7$\%$, (ii) halves the geodesic error, and (iii) augments predictions with object-level semantic labels. Ablation studies show that LoRA fine-tuning and the dual encoders are key to these improvements. DecoDINO outperformed the challenge baseline in both tasks of the DAMON Challenge. Our code is available at https://github.com/DavidePasero/deco/tree/main."
2510.23202,DRO-Based Computation Offloading and Trajectory Design for Low-Altitude Networks,['cs.CE'],"['Guanwang Jiang', 'Ziye Jia', 'Can Cui', 'Lijun He', 'Qiuming Zhu', 'Qihui Wu']","The low-altitude networks (LANs) integrating unmanned aerial vehicles (UAVs) and high-altitude platforms (HAPs) have become a promising solution for the rising computation demands. However, the uncertain task sizes and high mobility of UAVs pose great challenges to guarantee the quality of service. To address these issues, we propose an LAN architecture where UAVs and HAPs collaboratively provide computation offloading for ground users. Moreover, the uncertainty sets are constructed to characterize the uncertain task size, and a distributionally robust optimization problem is formulated to minimize the worst-case delay by jointly optimizing the offloading decisions and UAV trajectories. To solve the mixed-integer min-max optimization problem, we design the distributionally robust computation offloading and trajectories optimization algorithm. Specifically, the original problem is figured out by iteratively solving the outerlayer and inner-layer problems. The convex outer-layer problem with probability distributions is solved by the optimization toolkit. As for the inner-layer mixed-integer problem, we employ the Benders decomposition. The decoupled master problem concerning the binary offloading decisions is solved by the integer solver, and UAV trajectories in the sub-problem are optimized via the successive convex approximation. Simulation results show the proposed algorithm outperforms traditional optimization methods in balancing the worst-case delay and robustness."
2510.23201,Building Trust in Illiquid Markets: an AI-Powered Replication of Private Equity Funds,"['q-fin.PR', 'q-fin.PM', 'q-fin.TR']","['E. Benhamou', 'JJ. Ohana', 'B. Guez', 'E. Setrouk', 'T. Jacquot']","In response to growing demand for resilient and transparent financial instruments, we introduce a novel framework for replicating private equity (PE) performance using liquid, AI-enhanced strategies. Despite historically delivering robust returns, private equity's inherent illiquidity and lack of transparency raise significant concerns regarding investor trust and systemic stability, particularly in periods of heightened market volatility. Our method uses advanced graphical models to decode liquid PE proxies and incorporates asymmetric risk adjustments that emulate private equity's unique performance dynamics. The result is a liquid, scalable solution that aligns closely with traditional quarterly PE benchmarks like Cambridge Associates and Preqin. This approach enhances portfolio resilience and contributes to the ongoing discourse on safe asset innovation, supporting market stability and investor confidence."
2510.23200,Elliptic curves and Fourier coefficients of meromorphic modular forms,['math.NT'],['Pengcheng Zhang'],"We discuss several congruences satisfied by the coefficients of meromorphic modular forms, or equivalently, $p$-adic behaviors of meromorphic modular forms under the $U_p$ operator, that are summarized from numerical experiments, connecting meromorphic modular forms to symmetric powers of elliptic curves. We also provide heuristic explanations for these congruences as well as prove some of them using hypergeometric functions and the Borcherds--Shimura lift."
2510.23199,Rate-optimal Design for Anytime Best Arm Identification,"['stat.ML', 'cs.LG']","['Junpei Komiyama', 'Kyoungseok Jang', 'Junya Honda']","We consider the best arm identification problem, where the goal is to identify the arm with the highest mean reward from a set of $K$ arms under a limited sampling budget. This problem models many practical scenarios such as A/B testing. We consider a class of algorithms for this problem, which is provably minimax optimal up to a constant factor. This idea is a generalization of existing works in fixed-budget best arm identification, which are limited to a particular choice of risk measures. Based on the framework, we propose Almost Tracking, a closed-form algorithm that has a provable guarantee on the popular risk measure $H_1$. Unlike existing algorithms, Almost Tracking does not require the total budget in advance nor does it need to discard a significant part of samples, which gives a practical advantage. Through experiments on synthetic and real-world datasets, we show that our algorithm outperforms existing anytime algorithms as well as fixed-budget algorithms."
2510.23198,PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets,"['cs.LG', 'cs.AI', 'cs.CL']","['Etienne Goffinet', 'Shane Bergsma', 'Avraham Sheinin', 'Natalia Vassilieva', 'Shaheer Muhammad', 'Preslav Nakov', 'Gurpreet Gosal']","Continual pre-training (CPT) for domain adaptation must balance target-domain gains with stability on the base domain. Existing CPT scaling laws typically assume a fixed pre-training budget, which limits their ability to forecast adaptation outcomes for models trained at different tokens-per-parameter (PTPP). We present \emph{PTPP-aware} adaptation scaling laws that make the pre-training budget an explicit variable, enabling accurate \emph{prediction} of adaptation loss at unseen \ptpp. On a multilingual setup (English/Arabic $\rightarrow$ French), PTPP-aware formulations trained on early stages (\ptpp{}=\{15,31\}) predict target loss at \ptpp{}=279 and outperform a PTPP-agnostic \dcpt{} transfer baseline on metrics (Huber-on-log, MAE$_\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in the appendix. Beyond forecasting, we show a practical use case: planning replay ratios and adaptation token budgets that satisfy target and forgetting constraints under compute limits."
2510.23197,Model-free filtering in high dimensions via projection and score-based diffusions,"['math.ST', 'stat.ML']","['Sören Christensen', 'Jan Kallsen', 'Claudia Strauch', 'Lukas Trottner']","We consider the problem of recovering a latent signal $X$ from its noisy observation $Y$. The unknown law $\mathbb{P}^X$ of $X$, and in particular its support $\mathscr{M}$, are accessible only through a large sample of i.i.d.\ observations. We further assume $\mathscr{M}$ to be a low-dimensional submanifold of a high-dimensional Euclidean space $\mathbb{R}^d$. As a filter or denoiser $\widehat X$, we suggest an estimator of the metric projection $π_{\mathscr{M}}(Y)$ of $Y$ onto the manifold $\mathscr{M}$. To compute this estimator, we study an auxiliary semiparametric model in which $Y$ is obtained by adding isotropic Laplace noise to $X$. Using score matching within a corresponding diffusion model, we obtain an estimator of the Bayesian posterior $\mathbb{P}^{X \mid Y}$ in this setup. Our main theoretical results show that, in the limit of high dimension $d$, this posterior $\mathbb{P}^{X\mid Y}$ is concentrated near the desired metric projection $π_{\mathscr{M}}(Y)$."
2510.23196,Neural Networks for AC Optimal Power Flow: Improving Worst-Case Guarantees during Training,['eess.SY'],"['Bastien Giraud', 'Rahul Nellikath', 'Johanna Vorwerk', 'Maad Alowaifeer', 'Spyros Chatzivasileiadis']","The AC Optimal Power Flow (AC-OPF) problem is central to power system operation but challenging to solve efficiently due to its nonconvex and nonlinear nature. Neural networks (NNs) offer fast surrogates, yet their black-box behavior raises concerns about constraint violations that can compromise safety. We propose a verification-informed NN framework that incorporates worst-case constraint violations directly into training, producing models that are both accurate and provably safer. Through post-hoc verification, we achieve substantial reductions in worst-case violations and, for the first time, verify all operational constraints of large-scale AC-OPF proxies. Practical feasibility is further enhanced via restoration and warm-start strategies for infeasible operating points. Experiments on systems ranging from 57 to 793 buses demonstrate scalability, speed, and reliability, bridging the gap between ML acceleration and safe, real-time deployment of AC-OPF solutions - and paving the way toward data-driven optimal control."
2510.23195,Modeling a Smooth Surface by a Constrained Biharmonic Equation with Application in Soil Science,['math.NA'],"['Samson Seifu Bekele', 'Maregnesh Mechal Wolde', 'Claus Führer', 'Nils-Otto Kitterød', 'Anne Kværnø']","This paper presents a method for mathematical modelling of surfaces conditioned on empirical data. It is based on solving a discrete biharmonic equation over a domain with given inner point and inner curve data. The inner curve data is used to model boundary values while the inner point data is used for modeling a load vector with the goal to generate a smooth surface. The construction of boundary data is an ill-posed problem, for which a special regularization approach is suggested.
  The method is designed for surface construction problems with a very limited amount of measured data. In the paper we apply the method by using empirical data of soil thickness and geological maps indicating exposed bedrock regions."
2510.23194,Can the diffeomorphism and Gauss constraints be holonomy corrected in the deformed algebra approach to modified gravity?,['gr-qc'],"['Jamy-Jayme Thézier', 'Aurélien Barrau', 'Killian Martineau', 'Maxime De Sousa']","Deforming the algebra of constraint is a well known approach to effective loop quantum cosmology. More generally, it is a consistent way to modify gravity from the Hamiltonian perspective. In this framework, the Hamiltonian (scalar) constraint is usually the only one to be holonomy corrected. As a heuristic hypothesis, we consider the possibility to also correct the diffeomorphism and Gauss constraints. It is shown that it is impossible to correct the diffeomorphism constraint without correcting the Gauss one, while maintaining a first-class algebra. However, if all constraints are corrected, the algebra can be closed. The resulting differential equations to be fulfilled by the corrections (of the background and of the perturbations) are derived."
2510.23193,Locally trivial monodromy of moduli spaces of sheaves on Abelian surfaces,['math.AG'],['Ludovica Buelli'],"The aim of this work is to give a description of the locally trivial monodromy group of irreducible symplectic varieties arising from moduli spaces of semistable sheaves on Abelian surfaces with non-primitive Mukai vector. The outcome is that the locally trivial monodromy group of a singular moduli space of this type is isomorphic to the monodromy group of a smooth moduli space, extending Markman's and Mongardi's description to the non-primitive case."
2510.23192,Minimal Weak Gravity Conjecture And Gauge Duality in M-theory on K3xT2,['hep-th'],"['Mohammed Charkaoui', 'Rajae Sammani', 'El Hassan Saidi', 'Rachid Ahl Laamara']","The minimal Weak Gravity Conjecture (WGC) predicts the emergence of towers of superextremal states in both weak and strong coupling limits. In this work, we study M-theory compactified on a special class of Calabi-Yau threefolds to construct a 5D effective field theory (EFT) that accommodates both weak and strong gauge coupling limits. Building on a classification of fiber structures of Calabi-Yau threefolds with finite volume, we establish a correspondence between curves in the fiber and the base, which relates weak and strong gauge couplings. This allows us to probe non-perturbative effects by treating strong couplings through their weakly counterparts. We use this result and properties of Bogomol'nyi-Prasad-Sommerfield (BPS) states to demonstrate that M-theory on such Calabi-Yau threefold exhibits towers of superextremal BPS states in the aforementioned extreme limits as expected by the minimal WGC."
2510.23191,The Benchmarking Epistemology: Construct Validity for Evaluating Machine Learning Models,"['cs.LG', 'stat.ML']","['Timo Freiesleben', 'Sebastian Zezulka']","Predictive benchmarking, the evaluation of machine learning models based on predictive performance and competitive ranking, is a central epistemic practice in machine learning research and an increasingly prominent method for scientific inquiry. Yet, benchmark scores alone provide at best measurements of model performance relative to an evaluation dataset and a concrete learning problem. Drawing substantial scientific inferences from the results, say about theoretical tasks like image classification, requires additional assumptions about the theoretical structure of the learning problems, evaluation functions, and data distributions. We make these assumptions explicit by developing conditions of construct validity inspired by psychological measurement theory. We examine these assumptions in practice through three case studies, each exemplifying a typical intended inference: measuring engineering progress in computer vision with ImageNet; evaluating policy-relevant weather predictions with WeatherBench; and examining limitations of the predictability of life events with the Fragile Families Challenge. Our framework clarifies the conditions under which benchmark scores can support diverse scientific claims, bringing predictive benchmarking into perspective as an epistemological practice and a key site of conceptual and theoretical reasoning in machine learning."
2510.23190,Evaluation of Vision-LLMs in Surveillance Video,['cs.CV'],"['Pascal Benschop', 'Cristian Meo', 'Justin Dauwels', 'Jelte P. Mense']","The widespread use of cameras in our society has created an overwhelming amount of video data, far exceeding the capacity for human monitoring. This presents a critical challenge for public safety and security, as the timely detection of anomalous or criminal events is crucial for effective response and prevention. The ability for an embodied agent to recognize unexpected events is fundamentally tied to its capacity for spatial reasoning. This paper investigates the spatial reasoning of vision-language models (VLMs) by framing anomalous action recognition as a zero-shot, language-grounded task, addressing the embodied perception challenge of interpreting dynamic 3D scenes from sparse 2D video. Specifically, we investigate whether small, pre-trained vision--LLMs can act as spatially-grounded, zero-shot anomaly detectors by converting video into text descriptions and scoring labels via textual entailment. We evaluate four open models on UCF-Crime and RWF-2000 under prompting and privacy-preserving conditions. Few-shot exemplars can improve accuracy for some models, but may increase false positives, and privacy filters -- especially full-body GAN transforms -- introduce inconsistencies that degrade accuracy. These results chart where current vision--LLMs succeed (simple, spatially salient events) and where they falter (noisy spatial cues, identity obfuscation). Looking forward, we outline concrete paths to strengthen spatial grounding without task-specific training: structure-aware prompts, lightweight spatial memory across clips, scene-graph or 3D-pose priors during description, and privacy methods that preserve action-relevant geometry. This positions zero-shot, language-grounded pipelines as adaptable building blocks for embodied, real-world video understanding. Our implementation for evaluating VLMs is publicly available at: https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition"
2510.23189,DREaM: Drug-Drug Relation Extraction via Transfer Learning Method,"['cs.CL', 'cs.AI', 'cs.LG']","['Ali Fata', 'Hossein Rahmani', 'Parinaz Soltanzadeh', 'Amirhossein Derakhshan', 'Behrouz Minaei Bidgoli']","Relation extraction between drugs plays a crucial role in identifying drug drug interactions and predicting side effects. The advancement of machine learning methods in relation extraction, along with the development of large medical text databases, has enabled the low cost extraction of such relations compared to other approaches that typically require expert knowledge. However, to the best of our knowledge, there are limited datasets specifically designed for drug drug relation extraction currently available. Therefore, employing transfer learning becomes necessary to apply machine learning methods in this domain. In this study, we propose DREAM, a method that first employs a trained relation extraction model to discover relations between entities and then applies this model to a corpus of medical texts to construct an ontology of drug relationships. The extracted relations are subsequently validated using a large language model. Quantitative results indicate that the LLM agreed with 71 of the relations extracted from a subset of PubMed abstracts. Furthermore, our qualitative analysis indicates that this approach can uncover ambiguities in the medical domain, highlighting the challenges inherent in relation extraction in this field."
2510.23188,Embroidery Actuator Utilizing Embroidery Patterns to Generate Diverse Fabric Deformations,['eess.SY'],"['Yuki Ota', 'Yuki Funabora']","This paper presents a novel Embroidery Actuator, a fabric-integrated pneumatic actuator that enables diverse and controllable deformations through embroidery pattern design. Unlike conventional fabric actuators that rely on fiber- or thread-shaped actuators, the proposed actuator is fabricated by directly stitching an inflatable tube onto the fabric using a cord-embroidery technique. The embroidered thread and the fabric jointly form a sleeve that constrains the expansion of the inflatable tube, converting internal pressure into targeted bending or stretching deformations. By varying the embroidery pattern, such as zigzag or cross configurations, different geometric constraints can be realized, allowing for flexible control of deformation direction and magnitude. Analytical deformation models based on the Neo-Hookean model and Lagrange's equations were developed to predict the relationship between pneumatic pressure and bending angle, and were experimentally validated using motion-capture measurements. The results demonstrated that the actuator achieves strong agreement with the analytical deformation model."
2510.23187,GBNL: Graded Betti Number Learning of Complex Biological Data,['math.AC'],"['Mushal Zia', 'Faisal Suwayyid', 'Guo-Wei Wei']","While persistent homology is widely used for data shape analysis, persistent commutative algebra (PCA) has seen limited adoption in machine learning and data science. Unlike persistent homology, which delivers topological invariants in the form of Betti numbers, PCA provides both algebraic invariants and graded Betti numbers. However, graded Betti numbers have seldom been applied to real-world data. In this work, we introduce the first-of-its-kind application of commutative algebra graded Betti numbers in machine learning and data science. Specifically, we present Graded Betti Number Learning (GBNL) for protein-nucleic acid binding prediction. Protein-DNA/RNA interactions are fundamental to cellular processes such as replication, transcription, translation, and gene regulation, and their understanding and prediction remain challenging. GBNL represents each nucleic acid sequence as a family of $k$-mer-specific sets and derives persistent graded Betti invariants from PCA, generating multiscale topological representations of local nucleotide organization. To incorporate cross-molecule context, these graded Betti representations are paired with transformer-based protein embeddings, linking nucleotide-level signals with global protein patterns. The proposed graded Betti representations effectively detect single-site mutations and distinguish complete mutation patterns. Operating on primary sequences with minimal preprocessing, GBNL bridges commutative algebra, reduced algebraic topology, combinatorics, and machine learning, establishing a new paradigm for comparative sequence analysis. Numerical studies using three datasets highlight the success of GBNL in protein-nucleic acid binding prediction."
2510.23186,Approaching Domain Generalization with Embeddings for Robust Discrimination and Recognition of RF Communication Signals,['eess.SP'],"['Lukas Henneke', 'Frank Kurth']","Radio frequency (RF) signal recognition plays a critical role in modern wireless communication and security applications. Deep learning-based approaches have achieved strong performance but typically rely heavily on extensive training data and often fail to generalize to unseen signals. In this paper, we propose a method to learn discriminative embeddings without relying on real-world RF signal recordings by training on signals of synthetic wireless protocols. We validate the approach on a dataset of real RF signals and show that the learned embeddings capture features enabling accurate discrimination of previously unseen real-world signals, highlighting its potential for robust RF signal classification and anomaly detection."
2510.23185,"Trusses, ditrusses, weak trusses",['math.RA'],['Alberto Facchini'],"In this paper we extend to left skew trusses $(T,+,\circ,σ)$ previous work on left skew rings. We had presented a left skew ring as a group $(N,+)$ with two binary operations $\circ$ and $\cdot$ with $\circ$ associative, $\cdot$ left distributive over the addition $+$ of the group, and such that the difference of the two operations $\circ$ and $\cdot$ is the binary operation $π_1\colon N\times N\to N$. Here we extend this idea to the left skew trusses introduced in 2019 by Brzeziński, replacing the operation $π_1$ with the binary operation $σπ_1\colon T\times T\to T$. The case where the semigroup morphism $λ^T\colon T\to \End_\Gp(T,+)$ is constant turns out to be particular interesting. We get several canonical category isomorphisms. For instance, we get a category isomorphism between the category of all left skew trusses $(T,+,\circ,σ)$ with $λ^T\colon (T,\circ)\to \End_\Gp(T,+)$ a constant semigroup morphism and $σ,λ^T_0$ image-commuting idempotent endomorphisms and the category of all associative interchange near-rings. Interchange near-rings were introduced by Edmunds in 2016. When $σ$ is an idempotent group endomorphism of the group $(T,+)$ and $λ^T\colon (T,\circ)\to \End_\Gp(T,+)$ is a semigroup morphism constantly equal to a group endomorphism $τ$, we also get a sort of duality exchanging the mappings $σ$ and $τ$."
2510.23184,Finding 3D Scene Analogies with Multimodal Foundation Models,['cs.CV'],"['Junho Kim', 'Young Min Kim']","Connecting current observations with prior experiences helps robots adapt and plan in new, unseen 3D environments. Recently, 3D scene analogies have been proposed to connect two 3D scenes, which are smooth maps that align scene regions with common spatial relationships. These maps enable detailed transfer of trajectories or waypoints, potentially supporting demonstration transfer for imitation learning or task plan transfer across scenes. However, existing methods for the task require additional training and fixed object vocabularies. In this work, we propose to use multimodal foundation models for finding 3D scene analogies in a zero-shot, open-vocabulary setting. Central to our approach is a hybrid neural representation of scenes that consists of a sparse graph based on vision-language model features and a feature field derived from 3D shape foundation models. 3D scene analogies are then found in a coarse-to-fine manner, by first aligning the graph and refining the correspondence with feature fields. Our method can establish accurate correspondences between complex scenes, and we showcase applications in trajectory and waypoint transfer."
2510.23183,PEARL: Private Equity Accessibility Reimagined with Liquidity,['q-fin.TR'],"['E. Benhamou', 'JJ. Ohana', 'B. Guez', 'E. Setrouk', 'T. Jacquot']","In this work, we introduce PEARL (Private Equity Accessibility Reimagined with Liquidity), an AI-powered framework designed to replicate and decode private equity funds using liquid, cost-effective assets. Relying on previous research methods such as Erik Stafford's single stock selection (Stafford) and Thomson Reuters - Refinitiv's sector approach (TR), our approach incorporates an additional asymmetry to capture the reduced volatility and better performance of private equity funds resulting from sale timing, leverage, and stock improvements through management changes. As a result, our model exhibits a strong correlation with well-established liquid benchmarks such as Stafford and TR, as well as listed private equity firms (Listed PE), while enhancing performance to better align with renowned quarterly private equity benchmarks like Cambridge Associates, Preqin, and Bloomberg Private Equity Fund indices. Empirical findings validate that our two-step approachdecoding liquid daily private equity proxies with a degree of negative return asymmetry outperforms the initial daily proxies and yields performance more consistent with quarterly private equity benchmarks."
2510.23182,SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations,['cs.CL'],"['Shuai Huang', 'Wenxuan Zhao', 'Jun Gao']","As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git."
2510.23181,Physics-informed diffusion models for extrapolating crystal structures beyond known motifs,"['cond-mat.mtrl-sci', 'cs.LG']","['Andrij Vasylenko', 'Federico Ottomano', 'Christopher M. Collins', 'Rahul Savani', 'Matthew S. Dyer', 'Matthew J. Rosseinsky']","Discovering materials with previously unreported crystal frameworks is key to achieving transformative functionality. Generative artificial intelligence offers a scalable means to propose candidate crystal structures, however existing approaches mainly reproduce decorated variants of established motifs rather than uncover new configurations. Here we develop a physics-informed diffusion method, supported by chemically grounded validation protocol, which embeds descriptors of compactness and local environment diversity to balance physical plausibility with structural novelty. Conditioning on these metrics improves generative performance across architectures, increasing the fraction of structures outside 100 most common prototypes up to 67%. When crystal structure prediction (CSP) is seeded with generative structures, most candidates (97%) are reconstructed by CSP, yielding 145 (66%) low-energy frameworks not matching any known prototypes. These results show that while generative models are not substitutes for CSP, their chemically informed, diversity-guided outputs can enhance CSP efficiency, establishing a practical generative-CSP synergy for discovery-oriented exploration of chemical space."
2510.23180,Spinning-down RU Lup. Constraints on the physics of the outflow from high-resolution spectroscopy,['astro-ph.SR'],"['A. Armeni', 'B. Stelzer', 'A. Frasca', 'C. F. Manara', 'J. Campbell-White', 'J. F. Gameiro', 'M. Gangi']","Magnetic winds are a key mechanism for angular momentum removal in young stars. In this work, we aim at characterizing the multi-component outflow of RU Lup. The unprecedented high resolution of the Echelle SPectrograph for Rocky Exoplanets and Stable Spectroscopic Observations (ESPRESSO) enabled a detailed study of the forbidden emission lines and the blueshifted absorption in the lines of the Na I and Ca II doublets, which we resolved in three discrete absorption components at low, medium, and high velocities. We developed a method that disentangles vertical and toroidal velocities in the absorption components and infers the wind launching radius, magnetic lever arm, and mass-loss rate. We identified a low-velocity broad component in the [O I] 5577 line, consistent with a rotating magnetohydrodynamic disk wind launched near the disk truncation radius. We showed that the discrete absorption components trace spatially and physically distinct regions of the outflow. The medium and low velocity components are launched from the inner disk (< 6.76 stellar radii) with low lever arms indicative of warm, highly mass-loaded streamlines. However, the two components differ mainly in vertical velocity. The low velocity absorption is consistent with an outer absorbing shell, while the medium velocity absorption forms near the disk truncation radius. Its higher vertical velocity is compatible with either a slightly larger lever arm, or additional heating at the base of the flow. For plausible ionization levels in the inner disk, this outflow component removes a substantial fraction of the accretion spin-up torque. In conclusion, our work shows that RU Lup hosts a stratified, rotating, warm disk wind launched across a narrow annulus near the disk truncation radius, which is sufficiently mass-loaded to extract a large amount of the stellar spin-up torque. The observations disfavor an X-wind scenario."
2510.23179,Open-Source High-Fidelity Orbit Estimation for Planetary Science and Space Situational Awareness Using the Tudat Software,"['astro-ph.EP', 'astro-ph.IM']","['Luigi Gisolfi', 'Dominic Dirkx', 'Sam Fayolle', 'Valerio Filice', 'Riva Alkahal', 'Miguel Avillez', 'Tristan Dijkstra', 'Jonas Hener', 'Lars Hinüber', 'Marco Langbroek', 'Nicolò Maistri', 'Michael Plumaris', 'Alejandro Sanchez Rodriguez', 'Giuseppe Cimò', 'Kevin Cowan', 'Fabien Dahmani', 'João Encarnacao', 'Geoffrey Garrett', 'Steve Gehly', 'Xuanyu Hu', 'Marceau Jeanjean', 'Antonio López Rivera', 'Andrea Minervino Amodio', 'Guifre Molera Calvés', 'Markus Reichel']","The TU Delft Astrodynamics Toolbox (Tudat) is a free open-source software suite for research and education in astrodynamics. Initially focused on numerical simulations of orbital dynamics and state estimation, it enables combining optical and radiometric tracking data from multiple sources to estimate the dynamics and parameters of natural and artificial bodies. Recent developments have added functionality for real tracking data analysis, with applications to planetary missions and Space Situational Awareness (SSA). Tudat currently supports processing of (i) deep-space Doppler and range data from DSN and ESTRACK, (ii) Doppler and VLBI data from the PRIDE experiment, and (iii) optical astrometry from the Minor Planet Center (MPC) and Natural Satellite Data Center (NSDC). Using tracking data from the MRO and GRAIL spacecraft and astrometric data of the asteroid Eros, we present prefit residuals (from SPICE-based observables) and postfit residuals (from fitting data to the Tudat dynamical model). Postfit Doppler residuals reach 1-5 mHz for MRO and GRAIL, orbit differences are a few meters for GRAIL and about one meter for MRO, and MRO range residuals are a few meters. From eight years of Eros astrometry, we obtain an orbit difference from the JPL Horizons solution by several tens of kilometers, consistent with its 3-sigma formal error. Tudat's SSA capability is demonstrated by propagating the orbit of Kosmos 482, a Venus lander launched in 1972 that remained in Earth orbit, over 50 years, including its predicted re-entry on May 10 2025. These examples showcase Tudat's modular, flexible, high-fidelity modeling across diverse orbital regimes within a fully open-source framework. All example code is publicly available, and future Tudat analyses will be published with fully reproducible code, allowing anyone in the community to improve and expand upon our work."
2510.23178,Feedback in Dynamic Contests: Theory and Experiment,"['econ.TH', 'cs.GT']","['Sumit Goel', 'Yiqing Yan', 'Jeffrey Zeidel']","We study the effect of interim feedback policies in a dynamic all-pay auction where two players bid over two stages to win a common-value prize. We show that sequential equilibrium outcomes are characterized by Cheapest Signal Equilibria, wherein stage 1 bids are such that one player bids zero while the other chooses a cheapest bid consistent with some signal. Equilibrium payoffs for both players are always zero, and the sum of expected total bids equals the value of the prize. We conduct an experiment with four natural feedback policy treatments -- full, rank, and two cutoff policies -- and while the bidding behavior deviates from equilibrium, we fail to reject the hypothesis of no treatment effect on total bids. Further, stage 1 bids induce sunk costs and head starts, and we test for the resulting sunk cost and discouragement effects in stage 2 bidding."
2510.23177,Malliavin calculcus for a Hawkes process,['math.PR'],"['Alexandre Popier', 'Laurent Denis', 'Dorian Cacitti-Holland']","We develop a Malliavin calculus for nonlinear Hawkes processes in the sense of Carlen and Pardoux. This approach, based on perturbations of the jump times of the process, enables the construction of a local Dirichlet form. As an application, we establish criteria for the absolute continuity of solutions to stochastic differential equations driven by Hawkes processes. We also derive sensitivity formulas for the valuation of financial derivatives with respect to model parameters."
2510.23176,TARC: Time-Adaptive Robotic Control,"['cs.RO', 'cs.LG']","['Arnav Sukhija', 'Lenart Treven', 'Jin Cheng', 'Florian Dörfler', 'Stelian Coros', 'Andreas Krause']","Fixed-frequency control in robotics imposes a trade-off between the efficiency of low-frequency control and the robustness of high-frequency control, a limitation not seen in adaptable biological systems. We address this with a reinforcement learning approach in which policies jointly select control actions and their application durations, enabling robots to autonomously modulate their control frequency in response to situational demands. We validate our method with zero-shot sim-to-real experiments on two distinct hardware platforms: a high-speed RC car and a quadrupedal robot. Our method matches or outperforms fixed-frequency baselines in terms of rewards while significantly reducing the control frequency and exhibiting adaptive frequency control under real-world conditions."
2510.23175,Financial markets as a Le Bonian crowd during boom-and-bust episodes: A complementary theoretical framework in behavioural finance,['q-fin.GN'],['Claire Barraud'],"This article proposes a complementary theoretical framework in behavioural finance by interpreting financial markets during boom-and-bust episodes as a Le Bonian crowd. While behavioural finance has documented the limits of individual rationality through biases and heuristics, these contributions remain primarily microeconomic. A second, more macroeconomic strand appears to treat market instability as the aggregated result of individual biases, although it generally does so without an explicit theoretical account of how such aggregation operates. In contrast, this paper adopts a macro-psychological -and therefore macroeconomic -perspective, drawing on classical crowd psychology (Le Bon, 1895; Tarde, 1901; Freud, 1921). The central claim is that during speculative booms and crashes, markets behave as psychological crowds governed by unconscious processes, suggestion, emotional contagion, and impulsive action. These episodes cannot be understood merely as the sum of individual departures from rationality, but as the emergence of a collective mental state that follows its own psychological laws. By reintroducing crowd psychology into behavioural finance, this paper clarifies the mechanisms through which market-wide irrationality arises and offers a theoretical foundation for a macrobehavioural understanding of financial instability."
2510.23174,Dynamics and Model Representation of Two Contrasting Extreme Precipitation Events in the Sahel,['physics.ao-ph'],"['Souleymane Sanogo', 'Marlon Maranan', 'Andreas H. Fink', 'Beth J. Woodhams', 'Peter Knippertz']","Two extreme flood-inducing precipitation events in two cities in Mali, on 08 August 2012 in San (127 mm) and on 25 August 2019 in Kenieba (126 mm), are investigated with respect to rainfall structures, dynamical forcings, and the ability of the ICOsahedral Nonhydrostatic (ICON) model to represent their evolution. Two sets of experiments with convective parameterization enabled (PARAM) and disabled (EXPLC), both at 6.5 km grid spacing, are conducted for each case. While the (thermo)dynamical fields of the simulations are compared with ERA5 reanalysis data, the rainfall fields are tested against the satellite-based precipitation dataset IMERG by applying the spatial verification methods Fractions Skill Score (FSS) and the Structure-Amplitude-Location (SAL) score. In addition, a spectral filtering of tropical waves is applied to investigate their impact on the extreme events. The most prominent results are: (1) Both cases were caused by organized convective systems associated with a westward propagating cyclonic vortex, but differ in their environmental setting. Although both cases featured an east African wave (AEW), the San case involved convective enhancement along dry Saharan airmasses, whereas the Kenieba case occurred within an unusual widespread wet environment extending deep into the Sahel. (2) Although EXPLC captures the rainfall distribution in the San case better than PARAM, it fails to organize convection in the moisture-laden Kenieba case, which PARAM is capable of simulating. (3) The FSS confirms the case-dependency of the ICON skill. The SAL method hints towards a systematic deficiency of EXPLC to represent the convective organization by producing too many scattered and weak rainfall systems, while PARAM is more effective in converting abundant moisture into excessive rainfall."
2510.23173,"A skew group ring of $\mathbb Z/2\mathbb Z$ over $U(\mathfrak{sl}_2)$, Leonard triples and odd graphs","['math.CO', 'math.RT']","['Hau-Wen Huang', 'Chin-Yen Lee']","We employ a skew group ring of $\mathbb Z/2\mathbb Z$ over $U(\mathfrak{sl}_2)$ to construct modules over the universal Bannai--Ito algebra. In addition, we give the conditions under which the defining generators act as Leonard triples on the resulting modules. As a combinatorial realization, we establish an algebra homomorphism from the universal Bannai--Ito algebra onto the Terwilliger algebra of an odd graph. This homomorphism provides a unified description of Leonard triples on all irreducible modules over the Terwilliger algebra."
2510.23172,Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation,['cs.CR'],"['Mohsen Ahmadvand', 'Pedro Souto']","The Optimism derivation pipeline is engineered for correctness and liveness, not for succinct validity proofs. A straightforward port to a zkVM imposes significant overheads, making validity proofs significantly more costly than necessary. We systematically identify inefficiencies in the current design, analyze their impact on proving costs, and provide a soundness-preserving redesign tailored to zk proving. Our redesign achieves up to 6.5x faster derivation inside zkVMs (3.5x overall speedup) while maintaining identical safety guarantees."
2510.23171,"Benchmarking VQE Configurations: Architectures, Initializations, and Optimizers for Silicon Ground State Energy","['quant-ph', 'cs.LG']","['Zakaria Boutakka', 'Nouhaila Innan', 'Muhammed Shafique', 'Mohamed Bennai', 'Z. Sakhi']","Quantum computing presents a promising path toward precise quantum chemical simulations, particularly for systems that challenge classical methods. This work investigates the performance of the Variational Quantum Eigensolver (VQE) in estimating the ground-state energy of the silicon atom, a relatively heavy element that poses significant computational complexity. Within a hybrid quantum-classical optimization framework, we implement VQE using a range of ansatz, including Double Excitation Gates, ParticleConservingU2, UCCSD, and k-UpCCGSD, combined with various optimizers such as gradient descent, SPSA, and ADAM. The main contribution of this work lies in a systematic methodological exploration of how these configuration choices interact to influence VQE performance, establishing a structured benchmark for selecting optimal settings in quantum chemical simulations. Key findings show that parameter initialization plays a decisive role in the algorithm's stability, and that the combination of a chemically inspired ansatz with adaptive optimization yields superior convergence and precision compared to conventional approaches."
2510.23170,Set-valued data analysis for interlaboratory comparisons,"['stat.ME', 'stat.AP']","['Sébastien Petit', 'Sébastien Marmin', 'Nicolas Fischer']","This article introduces tools to analyze set-valued data statistically. The tools were initially developed to analyze results from an interlaboratory comparison made by the Electromagnetic Compatibility Working Group of Eurolab France, where the goal was to select a consensual set of injection points on an electrical device. Families based on the Hamming-distance from a consensus set are introduced and Fisher's noncentral hypergeometric distribution is proposed to model the number of deviations. A Bayesian approach is used and two types of techniques are proposed for the inference. Hierarchical models are also considered to quantify a possible within-laboratory effect."
2510.23169,MATCH: Task-Driven Code Evaluation through Contrastive Learning,"['cs.CL', 'cs.SE']","['Marah Ghoummaid', 'Vladimir Tchuiev', 'Ofek Glick', 'Michal Moschkovitz', 'Dotan Di Castro']","AI-based code generation is increasingly prevalent, with GitHub Copilot estimated to generate 46% of the code on GitHub. Accurately evaluating how well generated code aligns with developer intent remains a critical challenge. Traditional evaluation methods, such as unit tests, are often unscalable and costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code functionality, and metrics like CodeBERTScore require reference code, which is not always available. To address the gap in reference-free evaluation, with few alternatives such as ICE-Score, this paper introduces MATCH, a novel reference-free metric. MATCH uses Contrastive Learning to generate meaningful embeddings for code and natural language task descriptions, enabling similarity scoring that reflects how well generated code implements the task. We show that MATCH achieves stronger correlations with functional correctness and human preference than existing metrics across multiple programming languages."
2510.23168,Recasting and Forecasting Dark Matter Limits Without Raw Data: A Generalized Algorithm for Gamma-Ray Telescopes,"['astro-ph.HE', 'hep-ph']","[""Giacomo D'Amico"", 'Michele Doro', 'Michela De Caria']","We present a novel method for both forecasting and recasting upper limits (ULs) on dark matter (DM) annihilation cross sections, $\left< σv \right>^{UL}$, or decay lifetime $τ^{LL}$ . The forecasting method relies solely on the instrument response functions (IRFs) to predict ULs for a given observational setup, without the need for full analysis pipelines. The recasting procedure uses published ULs to reinterpret constraints for alternative DM models or channels. We demonstrate its utility across a range of canonical annihilation channels, including $b\bar{b}$, $W^+W^-$, $τ^+τ^-$, and $μ^+μ^-$, and apply it to several major gamma-ray experiments, including MAGIC, \textit{Fermi}-LAT, and CTAO. Notably, we develop a recasting approach that remains effective even when the IRF is unavailable by extracting generalized IRF-dependent coefficients from benchmark channels. We apply this method to reinterpret ULs derived from standard spectra (e.g., PPPC4DMID) in terms of more recent DM scenarios, including a Higgsino-like model with mixed final states and spectra generated with the CosmiXs model. Extensive Monte Carlo simulations and direct comparison with published results confirm the robustness and accuracy of our method, with discrepancies remaining within statistical uncertainties. The algorithm is generally applicable to any scenario where the expected signal model is parametric, offering a powerful tool for reinterpreting existing gamma-ray limits and efficiently exploring the DM parameter space in current and future indirect detection experiments."
2510.23167,Guiding Skill Discovery with Foundation Models,['cs.AI'],"['Zhao Yang', 'Thomas M. Moerland', 'Mike Preuss', 'Aske Plaat', 'Vincent François-Lavet', 'Edward S. Hu']","Learning diverse skills without hand-crafted reward functions could accelerate reinforcement learning in downstream tasks. However, existing skill discovery methods focus solely on maximizing the diversity of skills without considering human preferences, which leads to undesirable behaviors and possibly dangerous skills. For instance, a cheetah robot trained using previous methods learns to roll in all directions to maximize skill diversity, whereas we would prefer it to run without flipping or entering hazardous areas. In this work, we propose a Foundation model Guided (FoG) skill discovery method, which incorporates human intentions into skill discovery through foundation models. Specifically, FoG extracts a score function from foundation models to evaluate states based on human intentions, assigning higher values to desirable states and lower to undesirable ones. These scores are then used to re-weight the rewards of skill discovery algorithms. By optimizing the re-weighted skill discovery rewards, FoG successfully learns to eliminate undesirable behaviors, such as flipping or rolling, and to avoid hazardous areas in both state-based and pixel-based tasks. Interestingly, we show that FoG can discover skills involving behaviors that are difficult to define. Interactive visualisations are available from https://sites.google.com/view/submission-fog."
2510.23166,Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms,"['cs.CE', 'physics.comp-ph']","['Philippe Martin Wyder', 'Judah Goldfeder', 'Alexey Yermakov', 'Yue Zhao', 'Stefano Riva', 'Jan P. Williams', 'David Zoro', 'Amy Sara Rude', 'Matteo Tomasetto', 'Joe Germany', 'Joseph Bakarji', 'Georg Maierhofer', 'Miles Cranmer', 'J. Nathan Kutz']","Machine learning (ML) is transforming modeling and control in the physical, engineering, and biological sciences. However, rapid development has outpaced the creation of standardized, objective benchmarks - leading to weak baselines, reporting bias, and inconsistent evaluations across methods. This undermines reproducibility, misguides resource allocation, and obscures scientific progress. To address this, we propose a Common Task Framework (CTF) for scientific machine learning. The CTF features a curated set of datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints, including noise and limited data. Inspired by the success of CTFs in fields like natural language processing and computer vision, our framework provides a structured, rigorous foundation for head-to-head evaluation of diverse algorithms. As a first step, we benchmark methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz. These results illustrate the utility of the CTF in revealing method strengths, limitations, and suitability for specific classes of problems and diverse objectives. Next, we are launching a competition around a global real world sea surface temperature dataset with a true holdout dataset to foster community engagement. Our long-term vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets that raise the bar for rigor and reproducibility in scientific ML."
2510.23165,Ground-state properties of finite nuclei in relativistic Hartree-Bogoliubov theory with an improved quark mass density-dependent model,"['nucl-th', 'doi', '10.1088/1674-1137/ae0997']","['Renli Xu', 'Chen Wu', 'Jian Liu', 'Bin Hong', 'Jie Peng', 'Xiong Li', 'Ruxian Zhu', 'Zhizhen Zhao', 'Zhongzhou Ren']","A relativistic Hartree-Bogoliubov (RHB) model based on quark-meson coupling is developed, with a new parametrization derived from experimental observables. Using this model, we systematically investigate the ground-state properties of even-even nuclei spanning $8\leq Z\leq118$, including binding energies, quadrupole deformations, root-mean-square (rms) charge radii, two-nucleon separation energies, two-nucleon shell gaps, and $α$-decay energies. Comparisons with available experimental data demonstrate that this subnucleon-based RHB model reliably describes the ground-state properties of finite nuclei."
2510.23164,"Accelerating Kerr solution in SU(1,1)/U(1)-sigma model",['gr-qc'],['Vanke Hu'],"Under the coset formulation of pure Einstein spacetime, we solve the reduction problem of the nonlinear \(σ\)-model for this spacetime and present the process of the inverse scattering technique after simplification. Through the simplified inverse scattering process, taking the pure Rindler metric as the background, we introduce a pair of solitons on the imaginary axis and obtain an accelerating solution in the form of three solitons in this model. By means of appropriate parameter selection and coordinate transformation, we prove that this accelerating solution in the three-soliton formulation is equivalent to the accelerating Kerr solution."
2510.23163,Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs,"['cs.CL', 'cs.AI']","['Hang Lei', 'Shengyi Zong', 'Zhaoyan Li', 'Ziren Zhou', 'Hao Liu']","The screenplay serves as the foundation for television production, defining narrative structure, character development, and dialogue. While Large Language Models (LLMs) show great potential in creative writing, direct end-to-end generation approaches often fail to produce well-crafted screenplays. We argue this failure stems from forcing a single model to simultaneously master two disparate capabilities: creative narrative construction and rigid format adherence. The resulting outputs may mimic superficial style but lack the deep structural integrity and storytelling substance required for professional use. To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage Refinement (DSR), a decomposed framework that decouples creative narrative generation from format conversion. The first stage transforms a brief outline into rich, novel-style prose. The second stage refines this narrative into a professionally formatted screenplay. This separation enables the model to specialize in one distinct capability at each stage. A key challenge in implementing DSR is the scarcity of paired outline-to-novel training data. We address this through hybrid data synthesis: reverse synthesis deconstructs existing screenplays into structured inputs, while forward synthesis leverages these inputs to generate high-quality narrative texts as training targets. Blind evaluations by professional screenwriters show that DSR achieves a 75% win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of human-level performance. Our work demonstrates that decomposed generation architecture with tailored data synthesis effectively specializes LLMs in complex creative domains."
2510.23162,"Measurement-only circuit of perturbed toric code on triangular lattice: Topological entanglement, 1-form symmetry and logical qubits",['quant-ph'],"['Keisuke Kataoka', 'Yoshihito Kuno', 'Takahiro Orito', 'Ikuo Ichinose']","Measurement-only (quantum) circuit (MoC) gives possibility to realize the states with rich entanglements, topological orders and quantum memories. This work studies the MoC, in which the projective-measurement operators consist of stabilizers of the toric code and competitive local Pauli operators. The former correspond to terms of the toric code on a triangular lattice and the later to external magnetic and electric fields. We employ efficient numerical stabilizer algorithm to trace evolving states undergoing phase transitions. We elucidate the phase diagram of the MoC system with the observables such as, topological entanglement entropy (TEE), disorder parameters of 1-form symmetries and emergent logical operators. We clarify the locations of the phase transitions through the observation of the above quantities and obtain precise critical exponents to examine if the observables exhibit the critical behavior simultaneously under the MoC and transitions belong to the same universality class. In contrast to the TC Hamiltonian system and toric code MoC on a square lattice, the system on the triangular lattice is not self-dual nor bipartite, and then, coincidence by symmetries, such as critical behaviors across the TC and Higgs/confined phase, does not takes place. Then, the toric code MoC on the triangular lattice provides us a suitable playground to clarify the mutual relationship between the TEE, spontaneous symmetry breaking of the 1-form symmetries, and emergence of logical operators. Obtained results indicate that toric code MoC on the triangular lattice exhibits a few distinct phase transitions with different location and critical exponents, and some of them are closely related with the two-dimensional percolation transition."
2510.23161,The Sun as an X-ray star V.: A new method to retrieve coronal filling factors,['astro-ph.SR'],"['Wilhelmina Maryann Joseph', 'Beate Stelzer', 'Salvatore Orlando', 'Moritz Klawin']","Context. Stellar coronae are unresolved in X-rays, so inferences about their structure rely on spectral analysis. The ""Sun-as-an-X-ray-star"" (SaXS) approach uses the Sun as a spatially resolved template to interpret stellar spectra, but previous SaXS implementations were indirect and computationally heavy. Aims. We present a new SaXS implementation that converts solar emission measure distributions (EMDs) of distinct coronal region types into XSPEC spectral components and test whether broad-band X-ray spectra alone can recover their filling factors. Methods. We built XSPEC multi-temperature spectral models for four solar region types (background/quiet corona, active regions, cores, and flares) by using EMDs derived from Yohkoh/SXT data and translating each EMD bin into an isothermal apec component. These models were fit (using PyXspec) to two one-hour DAXSS spectra representative of quiescent (2022-06-29) and flaring (2022-04-25) states. Best-fit normalizations were converted into projected areas and filling factors and compared with near-coincident Hinode/XRT full-disk images. Results. Using the Yohkoh/SXT EMDs, the quiescent Sun spectrum is dominated by active region emission (filling factor ~22%), with the background corona poorly constrained. The flaring Sun spectrum is best described by a combination of active regions, cores, and flares with filling factors of ~47.5%, ~4.1%, and ~0.062%, respectively. The dominant components match spatial features seen in Hinode/XRT images. Limitations include the DAXSS low-energy cutoff (~0.7 keV) and the small, non-uniform Yohkoh EMD sample. Conclusions. Our SaXS implementation enables direct retrieval of coronal filling factors from broad-band X-ray spectra and provides a physically motivated alternative to ad hoc few-temperature fits, suitable for stellar X-ray analyses."
2510.23160,ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix,['cs.CL'],"['Zile Yang', 'Ling Li', 'Na Di', 'Jinlong Pang', 'Yao Zhou', 'Hao Cheng', 'Bo Han', 'Jiaheng Wei']","Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs) to domain-specific instructions by training on a carefully curated subset of high-quality instruction-response pairs, typically drawn from a larger dataset that often contains many low-quality or noisy samples. However, existing quality-first paradigms often overlook valuable signals in discarded low-quality data and rely on imperfect quality filters. We introduce ENTP (Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a framework that revitalizes low-quality corpora through symbolic purification and neural reconstruction. The symbolic module identifies and prunes noisy samples based on statistical priors, while the neural component synthesizes enriched instruction-response pairs by leveraging latent representations and model knowledge. This neural-symbolic synergy enhances data informativeness and diversity. Experiments show that ENTP-augmented datasets, constructed exclusively from low-quality data, outperform 13 established data-selection baselines across five instruction-following benchmarks, and even surpass fine-tuning on the full original dataset (approximately 300K examples). Our results highlight the untapped potential of low-quality data and underscore the importance of intelligent purification and synthesis for efficient instruction alignment."
2510.23159,Low-temperature scaling laws in unconventional flat-band superconductors,"['cond-mat.supr-con', 'cond-mat.str-el']","['Maximilian Buthenhoff', 'Yusuke Nishida']","In flat-band superconductors, the electron pairing is strongly enhanced so that the critical temperature scales linearly with the interaction strength. Identifying the governing pairing mechanism in flat-band superconducting systems is therefore a central task, which may be constrained by experimental probes via low-temperature scaling measurements. A key observable underlying the Meissner effect and the resulting divergent DC conductivity is the superfluid weight. While it is well established that the minimal quantum metric provides the dominant contribution to the superfluid weight in conventional superconductors with isolated flat bands, recent studies indicate that the unconventional pairing can generate additional nonlocal quantum geometric terms. This motivates us to derive the low-temperature scaling law of the superfluid weight in two-dimensional flat-band superconductors with sufficiently isolated bands. In particular, we consider the gap function with point or line nodes classified by the Weierstrass preparation theorem. Beyond the superfluid weight, we additionally deliver explicit low-temperature scaling laws of the order parameter, the tunneling conductance, the specific heat, the Sommerfeld coefficient, and the spin-lattice relaxation rate to provide complementary experimental discriminants of the underlying pairing symmetry. The implications of our results are also elucidated by applying them to a selection of superconducting states in $C_{6v}$-symmetric systems."
2510.23158,Matching Reverberant Speech Through Learned Acoustic Embeddings and Feedback Delay Networks,['eess.AS'],"['Philipp Götz', 'Gloria Dal Santo', 'Sebastian J. Schlecht', 'Vesa Välimäki', 'Emanuël A. P. Habets']","Reverberation conveys critical acoustic cues about the environment, supporting spatial awareness and immersion. For auditory augmented reality (AAR) systems, generating perceptually plausible reverberation in real time remains a key challenge, especially when explicit acoustic measurements are unavailable. We address this by formulating blind estimation of artificial reverberation parameters as a reverberant signal matching task, leveraging a learned room-acoustic prior. Furthermore, we propose a feedback delay network (FDN) structure that reproduces both frequency-dependent decay times and the direct-to-reverberation ratio of a target space. Experimental evaluation against a leading automatic FDN tuning method demonstrates improvements in estimated room-acoustic parameters and perceptual plausibility of artificial reverberant speech. These results highlight the potential of our approach for efficient, perceptually consistent reverberation rendering in AAR applications."
2510.23157,Weighted Sobolev inequalities and superlinear elliptic problems on exterior domains,['math.AP'],"['Ardra A', 'Ameerraja Ansari', 'Anumol Joseph', 'Lakshmi Sankar']","Let $B_1 ^c = \{ x\in \mathbb{R}^N: |x|>1 \}, N \geq 2$, and $\mathcal{D}^{1,N}_0(B^c_1)$, be the Beppo-Levi space. We prove that $\mathcal{D}^{1,N}_0(B^c_1)$ is compactly embedded into the weighted Lebesgue space $L^r(B_1^c;K(x))$ for all $r\in[1,\infty)$ for an appropriate class of weight functions $K$. As an application, we prove the existence of a positive solution to a superlinear semipositone problem on $B_1 ^c$ in $\mathbb{R}^2$. We also establish boundedness and regularity of solutions of certain boundary value problems and derive their Green's function representation."
2510.23156,Enabling Vibration-Based Gesture Recognition on Everyday Furniture via Energy-Efficient FPGA Implementation of 1D Convolutional Networks,"['cs.LG', 'cs.AI']","['Koki Shibata', 'Tianheng Ling', 'Chao Qian', 'Tomokazu Matsui', 'Hirohiko Suwa', 'Keiichi Yasumoto', 'Gregor Schiele']","The growing demand for smart home interfaces has increased interest in non-intrusive sensing methods like vibration-based gesture recognition. While prior studies demonstrated feasibility, they often rely on complex preprocessing and large Neural Networks (NNs) requiring costly high-performance hardware, resulting in high energy usage and limited real-world deployability. This study proposes an energy-efficient solution deploying compact NNs on low-power Field-Programmable Gate Arrays (FPGAs) to enable real-time gesture recognition with competitive accuracy. We adopt a series of optimizations: (1) We replace complex spectral preprocessing with raw waveform input, eliminating complex on-board preprocessing while reducing input size by 21x without sacrificing accuracy. (2) We design two lightweight architectures (1D-CNN and 1D-SepCNN) tailored for embedded FPGAs, reducing parameters from 369 million to as few as 216 while maintaining comparable accuracy. (3) With integer-only quantization and automated RTL generation, we achieve seamless FPGA deployment. A ping-pong buffering mechanism in 1D-SepCNN further improves deployability under tight memory constraints. (4) We extend a hardware-aware search framework to support constraint-driven model configuration selection, considering accuracy, deployability, latency, and energy consumption. Evaluated on two swipe-direction datasets with multiple users and ordinary tables, our approach achieves low-latency, energy-efficient inference on the AMD Spartan-7 XC7S25 FPGA. Under the PS data splitting setting, the selected 6-bit 1D-CNN reaches 0.970 average accuracy across users with 9.22 ms latency. The chosen 8-bit 1D-SepCNN further reduces latency to 6.83 ms (over 53x CPU speedup) with slightly lower accuracy (0.949). Both consume under 1.2 mJ per inference, demonstrating suitability for long-term edge operation."
2510.23155,The Detectability of Lunar-Origin Asteroids in the LSST Era,['astro-ph.EP'],"['Yixuan Wu', 'Yifei Jiao', 'Wen-Yue Dai', 'Yukun Huang', 'Zihan Liu', 'Bin Cheng', 'Hexi Baoyin', 'Junfeng Li']","While most near-Earth asteroids (NEAs) are thought to originate from the main belt, recent discoveries have suggested the existence of a lunar-derived NEA population, such as the asteroids Kamo'oalewa and 2024 PT5. These objects may hold key clues to the dynamical evolution of NEAs and the recent impact history of the Earth-Moon system. However, the population, distribution, and dynamical characteristics of these Lunar-Origin Asteroids (LOAs) remain poorly constrained. By combining the lunar ejecta production with N-body orbital simulations of the ejecta, we investigate their orbital evolution in the past millions of years and the current LOA population, revealing their significant potential for detection by future surveys. Specifically for the Vera C. Rubin Observatory's upcoming Legacy Survey of Space and Time (LSST), we predict an average detection rate of about 6 LOAs (with D > 5 m) per year. Additionally, we find that the LOAs tend to approach from sunward and anti-sunward directions, with encounter velocities significantly lower than those of typical NEAs. These findings offer valuable insights in guiding targeted ground-based surveys and planetary defense efforts for LOAs in the future."
2510.23154,Size-consistent implementation of Hamiltonian simulation-based quantum-selected configuration interaction method for the supramolecular approach,"['quant-ph', 'physics.chem-ph']",['Kenji Sugisaki'],"The quantum-selected configuration interaction (QSCI) method is a promising approach for large-scale quantum chemical calculations on currently available quantum hardware. However, its naive implementation lacks size consistency, which is essential for accurate intermolecular interaction energy calculations using the supramolecular approach. Here, we present a size-consistent implementation of QSCI by sampling Slater determinants for the dimer in the localized molecular orbital basis, constructing the subspaces for the monomers and dimer, and augmenting the dimer subspace with additional determinants required for size consistency. Implemented within the Hamiltonian simulation-based QSCI (HSB-QSCI) framework, our method numerically satisfies size consistency for 4H/8H clusters, the FH dimer, and the FH--H$_2$O system. Application to intermolecular interaction energy calculations of hydrogen-bonded FH dimer and FH--H$_2$O demonstrates that our approach reproduces complete active space-configuration interaction (CAS-CI) values with errors below 0.04 kcal mol$^{-1}$."
2510.23153,Robust and tuneable ion selectivity in vermiculite membranes intercalated with unexchangeable ions,"['cond-mat.soft', 'cond-mat.mtrl-sci', 'physics.chem-ph']","['Zhuang Liu', 'Yumei Tan', 'Jianhao Qian', 'Min Cao', 'Eli Hoenig', 'Guowei Yang', 'Fengchao Wang', 'Francois M. Peeters', 'Yi-Chao Zou', 'Liang-Yin Chu', 'Marcelo Lozada-Hidalgo']","Membranes selective to ions of the same charge are increasingly sought for wastewater processing and valuable element recovery. However, while narrow channels are known to be essential, other membrane parameters remain difficult to identify and control. Here we show that Zr$^{4+}$, Sn$^{4+}$, Ir$^{4+}$, and La$^{3+}$ ions intercalated into vermiculite laminate membranes become effectively unexchangeable, creating stable channels, one to two water layers wide, that exhibit robust and tuneable ion selectivity. Ion permeability in these membranes spans five orders of magnitude, following a trend dictated by the ions' Gibbs free energy of hydration. Unexpectedly, different intercalated ions lead to two distinct monovalent ion selectivity sequences, despite producing channels of identical width. The selectivity instead correlates with the membranes' stiffness and the entropy of hydration of the intercalated ions. These results introduce a new ion selectivity mechanism driven by entropic and mechanical effects, beyond classical size and charge exclusion."
2510.23152,Exploring LR-FHSS Modulation for Enhanced IoT Connectivity: A Measurement Campaign,['cs.NI'],"['Alexis Delplace', 'Samer Lahoud', 'Kinda Khawam']","This paper presents the first comprehensive real-world measurement campaign comparing LR-FHSS and LoRa modulations within LoRaWAN networks in urban environments. Conducted in Halifax, Canada, the campaign used a LoRaWAN platform capable of operating both modulations in the FCC-regulated US915 band. Real-world measurements are crucial for capturing the effects of urban topology and signal propagation challenges, which are difficult to fully replicate in simulations. Results show that LR-FHSS can achieve up to a 20% improvement in Packet Reception Rate (PRR) over traditional LoRa in dense urban areas. Additionally, the study investigated path loss and Received Signal Strength Indicator (RSSI), finding that LR-FHSS achieved a minimum RSSI of -138 dBm compared to LoRa's -120 dBm. The findings demonstrate that the introduction of LR-FHSS enhances communication robustness and reliability under regulatory limitations and suggest promising applications in LoRaWAN networks."
2510.23151,AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes,"['cs.CV', 'cs.LG']","['Sixian Liu', 'Chen Xu', 'Qiang Wang', 'Donghai Shi', 'Yiwen Li']","Multimodal camera-LiDAR fusion technology has found extensive application in 3D object detection, demonstrating encouraging performance. However, existing methods exhibit significant performance degradation in challenging scenarios characterized by sensor degradation or environmental disturbances. We propose a novel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates cross-modal knowledge by identifying reliable patterns for robust detection in complex scenes. Specifically, we first project features from each modality into a unified BEV space and enhance them using a window-based attention mechanism. Subsequently, an adaptive gated fusion module based on cross-modal attention is designed to integrate these features into reliable BEV representations robust to challenging environments. Furthermore, we construct a new dataset named Excavator3D (E3D) focusing on challenging excavator operation scenarios to benchmark performance in complex conditions. Our method not only achieves competitive performance on the standard KITTI dataset with 93.92% accuracy, but also significantly outperforms the baseline by 24.88% on the challenging E3D dataset, demonstrating superior robustness to unreliable modal information in complex industrial scenes."
2510.23150,Revisiting the Structure of Trend Premia: When Diversification Hides Redundancy,"['q-fin.PR', 'q-fin.PM', 'q-fin.RM', 'q-fin.TR', 'stat.ML']","['Alban Etiennea', 'Jean-Jacques Ohana', 'Eric Benhamou', 'Béatrice Guez', 'Ethan Setrouk', 'Thomas Jacquot']","Recent work has emphasized the diversification benefits of combining trend signals across multiple horizons, with the medium-term window-typically six months to one year-long viewed as the ""sweet spot"" of trend-following. This paper revisits this conventional view by reallocating exposure dynamically across horizons using a Bayesian optimization framework designed to learn the optimal weights assigned to each trend horizon at the asset level. The common practice of equal weighting implicitly assumes that all assets benefit equally from all horizons; we show that this assumption is both theoretically and empirically suboptimal. We first optimize the horizon-level weights at the asset level to maximize the informativeness of trend signals before applying Bayesian graphical models-with sparsity and turnover control-to allocate dynamically across assets. The key finding is that the medium-term band contributes little incremental performance or diversification once short- and long-term components are included. Removing the 125-day layer improves Sharpe ratios and drawdown efficiency while maintaining benchmark correlation. We then rationalize this outcome through a minimum-variance formulation, showing that the medium-term horizon largely overlaps with its neighboring horizons. The resulting ""barbell"" structure-combining short- and long-term trends-captures most of the performance while reducing model complexity. This result challenges the common belief that more horizons always improve diversification and suggests that some forms of time-scale diversification may conceal unnecessary redundancy in trend premia."
2510.23149,Complexity Dependent Error Rates for Physics-informed Statistical Learning via the Small-ball Method,"['stat.ML', 'cs.LG', 'math.ST']",['Diego Marcondes'],"Physics-informed statistical learning (PISL) integrates empirical data with physical knowledge to enhance the statistical performance of estimators. While PISL methods are widely used in practice, a comprehensive theoretical understanding of how informed regularization affects statistical properties is still missing. Specifically, two fundamental questions have yet to be fully addressed: (1) what is the trade-off between considering soft penalties versus hard constraints, and (2) what is the statistical gain of incorporating physical knowledge compared to purely data-driven empirical error minimisation. In this paper, we address these questions for PISL in convex classes of functions under physical knowledge expressed as linear equations by developing appropriate complexity dependent error rates based on the small-ball method. We show that, under suitable assumptions, (1) the error rates of physics-informed estimators are comparable to those of hard constrained empirical error minimisers, differing only by constant terms, and that (2) informed penalization can effectively reduce model complexity, akin to dimensionality reduction, thereby improving learning performance. This work establishes a theoretical framework for evaluating the statistical properties of physics-informed estimators in convex classes of functions, contributing to closing the gap between statistical theory and practical PISL, with potential applications to cases not yet explored in the literature."
2510.23148,Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI,"['cs.LG', 'cs.AI', 'eess.IV']","['Aryan Mathur', 'Asaduddin Ahmed']","Deep reinforcement learning agents often struggle when tasks require understanding both vision and language. Conventional architectures typically isolate perception (for example, CNN-based visual encoders) from decision-making (policy networks). This separation can be inefficient, since the policy's failures do not directly help the perception module learn what is important. To address this, we implement the Perception-Decision Interleaving Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that alternates between perception and decision layers within a single transformer. This interleaving allows feedback from decision-making to refine perceptual features dynamically. In addition, we integrate a contrastive loss inspired by CLIP to align textual mission embeddings with visual scene features. We evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that the approach achieves more stable rewards and stronger alignment compared to a standard PPO baseline. The results suggest that interleaved transformer encoders are a promising direction for developing more integrated autonomous agents."
2510.23147,"HAPS-ISAC for 6G: Architecture, Design Trade-offs, and a Practical Roadmap",['eess.SP'],"['Parisa Kanani', 'Mohammad Javad Omidi', 'Mahmoud Modarres-Hashemi', 'Halim Yanikomeroglu']","To meet the ambitious goals of next-generation 6G networks, including ultra-high data rates and ubiquitous coverage, we propose a novel high-altitude platform station (HAPS)-based integrated sensing and communication (ISAC) architecture. Operating in the stratosphere, the HAPS functions as both a powerful communication hub and an advanced environmental sensor. Combined with a fleet of cooperative uncrewed aerial vehicles (UAVs), this dual-purpose system forms a scalable and intelligent 3D network. Simulation results indicate that this approach significantly boosts network performance, improves sensing accuracy, and ensures a fairer service distribution across users, outperforming conventional UAV-only baselines. We conclude by outlining the prospective applications and a deployment roadmap for this technology for smart cities and other large-scale environments."
2510.23146,Fake scientific journals are here to stay,['cs.DL'],['Enrique Orduña-Malea'],"Scientific publishing is facing an alarming proliferation of fraudulent practices that threaten the integrity of research communication. The production and dissemination of fake research have become a profitable business, undermining trust in scientific journals and distorting the evaluation processes that depend on them. This brief piece examines the problem of fake journals through a three-level typology. The first level concerns predatory journals, which prioritise financial gain over scholarly quality by charging authors publication fees while providing superficial or fabricated peer review. The second level analyses hijacked journals, in which counterfeit websites impersonate legitimate titles to deceive authors into submitting and paying for publication. The third level addresses hacked journals, where legitimate platforms are compromised through cyberattacks or internal manipulation, enabling the distortion of review and publication processes. Together, these forms of misconduct expose deep vulnerabilities in the scientific communication ecosystem, exacerbated by the pressure to publish and the marketisation of research outputs. The manuscript concludes that combating these practices requires structural reforms in scientific evaluation and governance. Only by reducing the incentives that sustain the business of fraudulent publishing can the scholarly community restore credibility and ensure that scientific communication fulfils the essential purpose of reliable advancement of knowledge."
2510.23145,Implicit Modeling for Transferability Estimation of Vision Foundation Models,['cs.CV'],"['Yaoyan Zheng', 'Huiqun Wang', 'Nan Zhou', 'Di Huang']","Transferability estimation identifies the best pre-trained models for downstream tasks without incurring the high computational cost of full fine-tuning. This capability facilitates deployment and advances the pre-training and fine-tuning paradigm. However, existing methods often struggle to accurately assess transferability for emerging pre-trained models with diverse architectures, training strategies, and task alignments. In this work, we propose Implicit Transferability Modeling (ITM), a novel framework that implicitly models each model's intrinsic transferability, coupled with a Divide-and-Conquer Variational Approximation (DVA) strategy to efficiently approximate embedding space evolution. This design enables generalization across a broader range of models and downstream tasks. Extensive experiments on a comprehensive benchmark--spanning extensive training regimes and a wider variety of model types--demonstrate that ITM consistently outperforms existing methods in terms of stability, effectiveness, and efficiency."
2510.23144,DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios,['cs.CV'],"['Ziyu Wang', 'Wenhao Li', 'Ji Wu']","3D object detection from multi-view images in traffic scenarios has garnered significant attention in recent years. Many existing approaches rely on object queries that are generated from 3D reference points to localize objects. However, a limitation of these methods is that some reference points are often far from the target object, which can lead to false positive detections. In this paper, we propose a depth-guided query generator for 3D object detection (DQ3D) that leverages depth information and 2D detections to ensure that reference points are sampled from the surface or interior of the object. Furthermore, to address partially occluded objects in current frame, we introduce a hybrid attention mechanism that fuses historical detection results with depth-guided queries, thereby forming hybrid queries. Evaluation on the nuScenes dataset demonstrates that our method outperforms the baseline by 6.3\% in terms of mean Average Precision (mAP) and 4.3\% in the NuScenes Detection Score (NDS)."
2510.23143,Singularities of Landau-Ginzburg models for complete intersections and derived categories,['math.AG'],['Victor Przyjalkowski'],Mirror symmetry predicts that bounded derived category of a smooth Fano variety is equivalent to Fukaya-Seidel category of its Landau-Ginzburg model. It is expected that fibers of Landau-Ginzburg model with ordinary double points correspond to an exceptional collection of a Fano variety. We verify this expectation on a numerical level for Fano complete intersections and Calabi-Yau compactifications of their toric Landau-Ginzburg models of Givental's type.
2510.23142,Rethinking GSPO: The Perplexity-Entropy Equivalence,"['cs.LG', 'cs.AI', 'cs.CL']",['Chi Liu'],"We provide a new perspective on GSPO's length-normalized importance ratios by establishing their connection to information-theoretic quantities. We show that GSPO's sequence-level weight $s(θ) = (π_θ/π_{θ_{\text{old}}})^{1/|y|}$ can be equivalently expressed as the inverse perplexity ratio $\text{PPL}_{θ_{\text{old}}}/\text{PPL}_θ$ and as the exponential cross-entropy change $\exp(ΔH)$. While the perplexity-entropy relationship follows from standard definitions, this observation provides a useful lens for understanding GSPO: the algorithm weights policy gradient updates by perplexity ratios, offering an information-theoretic interpretation of the importance weights. This perspective helps explain GSPO's empirical properties, including log-domain variance reduction through geometric averaging and stability in training mixture-of-experts models. We validate the mathematical equivalences and variance predictions through controlled experiments on mathematical reasoning tasks."
2510.23141,"Treble10: A high-quality dataset for far-field speech recognition, dereverberation, and enhancement","['eess.AS', 'cs.LG']","['Sarabeth S. Mullins', 'Georg Götz', 'Eric Bezzam', 'Steven Zheng', 'Daniel Gert Nielsen']","Accurate far-field speech datasets are critical for tasks such as automatic speech recognition (ASR), dereverberation, speech enhancement, and source separation. However, current datasets are limited by the trade-off between acoustic realism and scalability. Measured corpora provide faithful physics but are expensive, low-coverage, and rarely include paired clean and reverberant data. In contrast, most simulation-based datasets rely on simplified geometrical acoustics, thus failing to reproduce key physical phenomena like diffraction, scattering, and interference that govern sound propagation in complex environments. We introduce Treble10, a large-scale, physically accurate room-acoustic dataset. Treble10 contains over 3000 broadband room impulse responses (RIRs) simulated in 10 fully furnished real-world rooms, using a hybrid simulation paradigm implemented in the Treble SDK that combines a wave-based and geometrical acoustics solver. The dataset provides six complementary subsets, spanning mono, 8th-order Ambisonics, and 6-channel device RIRs, as well as pre-convolved reverberant speech scenes paired with LibriSpeech utterances. All signals are simulated at 32 kHz, accurately modelling low-frequency wave effects and high-frequency reflections. Treble10 bridges the realism gap between measurement and simulation, enabling reproducible, physically grounded evaluation and large-scale data augmentation for far-field speech tasks. The dataset is openly available via the Hugging Face Hub, and is intended as both a benchmark and a template for next-generation simulation-driven audio research."
2510.23140,Fast Voxel-Wise Kinetic Modeling in Dynamic PET using a Physics-Informed CycleGAN,"['cs.CV', 'q-bio.OT']","['Christian Salomonsen', 'Samuel Kuttner', 'Michael Kampffmeyer', 'Robert Jenssen', 'Kristoffer Wickstrøm', 'Jong Chul Ye', 'Elisabeth Wetzer']","Tracer kinetic modeling serves a vital role in diagnosis, treatment planning, tracer development and oncology, but burdens practitioners with complex and invasive arterial input function estimation (AIF). We adopt a physics-informed CycleGAN showing promise in DCE-MRI quantification to dynamic PET quantification. Our experiments demonstrate sound AIF predictions and parameter maps closely resembling the reference."
2510.23139,Unveiling the delicate hidden conditions at the interface of 2D materials by advanced atomic force microscopy,['cond-mat.mtrl-sci'],"['Yanyan Geng', 'Chang Li', 'Shuo Mi', 'Manyu Wang', 'Xinen Han', 'Huiji Hu', 'Yunzhen Wang', 'Haojie You', 'Shumin Meng', 'Hanxiang Wu', 'Jianfeng Guo', 'Shiyu Zhu', 'Yanjun Li', 'Yasuhiro Sugawara', 'Sabir Hussain', 'Fei Pang', 'Rui Xu', 'Zhihai Cheng']","The delicate interfacial conditions and behaviors play critical roles in determining the valuable physical properties of two-dimensional materials and their heterostructures on substrates. However, directly probing these complex interface conditions remains challenging. Here, we reveal the complex in-plane strain and out-of-plane bonding interface conditions in strain-engineered WS2 flakes by combined dual-harmonic electrostatic force microscopy (DH-EFM) and scanning microwave impedance microscopy (sMIM). A significant contradiction is observed between the intrinsically compressive-strain-induced larger bandgap (lower electrical conductivity) detected by DH-EFM, and the higher electrical conductivity measured by sMIM. Comparative electrical conductivity measurements under different sMIM modes demonstrate that this contradiction arises from the tip-loading-force-induced dynamic puckering effect, which is modulated by interfacial bonding strength. Furthermore, the accumulation and release of electrical conductivity during forward/backward sMIM-contact measurements further confirmed the dynamic puckering effect, revealing the difference in interface conditions between open ring and closed ring regions of WS2. This work resolves the correlation between electrical properties and interface conditions, providing insights for interface-engineered devices."
2510.23138,"Exploring high-dimensional random landscapes: from spin glasses to random matrices, passing through simple chaotic systems",['cond-mat.dis-nn'],['Alessandro Pacco'],"High-dimensional random landscapes underlie phenomena as diverse as glassy physics and optimization in machine learning, and even their simplest toy models already display extraordinarily rich behavior. This thesis aims to deepen our understanding of that behavior, by combining landscape-based approaches, via the Kac-Rice formalism, with dynamical approaches, paying special attention to both systems with reciprocal and with non-reciprocal interactions. After surveying core techniques and results through the spherical p-spin model, this thesis delivers three main advances: (i) exact dynamic-static comparison in a solvable class of models with non-reciprocal interactions, pinpointing differences and similarities of the two approaches; (ii) a stability-based calculation of the mean number of fixed points (i.e., annealed complexity) of the Sompolinsky-Crisanti-Sommers random neural network, for any level of non-reciprocity; (iii) two approaches to probe the barriers and the distribution of deep local minima in the landscape of the p-spin model; (iv) some results on the overlaps among eigenvectors of spiked, correlated random matrices, which are useful to explore the geometry of energy landscapes. Together, these results sharpen our understanding of these systems, while providing new tools and opening new doors for future research directions."
2510.23137,Note on the Construction of Structure Tensor,"['cs.CV', 'math.SP']","['Josef Bigun', 'Fernado Alonso-Fernandez']","This note presents a theoretical discussion of two structure tensor constructions: one proposed by Bigun and Granlund 1987, and the other by Granlund and Knutsson 1995. At first glance, these approaches may appear quite different--the former is implemented by averaging outer products of gradient filter responses, while the latter constructs the tensor from weighted outer products of tune-in frequency vectors of quadrature filters. We argue that when both constructions are viewed through the common lens of Total Least Squares (TLS) line fitting to the power spectrum, they can be reconciled to a large extent, and additional benefits emerge. From this perspective, the correction term introduced in Granlund and Knutsson 1995 becomes unnecessary. Omitting it ensures that the resulting tensor remains positive semi-definite, thereby simplifying the interpretation of its eigenvalues. Furthermore, this interpretation allows fitting more than a single 0rientation to the input by reinterpreting quadrature filter responses without relying on a structure tensor. It also removes the constraint that responses must originate strictly from quadrature filters, allowing the use of alternative filter types and non-angular tessellations. These alternatives include Gabor filters--which, although not strictly quadrature, are still suitable for structure tensor construction--even when they tessellate the spectrum in a Cartesian fashion, provided they are sufficiently concentrated."
2510.23136,A method for outlier detection based on cluster analysis and visual expert criteria,"['cs.LG', 'doi', '10.1111/exsy.12473']","['Juan A. Lara', 'David Lizcano', 'Víctor Rampérez', 'Javier Soriano']","Outlier detection is an important problem occurring in a wide range of areas. Outliers are the outcome of fraudulent behaviour, mechanical faults, human error, or simply natural deviations. Many data mining applications perform outlier detection, often as a preliminary step in order to filter out outliers and build more representative models. In this paper, we propose an outlier detection method based on a clustering process. The aim behind the proposal outlined in this paper is to overcome the specificity of many existing outlier detection techniques that fail to take into account the inherent dispersion of domain objects. The outlier detection method is based on four criteria designed to represent how human beings (experts in each domain) visually identify outliers within a set of objects after analysing the clusters. This has an advantage over other clustering-based outlier detection techniques that are founded on a purely numerical analysis of clusters. Our proposal has been evaluated, with satisfactory results, on data (particularly time series) from two different domains: stabilometry, a branch of medicine studying balance-related functions in human beings and electroencephalography (EEG), a neurological exploration used to diagnose nervous system disorders. To validate the proposed method, we studied method outlier detection and efficiency in terms of runtime. The results of regression analyses confirm that our proposal is useful for detecting outlier data in different domains, with a false positive rate of less than 2% and a reliability greater than 99%."
2510.23135,Universal Relations in Long-range Quantum Spin Chains,"['cond-mat.quant-gas', 'quant-ph']","['Ning Sun', 'Lei Feng', 'Pengfei Zhang']","Understanding the emergence of novel collective behaviors in strongly interacting systems lies at the heart of quantum many-body physics. Valuable insight comes from examining how few-body correlations manifest in many-body systems, embodying the ``from few to many'' philosophy. An intriguing example is the set of universal relations in ultracold atomic gases, which connect a wide range of observables to a single quantity known as the contact. In this Letter, we demonstrate that universal relations manifest in a distinct class of quantum many-body systems, long-range quantum spin chains, which belong to a completely new universality class. Using effective field theory and the operator product expansion, we establish connections between the asymptotic behavior of equal-time spin correlation functions, the dynamical structure factor, and the contact density. The theoretical predictions for equal-time correlators are explicitly verified through numerical simulations based on matrix product states. Our results could be readily tested in state-of-the-art trapped-ion systems."
2510.23134,THz mixing of high-order harmonics using $YBa_2Cu_3O_{7-δ}$ nanobridges,"['cond-mat.supr-con', 'physics.ins-det']","['Núria Alcalde-Herraiz', 'Alessia Garibaldi', 'Karn Rongrueangkul', 'Alexei Kalaboukhov', 'Floriana Lombardi', 'Sergey Cherednichenko', 'Thilo Bauch']","Superconducting materials are a key for technologies enabling a large number of devices including THz wave mixers and single photon detectors, though limited at very low temperatures for conventional superconductors. High temperature operation could in principle be offered using cuprate superconductors. However, the complexity of the material in thin film form, the extremely short coherence length and material stability, have hindered the realization of THz devices working at liquid nitrogen temperatures. $YBa_2Cu_3O_{7-δ}$ (YBCO) nanodevices have demonstrated non-linear properties typical of Josephson-like behavior, which have the potential for the mixing of AC signals in the THz range due to the large superconducting energy gap. Here, we present AC Josephson functionalities for terahertz waves utilizing Abrikosov vortex motion in nanoscale-confined fully planar YBCO thin film bridges. We observe Shapiro step-like features in the current voltage characteristics when irradiating the device with monochromatic sub-THz waves (100 GHz to 215 GHz) at 77 K. We further explore these nonlinear effects by detecting THz high-order harmonic mixing for signals from 200 GHz up to 1.4 THz using a local oscillator at 100 GHz. Our results open a path to an easy-fabricated HTS nonlinear nanodevice based on dimensional confinement suitable for terahertz applications."
2510.23133,"Thermal Transport in Ag8TS6 (T= Si, Ge, Sn) Argyrodites: An Integrated Experimental, Quantum-Chemical, and Computational Modelling Study",['cond-mat.mtrl-sci'],"['Joana Bustamante', 'Anupama Ghata', 'Aakash A. Naik', 'Christina Ertural', 'Katharina Ueltzen', 'Wolfgang G. Zeier', 'Janine George']","Argyrodite-type Ag-based sulfides combine exceptionally low lattice thermal and high ionic conductivity, making them promising candidates for thermoelectric and solid-state energy applications. In this work, we studied Ag8TS6 (T= Si, Ge, Sn) argyrodite family by combining chemical-bonding analysis, lattice vibrational properties simulation, and experimental measurements to investigate their structural and thermal transport properties. Furthermore, we propose a two-channel lattice-dynamics model based on Grüneisen-derived phonon lifetimes and compare it to an approach using machine-learned interatomic potentials. Both approaches are able to predict thermal conductivity in agreement with experimental lattice thermal conductivities along the whole temperature range, highlighting their potential suitability for future high-throughput predictions. Our findings also reveal a relationship between bond heterogeneity arising from weakly bonded Ag+ ions and occupied antibonding states in Ag-S and Ag-Ag interactions and strong anharmonicity, including large Grüneisen parameters, and low sound velocities, which are responsible for the low lattice thermal conductivity of Ag8SnS6, Ag8GeS6, and Ag8SiS6. We furthermore show that thermal and ionic conductivities in all three compounds are independent of each other and can likely be tuned individually."
2510.23132,Solvability of The Operator Equations $ AX-XB=C $ and $ AX-YB=C $,['math.FA'],"['Farida Lombarkia', 'Assia Bezai', 'Néstor Thome']","This paper provides new necessary and sufficient conditions for the solvability to the operator equations $ AX-XB=C$ and $AX-YB=C,$ where $A $ and $B $ are group invertible operators defined on an infinite dimensional Hilbert space. In addition, the general solutions to the equation $AX-YB=C,$ are derived in terms of group inverse of $ A $ and $ B $. As a consequence, new necessary and sufficient conditions for the solvability to the operator equation $ AYB-Y=C,$ are derived."
2510.23131,Corpus Frequencies in Morphological Inflection: Do They Matter?,['cs.CL'],"['Tomáš Sourada', 'Jana Straková']","The traditional approach to morphological inflection (the task of modifying a base word (lemma) to express grammatical categories) has been, for decades, to consider lexical entries of lemma-tag-form triples uniformly, lacking any information about their frequency distribution. However, in production deployment, one might expect the user inputs to reflect a real-world distribution of frequencies in natural texts. With future deployment in mind, we explore the incorporation of corpus frequency information into the task of morphological inflection along three key dimensions during system development: (i) for train-dev-test split, we combine a lemma-disjoint approach, which evaluates the model's generalization capabilities, with a frequency-weighted strategy to better reflect the realistic distribution of items across different frequency bands in training and test sets; (ii) for evaluation, we complement the standard type accuracy (often referred to simply as accuracy), which treats all items equally regardless of frequency, with token accuracy, which assigns greater weight to frequent words and better approximates performance on running text; (iii) for training data sampling, we introduce a method novel in the context of inflection, frequency-aware training, which explicitly incorporates word frequency into the sampling process. We show that frequency-aware training outperforms uniform sampling in 26 out of 43 languages."
2510.23130,Hidden regular variation for stochastic recursions with diagonal matrices,['math.PR'],"['Ewa Damek', 'Sebastian Mentemeier']","We consider random vectors $X$ that satisfy the equation in law $X=AX+B$, where $A$ is a given random diagonal matrix and $B$ a given random vector, both independent of $X$. It is well known by the works of Kesten and Goldie that the marginals of $X$ may exhibit heavy tails, with possibly different tail indices.
  In recent works (Damek 2025, Mentemeier and Wintenberger 2022) it was observed that asymptotic independence may occur despite strong dependencies in the entries of $A$: The probability that both marginals are simultaneously large decays faster than the marginal probability of an extreme event; the tail measure is concentrated on the axis. In this work, we analyse the hidden regular variation properties of $X$, that is, we find the proper scaling for which one observes simultaneous extremes."
2510.23129,Combining High Level Scheduling and Low Level Control to Manage Fleets of Mobile Robots,"['cs.RO', 'eess.SY']","['Sabino Francesco Roselli', 'Ze Zhang', 'Knut Åkesson']","The deployment of mobile robots for material handling in industrial environments requires scalable coordination of large fleets in dynamic settings. This paper presents a two-layer framework that combines high-level scheduling with low-level control. Tasks are assigned and scheduled using the compositional algorithm ComSat, which generates time-parameterized routes for each robot. These schedules are then used by a distributed Model Predictive Control (MPC) system in real time to compute local reference trajectories, accounting for static and dynamic obstacles. The approach ensures safe, collision-free operation, and supports rapid rescheduling in response to disruptions such as robot failures or environmental changes. We evaluate the method in simulated 2D environments with varying road capacities and traffic conditions, demonstrating high task completion rates and robust behavior even under congestion. The modular structure of the framework allows for computational tractability and flexibility, making it suitable for deployment in complex, real-world industrial scenarios."
2510.23128,From nonlinear Schrödinger equation to interacting particle system: 1 < p < 2,['math.AP'],"['Xin Liao', 'Juntao Lv']","We investigate the limiting behavior of solutions with infinitely many peaks to nonlinear Schrödinger equations [-epsilon^2 Delta u_epsilon + u_epsilon = u_epsilon^p, u_epsilon > 0 in R^n,] as epsilon -> 0, where p is Sobolev subcritical. We derive the interaction law among the limiting peak points and complete the analysis for the previously unresolved range 1 < p < 2, extending the work of Ao, Lv, and Wang (J. Differential Equations, 2025)."
2510.23127,Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs,['cs.AI'],"['Kai Zhuang', 'Jiawei Zhang', 'Yumou Liu', 'Hanqun Cao', 'Chunbin Gu', 'Mengdi Liu', 'Zhangyang Gao', 'Zitong Jerry Wang', 'Xuanhe Zhou', 'Pheng-Ann Heng', 'Lijun Wu', 'Conghui He', 'Cheng Tan']","Scientific Large Language Models (Sci-LLMs) have emerged as a promising frontier for accelerating biological discovery. However, these models face a fundamental challenge when processing raw biomolecular sequences: the tokenization dilemma. Whether treating sequences as a specialized language, risking the loss of functional motif information, or as a separate modality, introducing formidable alignment challenges, current strategies fundamentally limit their reasoning capacity. We challenge this sequence-centric paradigm by positing that a more effective strategy is to provide Sci-LLMs with high-level structured context derived from established bioinformatics tools, thereby bypassing the need to interpret low-level noisy sequence data directly. Through a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we tested three input modes: sequence-only, context-only, and a combination of both. Our findings are striking: the context-only approach consistently and substantially outperforms all other modes. Even more revealing, the inclusion of the raw sequence alongside its high-level context consistently degrades performance, indicating that raw sequences act as informational noise, even for models with specialized tokenization schemes. These results suggest that the primary strength of existing Sci-LLMs lies not in their nascent ability to interpret biomolecular syntax from scratch, but in their profound capacity for reasoning over structured, human-readable knowledge. Therefore, we argue for reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines over expert knowledge. This work lays the foundation for a new class of hybrid scientific AI agents, repositioning the developmental focus from direct sequence interpretation towards high-level knowledge synthesis. The code is available at github.com/opendatalab-raise-dev/CoKE."
2510.23126,A volcanic chronosequence as a time-resolved paleo-detector array to study the cosmic-ray flux in the Late Pleistocene and Holocene,"['astro-ph.HE', 'hep-ph']","['Claudio Galelli', 'Lorenzo Caccianiga', 'Lorenzo Apollonio', 'Paolo Magnani', 'Vincent Breton']","We present a phenomenological study demonstrating the feasibility of using olivine xenoliths from the Chaîne des Puys as a time-resolved paleo-detector array to probe the cosmic-ray flux over the last 40,000 years. This volcanic region provides a unique chronosequence of samples brought to the surface by well-dated eruptions. By modeling the expected density of nuclear recoil tracks induced by cosmic-ray muons in olivine, we show that the signal is detectable and above backgrounds from natural radioactivity. We demonstrate that by analyzing samples with different exposure ages, it is possible to construct a time-differential measurement of the cosmic-ray flux. This method shows sensitivity to historical variations, such as the enhanced flux expected during the Laschamp geomagnetic excursion ($\sim$41~kyr) and the potential contribution from nearby supernovae, for which we use the Antlia supernova remnant precursor as a benchmark. This work establishes a new application of the paleo-detector technique for long-scale time-domain high-energy astrophysics and provides the direct scientific motivation for experimental efforts to measure these track records."
2510.23125,Context-awareness for Dependable Low-Power IoT,['eess.SY'],"['David E. Ruiz-Guirola', 'Prasoon Raghuwanshi', 'Gabriel M. de Jesus', 'Mateen Ashraf', 'Onel L. A. López']","Dependability is the ability to consistently deliver trusted and uninterrupted service in the face of operational uncertainties. Ensuring dependable operation in large-scale, energy-constrained Internet of Things (IoT) deployments is as crucial as challenging, and calls for context-aware protocols where context refers to situational or state information. In this paper, we identify four critical context dimensions for IoT networks, namely energy status, information freshness, task relevance, and physical/medium conditions, and show how each one underpins core dependability attributes. Building on these insights, we propose a two-step protocol design framework that incorporates operation-specific context fields. Through three representative use cases, we demonstrate how context awareness can significantly enhance system dependability while imposing only minimal control-plane overhead."
2510.23124,DeepSalt: Bridging Laboratory and Satellite Spectra through Domain Adaptation and Knowledge Distillation for Large-Scale Soil Salinity Estimation,"['cs.CV', 'cs.LG']","['Rupasree Dey', 'Abdul Matin', 'Everett Lewark', 'Tanjim Bin Faruk', 'Andrei Bachinin', 'Sam Leuthold', 'M. Francesca Cotrufo', 'Shrideep Pallickara', 'Sangmi Lee Pallickara']","Soil salinization poses a significant threat to both ecosystems and agriculture because it limits plants' ability to absorb water and, in doing so, reduces crop productivity. This phenomenon alters the soil's spectral properties, creating a measurable relationship between salinity and light reflectance that enables remote monitoring. While laboratory spectroscopy provides precise measurements, its reliance on in-situ sampling limits scalability to regional or global levels. Conversely, hyperspectral satellite imagery enables wide-area observation but lacks the fine-grained interpretability of laboratory instruments. To bridge this gap, we introduce DeepSalt, a deep-learning-based spectral transfer framework that leverages knowledge distillation and a novel Spectral Adaptation Unit to transfer high-resolution spectral insights from laboratory-based spectroscopy to satellite-based hyperspectral sensing. Our approach eliminates the need for extensive ground sampling while enabling accurate, large-scale salinity estimation, as demonstrated through comprehensive empirical benchmarks. DeepSalt achieves significant performance gains over methods without explicit domain adaptation, underscoring the impact of the proposed Spectral Adaptation Unit and the knowledge distillation strategy. The model also effectively generalized to unseen geographic regions, explaining a substantial portion of the salinity variance."
2510.23123,Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation,"['cs.CL', 'cs.LG']","['Shiwei Li', 'Xiandi Luo', 'Haozhao Wang', 'Xing Tang', 'Ziqiang Cui', 'Dugang Liu', 'Yuhua Li', 'Xiuqiang He', 'Ruixuan Li']","Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). LoRA essentially describes the projection of an input space into a low-dimensional output space, with the dimensionality determined by the LoRA rank. In standard LoRA, all input tokens share the same weights and undergo an identical input-output projection. This limits LoRA's ability to capture token-specific information due to the inherent semantic differences among tokens. To address this limitation, we propose Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts LoRA weights according to the input token, thereby learning token-wise input-output projections in an end-to-end manner. Formally, the weights of TopLoRA can be expressed as $BΣ_X A$, where $A$ and $B$ are low-rank matrices (as in standard LoRA), and $Σ_X$ is a diagonal matrix generated from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA weights but achieves more granular adaptation by learning token-wise LoRA weights (i.e., token-wise input-output projections). Extensive experiments across multiple models and datasets demonstrate that TopLoRA consistently outperforms LoRA and its variants. The code is available at https://github.com/Leopold1423/toplora-neurips25."
2510.23122,FlowCapX: Physics-Grounded Flow Capture with Long-Term Consistency,['cs.GR'],"['Ningxiao Tao', 'Liru Zhang', 'Xingyu Ni', 'Mengyu Chu', 'Baoquan Chen']","We present FlowCapX, a physics-enhanced framework for flow reconstruction from sparse video inputs, addressing the challenge of jointly optimizing complex physical constraints and sparse observational data over long time horizons. Existing methods often struggle to capture turbulent motion while maintaining physical consistency, limiting reconstruction quality and downstream tasks. Focusing on velocity inference, our approach introduces a hybrid framework that strategically separates representation and supervision across spatial scales. At the coarse level, we resolve sparse-view ambiguities via a novel optimization strategy that aligns long-term observation with physics-grounded velocity fields. By emphasizing vorticity-based physical constraints, our method enhances physical fidelity and improves optimization stability. At the fine level, we prioritize observational fidelity to preserve critical turbulent structures. Extensive experiments demonstrate state-of-the-art velocity reconstruction, enabling velocity-aware downstream tasks, e.g., accurate flow analysis, scene augmentation with tracer visualization and re-simulation."
2510.23121,Reliable Robotic Task Execution in the Face of Anomalies,['cs.RO'],"['Bharath Santhanam', 'Alex Mitrevski', 'Santosh Thoduka', 'Sebastian Houben', 'Teena Hassan']","Learned robot policies have consistently been shown to be versatile, but they typically have no built-in mechanism for handling the complexity of open environments, making them prone to execution failures; this implies that deploying policies without the ability to recognise and react to failures may lead to unreliable and unsafe robot behaviour. In this paper, we present a framework that couples a learned policy with a method to detect visual anomalies during policy deployment and to perform recovery behaviours when necessary, thereby aiming to prevent failures. Specifically, we train an anomaly detection model using data collected during nominal executions of a trained policy. This model is then integrated into the online policy execution process, so that deviations from the nominal execution can trigger a three-level sequential recovery process that consists of (i) pausing the execution temporarily, (ii) performing a local perturbation of the robot's state, and (iii) resetting the robot to a safe state by sampling from a learned execution success model. We verify our proposed method in two different scenarios: (i) a door handle reaching task with a Kinova Gen3 arm using a policy trained in simulation and transferred to the real robot, and (ii) an object placing task with a UFactory xArm 6 using a general-purpose policy model. Our results show that integrating policy execution with anomaly detection and recovery increases the execution success rate in environments with various anomalies, such as trajectory deviations and adversarial human interventions."
2510.23120,Weyl group action on Radon hypergeometric function and its symmetry,['math.CA'],['Hironobu Kimura'],"For positive integers $r,n,N:=rn$, we consider the Radon hypergeometric function (Radon HGF) associated with a partition $λ$ of $n$ defined on the Grassmannian $Gr(m,N)$ for $r<m<N$, which is obtained as the Radon transform of a character of the group $H_λ\subset G:=GL(N)$. We study its symmetry described by the Weyl group analogue $N_{G}(H_λ)/H_λ$. We consider the Hermitian matrix integral analogue of the Gauss HGF and its confluent family, which are understood as the Radon HGF on $Gr(2r,4r)$ for partitions $λ$ of $4$, we apply the result of symmetry to these particular cases and derive a transformation formula for the Gauss analogue which is known as a part of ""24 solutions of Kummer"" for the classical Gauss HGF. We derive a similar transformation formula for the analogue Kummer's confluent HGF."
2510.23119,OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback,['cs.RO'],"['Yi-Lin Wei', 'Zhexi Luo', 'Yuhao Lin', 'Mu Lin', 'Zhizhao Liang', 'Shuoyu Chen', 'Wei-Shi Zheng']","Enabling robots to dexterously grasp and manipulate objects based on human commands is a promising direction in robotics. However, existing approaches are challenging to generalize across diverse objects or tasks due to the limited scale of semantic dexterous grasp datasets. Foundation models offer a new way to enhance generalization, yet directly leveraging them to generate feasible robotic actions remains challenging due to the gap between abstract model knowledge and physical robot execution. To address these challenges, we propose OmniDexGrasp, a generalizable framework that achieves omni-capabilities in user prompting, dexterous embodiment, and grasping tasks by combining foundation models with the transfer and control strategies. OmniDexGrasp integrates three key modules: (i) foundation models are used to enhance generalization by generating human grasp images supporting omni-capability of user prompt and task; (ii) a human-image-to-robot-action transfer strategy converts human demonstrations into executable robot actions, enabling omni dexterous embodiment; (iii) force-aware adaptive grasp strategy ensures robust and stable grasp execution. Experiments in simulation and on real robots validate the effectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexterous hands, and further results show its extensibility to dexterous manipulation tasks."
2510.23118,Task-Agnostic Fusion of Time Series and Imagery for Earth Observation,['cs.CV'],"['Gianfranco Basile', 'Johannes Jakubik', 'Benedikt Blumenstiel', 'Thomas Brunschwiler', 'Juan Bernabe Moreno']","We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance. Our approach explores deterministic and learned strategies for time series quantization and then leverages a masked correlation learning objective, aligning discrete image and time series tokens in a unified representation space. Instantiated in the Earth observation domain, the pretrained model generates consistent global temperature profiles from satellite imagery and is validated through counterfactual experiments. Across downstream tasks, our task-agnostic pretraining outperforms task-specific fusion by 6\% in R$^2$ and 2\% in RMSE on average, and exceeds baseline methods by 50\% in R$^2$ and 12\% in RMSE. Finally, we analyze gradient sensitivity across modalities, providing insights into model robustness. Code, data, and weights will be released under a permissive license."
2510.23117,Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction,"['cs.LG', 'cs.CV']","['Omer Jauhar Khan', 'Sudais Khan', 'Hafeez Anwar']","Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges."
2510.23116,Residual Diffusion Bridge Model for Image Restoration,['cs.CV'],"['Hebaixu Wang', 'Jing Zhang', 'Haoyang Chen', 'Haonan Guo', 'Di Wang', 'Jiayi Ma', 'Bo Du']","Diffusion bridge models establish probabilistic paths between arbitrary paired distributions and exhibit great potential for universal image restoration. Most existing methods merely treat them as simple variants of stochastic interpolants, lacking a unified analytical perspective. Besides, they indiscriminately reconstruct images through global noise injection and removal, inevitably distorting undegraded regions due to imperfect reconstruction. To address these challenges, we propose the Residual Diffusion Bridge Model (RDBM). Specifically, we theoretically reformulate the stochastic differential equations of generalized diffusion bridge and derive the analytical formulas of its forward and reverse processes. Crucially, we leverage the residuals from given distributions to modulate the noise injection and removal, enabling adaptive restoration of degraded regions while preserving intact others. Moreover, we unravel the fundamental mathematical essence of existing bridge models, all of which are special cases of RDBM and empirically demonstrate the optimality of our proposed models. Extensive experiments are conducted to demonstrate the state-of-the-art performance of our method both qualitatively and quantitatively across diverse image restoration tasks. Code is publicly available at https://github.com/MiliLab/RDBM."
2510.23115,Luminosity Functions and Detectability of Binary Neutron Star Merger-nova Signals with Various Merger Remnants,['astro-ph.HE'],"['Zhiwei Chen', 'Youjun Lu', 'Hao Ma', 'Qingbo Chu']","With the rapid advancements in next-generation ground-based gravitational wave (GW) detectors, it is anticipated that $10^3$-$10^5$ binary neutron star (BNS) mergers per year will be detected, with a significant fraction accompanied by observable merger-nova signals through future sky surveys. Merger-novae are typically powered by the radioactive decay of heavy elements synthesized via the r-process. If the post-merger remnant is a long-lived rapid-rotating neutron star, the merger-nova can be significantly enhanced due to strong magnetized winds. In this paper, we generate mock BNS merger samples using binary population synthesis model and classify their post-merger remnants--black hole (BH) and magnetar, (i.e., long-lived supramassive NS and stable NS), based on results from numerical simulations. We then construct merger-nova radiation models to estimate their luminosity function. We find that the luminosity function may exhibit a distinctive triple-peak structure, with the relative positions and heights of these peaks depending on the equation of state (EOS) of the BNS. Furthermore, we estimate the average Target-of-Opportunity (ToO) detection efficiency $\langle f_{\rm eff} \rangle$ with the Chinese Space Station Telescope (CSST) and find that due to possible enhanced luminosity, the largest source redshift with $\langle f_{\rm eff} \rangle>0.1$ can be enlarged from $z_{\rm s}\sim 0.5$ to $z_{\rm s}\sim 1-1.5$. Besides, we also generate the detectable mass spectrum for merger-novae by $\langle f_{\rm eff}\rangle$, which may provide insights to the ToO searching strategy."
2510.23114,Flexing in 73 Languages: A Single Small Model for Multilingual Inflection,"['cs.CL', 'doi', '10.1007/978-3-032-02551-7_5']","['Tomáš Sourada', 'Jana Straková']","We present a compact, single-model approach to multilingual inflection, the task of generating inflected word forms from base lemmas to express grammatical categories. Our model, trained jointly on data from 73 languages, is lightweight, robust to unseen words, and outperforms monolingual baselines in most languages. This demonstrates the effectiveness of multilingual modeling for inflection and highlights its practical benefits: simplifying deployment by eliminating the need to manage and retrain dozens of separate monolingual models. In addition to the standard SIGMORPHON shared task benchmarks, we evaluate our monolingual and multilingual models on 73 Universal Dependencies (UD) treebanks, extracting lemma-tag-form triples and their frequency counts. To ensure realistic data splits, we introduce a novel frequency-weighted, lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack of an open-source, general-purpose, multilingual morphological inflection system capable of handling unseen words across a wide range of languages, including Czech. All code is publicly released at: https://github.com/tomsouri/multilingual-inflection."
2510.23113,Revisiting Very High Energy Gamma-Ray Absorption in Cosmic Propagation under the Combined Effects of Axion-Like Particles and Lorentz Violation,"['astro-ph.HE', 'hep-ph']","['Longhua Qin', 'Jiancheng Wang', 'Chuyuan Yang', 'Huaizhen Li', 'Quangui Gao', 'Ju Ma', 'Ao Wang', 'Weiwei Na', 'Ming Zhou', 'Zunli Yuan', 'Chunxia Gu']","Very high energy (VHE) gama rays above 100 GeV are expected to undergo significant attenuation during cosmic propagation due to pair production with thr extralactic background light (EBL). However, recent observations - particulary the tentative detection of gamma ray burst GRB 221009A up to 18 TeV by LHASSO and up to 251 TeV by Carpet-2, challenge the predictions of classical EBL absorption models. These exceptionally high-energy photons suggest the possibility of new physics affecting photon propagation over cosmological distances. In this context, the gamma-ray spectrum in the tens to hundreds of TeV range serves as a valuable probe for potential Lorentz invariance violation (LIV) effects or for modifications to photon attenuation due to axion-like particles (ALPs) mixing with photons in cosmic magnetic fields. However, both LIV and ALPs explanations, when considered separately, face certain limitations in fully accounting for the observed transparency. In this paper, we propose a unified framework that combines the effects of ALPs and LIV to explain the unexpectedly high survival probability of VHE photons. Specifically, we investigate the multi-wavelength spectrum of GRB 221009A under this synergistic scenario, demonstrating that for photons at 18 TeV and 251 TeV, the combined influence of ALPs with coupling $g_{aγ} = 13.353 \times 10^{-11}\ \mathrm{GeV}^{-1}$ and mass $m_a = 9.492 \times 10^{-7}\ \mathrm{eV}$, together with LIV characterized by energy scales $E_1 = 2.4~E_{\rm Planck}$ for $n = 1$ , can substantially enhance the photon survival probability. This synergy offers a compelling explanation for the observed VHE gamma-ray transparency in extreme astrophysical environments."
2510.23112,GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction,"['cs.CE', 'cs.AI']","['Minjoo Kim', 'Jinwoong Kim', 'Sangjin Park']","Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability. To address this limitation, explainable AI techniques, particularly SHAP (SHapley Additive Explanations), have been employed to identify influential features. However, SHAP's computational cost grows exponentially with input features, making it impractical for large-scale text-based financial data. This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups rather than individual tokens, substantially reducing computational burden while preserving interpretability. We employed FinBERT to embed news articles from 2015 to 2024, clustered them into coherent semantic groups, and applied GroupSHAP to measure each group's contribution to stock price movements. The resulting group-level SHAP variables across multiple topics were used as input features for the prediction model. Empirical results from one-day-ahead forecasting of the S&P 500 index throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared with benchmark models without the GroupSHAP mechanism. This research presents the first application of GroupSHAP in news-driven financial forecasting, showing that grouped sentiment representations simultaneously enhance interpretability and predictive performance."
2510.23111,Neural Emulator Superiority: When Machine Learning for PDEs Surpasses its Training Data,['cs.LG'],"['Felix Koehler', 'Nils Thuerey']","Neural operators or emulators for PDEs trained on data from numerical solvers are conventionally assumed to be limited by their training data's fidelity. We challenge this assumption by identifying ""emulator superiority,"" where neural networks trained purely on low-fidelity solver data can achieve higher accuracy than those solvers when evaluated against a higher-fidelity reference. Our theoretical analysis reveals how the interplay between emulator inductive biases, training objectives, and numerical error characteristics enables superior performance during multi-step rollouts. We empirically validate this finding across different PDEs using standard neural architectures, demonstrating that emulators can implicitly learn dynamics that are more regularized or exhibit more favorable error accumulation properties than their training data, potentially surpassing training data limitations and mitigating numerical artifacts. This work prompts a re-evaluation of emulator benchmarking, suggesting neural emulators might achieve greater physical fidelity than their training source within specific operational regimes. Project Page: https://tum-pbs.github.io/emulator-superiority"
2510.23110,Quantum-Enhanced Multiparameter Sensing via Driven Multi-ensemble Superradiance,['quant-ph'],"['Kang Shen', 'Xiangming Hu', 'Fei Wang']","We analytically derive the dark-state covariance matrix of a driven multi-ensemble superradiant system in the large-$N$ limit, revealing entanglement among the ensembles that enhances quantum metrology. The minimum eigenvalue, linked to the curvature of the superradiance potential trapping the dark state, corresponds to the optimal multiparameter squeezing coefficient, defined as the Rayleigh quotient of the squeezing matrix. This system provides a concrete protocol for achieving optimal quantum enhancement in arbitrary multiparameter estimation tasks, making it a promising candidate for multimode quantum interferometry."
2510.23109,An Automated Tape Laying System Employing a Uniaxial Force Control Device,['cs.RO'],"['Bernhard Rameder', 'Hubert Gattringer', 'Ronald Naderer', 'Andreas Mueller']","This paper deals with the design of a cost effective automated tape laying system (ATL system) with integrated uniaxial force control to ensure the necessary compaction forces as well as with an accurate temperature control to guarantee the used tape being melted appropriate. It is crucial to control the substrate and the oncoming tape onto a specific temperature level to ensure an optimal consolidation between the different layers of the product. Therefore, it takes several process steps from the spooled tape on the coil until it is finally tacked onto the desired mold. The different modules are divided into the tape storage spool, a tape-guiding roller, a tape processing unit, a heating zone and the consolidation unit. Moreover, a special robot control concept for testing the ATL system is presented. In contrast to many other systems, with this approach, the tape laying device is spatially fixed and the shape is moved accordingly by the robot, which allows for handling of rather compact and complex shapes. The functionality of the subsystems and the taping process itself was finally approved in experimental results using a carbon fiber reinforced HDPE tape."
2510.23108,Heart Rate Variability Patterns Reflect Yoga Intervention in Chronically Stressed Pregnant Women: A Quasi-Randomized Controlled Trial,['q-bio.QM'],"['Marlene J E Mayer', 'Nicolas B Garnier', 'Clara Becker', 'Marta C Antonelli', 'Silvia M Lobmaier', 'Martin G Frasch']","Prenatal maternal stress (PS) is a risk factor for adverse offspring neurodevelopment. Heart rate variability (HRV) complexity provides a non-invasive marker of maternal autonomic regulation and may be influenced by mind--body interventions such as Yoga. In this quasi-randomized controlled trial, 28 chronically stressed pregnant women were followed from the second trimester until birth: 14 participated in weekly Hatha Yoga with electrocardiogram (ECG) recordings, and 14 received standard obstetric care with monthly ECGs. Group allocation was based on availability, with participants unaware of their assignment at enrollment. HRV complexity was assessed first with Sample Entropy and Entropy Rate and then expanded to 94 HRV metrics spanning temporal, frequency, nonlinear, and information-theoretical domains. All metrics were covariate-adjusted (maternal age, BMI, gestational age), standardized, and analyzed using timepoint-specific principal component analysis (PCA). From this, a unified HRV index was derived. Analyses revealed that HRV metric relationships changed dynamically across pregnancy, with PCA loadings shifting from frequency toward complexity measures in late gestation. The mixed effects model identified a significant time x group interaction effect (p = 0.041). These findings suggest a restructuring of HRV signal-analytical domains with advancing pregnancy attributable to Yoga and highlight the utility of advanced HRV analysis frameworks for future, larger trials."
